name: harmony-demonstration

x-logging:
  &default-logging
  driver: "json-file"
  options:
    max-size: 10m
    max-file: "10"

networks:
  harmony_demonstration_network:
    name: harmony_demonstration_network
    driver: bridge
  druid_network:
    name: druid_network
    driver: bridge
    external: true

services:
  postgres:
    # Postgres included for ease of testing. The configuration for postgres
    # included here is not production ready!
    # For production, postgres should be configured to be highly available
    # and should have appropriate backup and recovery procedures in place.
    profiles:
      - postgres
    image: postgres:13.7-alpine
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - pgdata-volume:/var/lib/postgresql/data
    # If resource management is required - see:
    # https://docs.docker.com/compose/compose-file/deploy/#resources
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.50'
    #       memory: 50M
    #       pids: 1
    #     reservations:
    #       cpus: '0.25'
    #       memory: 20M
    networks:
      - harmony_demonstration_network
  minio:
    # Minio included for ease of testing. The configuration for minio
    # included here is not production ready!
    # For production, minio should be configured to be highly available
    # and should have appropriate backup and recovery procedures in place.
    profiles:
      - minio
    image: minio/minio
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    hostname: minio
    ports:
      - 9000:9000
      - 9090:9090
    volumes:
      - minio-data-volume:/data
    command: [ "server", "/data", "--console-address", ":9090" ]
    networks:
      - harmony_demonstration_network
  nginx:
    image: nginxproxy/nginx-proxy:alpine
    restart: always
    pull_policy: always
    ports:
      - 80:80
      - 443:443
    labels:
      com.github.jrcs.letsencrypt_nginx_proxy_companion.nginx_proxy: ""
    volumes:
      - /var/run/docker.sock:/tmp/docker.sock:ro
      # Bind the SSL certificate path from the host system to the
      # directory the nginx container uses for all certs.
      - ssl-certs:/etc/nginx/certs:ro
      - nginx_vhost:/etc/nginx/vhost.d
      - nginx_html:/usr/share/nginx/html
      # Include Zenysis static assets and serve them directly from nginx.
      - ${OUTPUT_PATH}/zenysis_static:/usr/share/nginx/html/zenysis_static:ro
      # Set a custom vhost default configuration that includes compression and caching.
      - ${NGINX_DEFAULT_VHOST_CONFIG}:/etc/nginx/vhost.d/default:ro
    logging: *default-logging
    networks:
      - harmony_demonstration_network

  nginx-letsencrypt-companion:
    image: nginxproxy/acme-companion:latest
    restart: always
    pull_policy: always
    environment:
      - DHPARAM_BITS=4096
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # These are the same volumes used by the nginx image.
      # This is on purpose since the letsencrypt companion container works
      # with the nginx container collaboratively to generate SSL certs.
      - ssl-certs:/etc/nginx/certs:rw
      - nginx_vhost:/etc/nginx/vhost.d
      - nginx_html:/usr/share/nginx/html
      - nginx-acme:/etc/acme.sh
    logging: *default-logging
    networks:
      - harmony_demonstration_network

  web_client:
    platform: linux/amd64
    image: ${DOCKER_NAMESPACE}/harmony-web-client:${DOCKER_TAG}
    build:
      dockerfile: docker/web/Dockerfile_web-client
      context: ../
      args:
        NAMESPACE: zengineering
        TAG: latest
  web_server:
    platform: linux/amd64
    image: ${DOCKER_NAMESPACE}/harmony-web-server:${DOCKER_TAG}
    build:
      dockerfile: docker/web/Dockerfile_web-server
      context: ../
      args:
        NAMESPACE: zengineering
        TAG: latest
  web:
    platform: linux/amd64
    image: ${DOCKER_NAMESPACE}/harmony-web:${DOCKER_TAG}
    depends_on:
      - web_client
      - web_server
    build:
      # We build from scratch, to be sure that our local image matches the codebase.
      dockerfile: docker/web/Dockerfile_web
      context: ../
      args:
        NAMESPACE: zengineering
        TAG: latest
    restart: always
    # We set the pull policy to never, in order to allow usage of locally built image.
    # e.g. if you run `make web_client_build ; make web_server_build ; make web_build`
    # you'll have a locally built copy. If the pull_policy were set to always, docker
    # would never use it, and always pull from remote.
    pull_policy: never
    ports:
      - 5000:5000
    environment:
      - ZEN_ENV=${ZEN_WEB_ENV}
      - DONT_SYNC_SOURCEMAPS=true
      - DATABASE_URL=${POSTGRES_DB_URI}
      - HSTS=off
      - SSL_POLICY=Mozilla-Modern
      - VIRTUAL_HOST=${ZEN_WEB_HOST}
      - LETSENCRYPT_HOST=${ZEN_WEB_HOST}
      - LETSENCRYPT_EMAIL=${ZEN_WEB_EMAIL}
      - DRUID_HOST=${DRUID_HOST}
    volumes:
      - ${OUTPUT_PATH}:/data/output
      - ${OUTPUT_PATH}/logs:/logs
      - ${INSTANCE_CONFIG}:/zenysis/instance_config.json
      - ${GLOBAL_CONFIG}:/zenysis/global_config.py
      - ${UPLOADS_DIR}:/zenysis/uploads
    command: [ "/zenysis/docker/entrypoint_web.sh" ]
    logging: *default-logging
    networks:
      - harmony_demonstration_network
      - druid_network

  pipeline:
    profiles:
      - pipeline
    restart: "no" # Once the pipeline is done running, we exit
    build:
      context: ../../
      dockerfile: demonstration/harmony/pipeline/Dockerfile
    volumes:
      - ./mc.config.json:/root/.mc/config.json
      - ../..:/src/zenysis
      # exclude these folders:
      - /src/zenysis/venv
      - /src/zenysis/venv_pypy3
      - /src/zenysis/node_modules
    networks:
      - harmony_demonstration_network
      - druid_network

  worker:
    platform: linux/amd64
    image: ${DOCKER_NAMESPACE}/${DOCKER_IMAGE:-harmony-web}:${DOCKER_TAG}
    restart: always
    pull_policy: always
    ports:
      - 61234:61234
    environment:
      - ZEN_ENV=${ZEN_WEB_ENV}
      - ZEN_LOG_CFG=/zenysis/log/log.prod-stackdriver.cfg
      - BROKER_URL=redis://redis:6379/0
    volumes:
      - ${OUTPUT_PATH}/logs:/logs
      - ${INSTANCE_CONFIG}:/zenysis/instance_config.json
      - ${GLOBAL_CONFIG}:/zenysis/global_config.py
    command:
      [
        "celery",
        "-A",
        "web.background_worker.celery",
        "worker",
        "--beat",
        "--loglevel=INFO"
      ]
    logging: *default-logging
    networks:
      - harmony_demonstration_network

  redis:
    image: redis:latest
    restart: always
    pull_policy: always
    ports:
      - 6379:6379
    networks:
      - harmony_demonstration_network
    volumes:
      - redis-volume:/data

  hasura:
    image: hasura/graphql-engine:latest.cli-migrations-v2
    restart: always
    pull_policy: always
    ports:
      - 8080:8080
    environment:
      HASURA_GRAPHQL_ENABLE_TELEMETRY: "false"
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true"
      HASURA_GRAPHQL_MIGRATIONS_SERVER_TIMEOUT: 500
      HASURA_GRAPHQL_DATABASE_URL: ${GRAPHQL_DATABASE_URL}
    volumes:
      - ${OUTPUT_PATH}/logs:/logs
    networks:
      - harmony_demonstration_network
      - druid_network

  utility:
    # Convenience container for debugging. It can be useful to jump
    # onto a container that has curl/ping etc. installed to troubleshoot
    # from within the container context.
    # `docker compose run utility /bin/bash`
    build:
      context: utility
      dockerfile: Dockerfile
    networks:
      - harmony_demonstration_network
      - druid_network
    profiles:
      - utility

volumes:
  # Let's Encrypt container needs to configure the vhosts during
  # cert issuing.
  nginx_vhost: # This html directory is only used to write let's encrypt challenge

  # files during the cert issuing process.
  nginx_html: # The acme.sh diretory stores the generated certificate state

  # This prevents re-issuing of the certificate if the container is restarted/re-created
  # It should only issue if there is no certificate or it is close to expiry.
  # Read more here: https://github.com/nginx-proxy/acme-companion/issues/510
  nginx-acme:

  # This volume is used to store the postgres data.
  pgdata-volume:
    # We may want to point this to a specific location on the host system,
    # in that case - uncomment the following lines and set the PG_DATA_VOLUME
    # driver: local
    # driver_opts:
    #   type: none
    #   o: bind
    #   device: "${PG_DATA_VOLUME:-/var/lib/postgresql/data}"

  # This volume is used to store the minio data.
  minio-data-volume:
    # We may want to point this to a specific location on the host system,
    # in that case - uncomment the following lines and set the MINIO_DATA_VOLUME
    # driver: local
    # driver_opts:
    #   type: none
    #   o: bind
    #   device: "${MINIO_DATA_VOLUME:-/var/lib/minio/data}"

  redis-volume:

  ssl-certs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${SSL_CERTS:-/etc/ssl/certs}"
