schema {
  query: query_root
  mutation: mutation_root
  subscription: subscription_root
}

# whether this query should be cached (Hasura Cloud only)
directive @cached(
  # measured in seconds
  ttl: Int! = 60

  # refresh the cache entry
  refresh: Boolean! = false
) on QUERY

# columns and relationships of "alert_definitions"
type alert_definitions implements Node {
  # An array relationship
  alert_notifications(
    # distinct select on columns
    distinct_on: [alert_notifications_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [alert_notifications_order_by!]

    # filter the rows returned
    where: alert_notifications_bool_exp
  ): [alert_notifications!]!

  # An aggregate relationship
  alert_notifications_aggregate(
    # distinct select on columns
    distinct_on: [alert_notifications_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [alert_notifications_order_by!]

    # filter the rows returned
    where: alert_notifications_bool_exp
  ): alert_notifications_aggregate!

  # An array relationship connection
  alert_notifications_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [alert_notifications_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [alert_notifications_order_by!]

    # filter the rows returned
    where: alert_notifications_bool_exp
  ): alert_notificationsConnection!
  authorization_resource_id: Int!
  checks(
    # JSON select path
    path: String
  ): jsonb!
  dimension_name: String
  fields(
    # JSON select path
    path: String
  ): jsonb!
  filters(
    # JSON select path
    path: String
  ): jsonb!
  id: ID!
  time_granularity: String!
  title: String!
  user_id: Int!
}

# append existing jsonb value of filtered columns with new jsonb value
input alert_definitions_append_input {
  checks: jsonb
  fields: jsonb
  filters: jsonb
}

# Boolean expression to filter rows from the table "alert_definitions". All fields are combined with a logical 'AND'.
input alert_definitions_bool_exp {
  _and: [alert_definitions_bool_exp!]
  _not: alert_definitions_bool_exp
  _or: [alert_definitions_bool_exp!]
  alert_notifications: alert_notifications_bool_exp
  authorization_resource_id: Int_comparison_exp
  checks: jsonb_comparison_exp
  dimension_name: String_comparison_exp
  fields: jsonb_comparison_exp
  filters: jsonb_comparison_exp
  id: Int_comparison_exp
  time_granularity: String_comparison_exp
  title: String_comparison_exp
  user_id: Int_comparison_exp
}

# unique or primary key constraints on table "alert_definitions"
enum alert_definitions_constraint {
  # unique or primary key constraint on columns "id"
  alert_definitions_pkey

  # unique or primary key constraint on columns "authorization_resource_id"
  unique_alert_resource
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input alert_definitions_delete_at_path_input {
  checks: [String!]
  fields: [String!]
  filters: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input alert_definitions_delete_elem_input {
  checks: Int
  fields: Int
  filters: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input alert_definitions_delete_key_input {
  checks: String
  fields: String
  filters: String
}

# input type for incrementing numeric columns in table "alert_definitions"
input alert_definitions_inc_input {
  authorization_resource_id: Int
  id: Int
  user_id: Int
}

# input type for inserting data into table "alert_definitions"
input alert_definitions_insert_input {
  alert_notifications: alert_notifications_arr_rel_insert_input
  authorization_resource_id: Int
  checks: jsonb
  dimension_name: String
  fields: jsonb
  filters: jsonb
  id: Int
  time_granularity: String
  title: String
  user_id: Int
}

# response of any mutation on the table "alert_definitions"
type alert_definitions_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [alert_definitions!]!
}

# input type for inserting object relation for remote table "alert_definitions"
input alert_definitions_obj_rel_insert_input {
  data: alert_definitions_insert_input!

  # upsert condition
  on_conflict: alert_definitions_on_conflict
}

# on_conflict condition type for table "alert_definitions"
input alert_definitions_on_conflict {
  constraint: alert_definitions_constraint!
  update_columns: [alert_definitions_update_column!]! = []
  where: alert_definitions_bool_exp
}

# Ordering options when selecting data from "alert_definitions".
input alert_definitions_order_by {
  alert_notifications_aggregate: alert_notifications_aggregate_order_by
  authorization_resource_id: order_by
  checks: order_by
  dimension_name: order_by
  fields: order_by
  filters: order_by
  id: order_by
  time_granularity: order_by
  title: order_by
  user_id: order_by
}

# primary key columns input for table: alert_definitions
input alert_definitions_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input alert_definitions_prepend_input {
  checks: jsonb
  fields: jsonb
  filters: jsonb
}

# select columns of table "alert_definitions"
enum alert_definitions_select_column {
  # column name
  authorization_resource_id

  # column name
  checks

  # column name
  dimension_name

  # column name
  fields

  # column name
  filters

  # column name
  id

  # column name
  time_granularity

  # column name
  title

  # column name
  user_id
}

# input type for updating data in table "alert_definitions"
input alert_definitions_set_input {
  authorization_resource_id: Int
  checks: jsonb
  dimension_name: String
  fields: jsonb
  filters: jsonb
  id: Int
  time_granularity: String
  title: String
  user_id: Int
}

# update columns of table "alert_definitions"
enum alert_definitions_update_column {
  # column name
  authorization_resource_id

  # column name
  checks

  # column name
  dimension_name

  # column name
  fields

  # column name
  filters

  # column name
  id

  # column name
  time_granularity

  # column name
  title

  # column name
  user_id
}

input alert_definitions_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: alert_definitions_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: alert_definitions_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: alert_definitions_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: alert_definitions_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: alert_definitions_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: alert_definitions_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: alert_definitions_set_input
  where: alert_definitions_bool_exp!
}

# A Relay connection object on "alert_definitions"
type alert_definitionsConnection {
  edges: [alert_definitionsEdge!]!
  pageInfo: PageInfo!
}

type alert_definitionsEdge {
  cursor: String!
  node: alert_definitions!
}

# columns and relationships of "alert_notifications"
type alert_notifications implements Node {
  # An object relationship
  alert_definition: alert_definitions
  alert_definition_id: Int

  # An object relationship
  case: case
  compared_val: String
  dimension_info(
    # JSON select path
    path: String
  ): jsonb
  generation_date: String
  id: ID!
  query_interval: String
  reported_val: String
}

# aggregated selection of "alert_notifications"
type alert_notifications_aggregate {
  aggregate: alert_notifications_aggregate_fields
  nodes: [alert_notifications!]!
}

# aggregate fields of "alert_notifications"
type alert_notifications_aggregate_fields {
  avg: alert_notifications_avg_fields
  count(columns: [alert_notifications_select_column!], distinct: Boolean): Int!
  max: alert_notifications_max_fields
  min: alert_notifications_min_fields
  stddev: alert_notifications_stddev_fields
  stddev_pop: alert_notifications_stddev_pop_fields
  stddev_samp: alert_notifications_stddev_samp_fields
  sum: alert_notifications_sum_fields
  var_pop: alert_notifications_var_pop_fields
  var_samp: alert_notifications_var_samp_fields
  variance: alert_notifications_variance_fields
}

# order by aggregate values of table "alert_notifications"
input alert_notifications_aggregate_order_by {
  avg: alert_notifications_avg_order_by
  count: order_by
  max: alert_notifications_max_order_by
  min: alert_notifications_min_order_by
  stddev: alert_notifications_stddev_order_by
  stddev_pop: alert_notifications_stddev_pop_order_by
  stddev_samp: alert_notifications_stddev_samp_order_by
  sum: alert_notifications_sum_order_by
  var_pop: alert_notifications_var_pop_order_by
  var_samp: alert_notifications_var_samp_order_by
  variance: alert_notifications_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input alert_notifications_append_input {
  dimension_info: jsonb
}

# input type for inserting array relation for remote table "alert_notifications"
input alert_notifications_arr_rel_insert_input {
  data: [alert_notifications_insert_input!]!

  # upsert condition
  on_conflict: alert_notifications_on_conflict
}

# aggregate avg on columns
type alert_notifications_avg_fields {
  alert_definition_id: Float
  id: Float
}

# order by avg() on columns of table "alert_notifications"
input alert_notifications_avg_order_by {
  alert_definition_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table "alert_notifications". All fields are combined with a logical 'AND'.
input alert_notifications_bool_exp {
  _and: [alert_notifications_bool_exp!]
  _not: alert_notifications_bool_exp
  _or: [alert_notifications_bool_exp!]
  alert_definition: alert_definitions_bool_exp
  alert_definition_id: Int_comparison_exp
  case: case_bool_exp
  compared_val: String_comparison_exp
  dimension_info: jsonb_comparison_exp
  generation_date: String_comparison_exp
  id: Int_comparison_exp
  query_interval: String_comparison_exp
  reported_val: String_comparison_exp
}

# unique or primary key constraints on table "alert_notifications"
enum alert_notifications_constraint {
  # unique or primary key constraint on columns "id"
  alert_notifications_pkey
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input alert_notifications_delete_at_path_input {
  dimension_info: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input alert_notifications_delete_elem_input {
  dimension_info: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input alert_notifications_delete_key_input {
  dimension_info: String
}

# input type for incrementing numeric columns in table "alert_notifications"
input alert_notifications_inc_input {
  alert_definition_id: Int
  id: Int
}

# input type for inserting data into table "alert_notifications"
input alert_notifications_insert_input {
  alert_definition: alert_definitions_obj_rel_insert_input
  alert_definition_id: Int
  case: case_obj_rel_insert_input
  compared_val: String
  dimension_info: jsonb
  generation_date: String
  id: Int
  query_interval: String
  reported_val: String
}

# aggregate max on columns
type alert_notifications_max_fields {
  alert_definition_id: Int
  compared_val: String
  generation_date: String
  id: Int
  query_interval: String
  reported_val: String
}

# order by max() on columns of table "alert_notifications"
input alert_notifications_max_order_by {
  alert_definition_id: order_by
  compared_val: order_by
  generation_date: order_by
  id: order_by
  query_interval: order_by
  reported_val: order_by
}

# aggregate min on columns
type alert_notifications_min_fields {
  alert_definition_id: Int
  compared_val: String
  generation_date: String
  id: Int
  query_interval: String
  reported_val: String
}

# order by min() on columns of table "alert_notifications"
input alert_notifications_min_order_by {
  alert_definition_id: order_by
  compared_val: order_by
  generation_date: order_by
  id: order_by
  query_interval: order_by
  reported_val: order_by
}

# response of any mutation on the table "alert_notifications"
type alert_notifications_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [alert_notifications!]!
}

# input type for inserting object relation for remote table "alert_notifications"
input alert_notifications_obj_rel_insert_input {
  data: alert_notifications_insert_input!

  # upsert condition
  on_conflict: alert_notifications_on_conflict
}

# on_conflict condition type for table "alert_notifications"
input alert_notifications_on_conflict {
  constraint: alert_notifications_constraint!
  update_columns: [alert_notifications_update_column!]! = []
  where: alert_notifications_bool_exp
}

# Ordering options when selecting data from "alert_notifications".
input alert_notifications_order_by {
  alert_definition: alert_definitions_order_by
  alert_definition_id: order_by
  case: case_order_by
  compared_val: order_by
  dimension_info: order_by
  generation_date: order_by
  id: order_by
  query_interval: order_by
  reported_val: order_by
}

# primary key columns input for table: alert_notifications
input alert_notifications_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input alert_notifications_prepend_input {
  dimension_info: jsonb
}

# select columns of table "alert_notifications"
enum alert_notifications_select_column {
  # column name
  alert_definition_id

  # column name
  compared_val

  # column name
  dimension_info

  # column name
  generation_date

  # column name
  id

  # column name
  query_interval

  # column name
  reported_val
}

# input type for updating data in table "alert_notifications"
input alert_notifications_set_input {
  alert_definition_id: Int
  compared_val: String
  dimension_info: jsonb
  generation_date: String
  id: Int
  query_interval: String
  reported_val: String
}

# aggregate stddev on columns
type alert_notifications_stddev_fields {
  alert_definition_id: Float
  id: Float
}

# order by stddev() on columns of table "alert_notifications"
input alert_notifications_stddev_order_by {
  alert_definition_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type alert_notifications_stddev_pop_fields {
  alert_definition_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "alert_notifications"
input alert_notifications_stddev_pop_order_by {
  alert_definition_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type alert_notifications_stddev_samp_fields {
  alert_definition_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "alert_notifications"
input alert_notifications_stddev_samp_order_by {
  alert_definition_id: order_by
  id: order_by
}

# aggregate sum on columns
type alert_notifications_sum_fields {
  alert_definition_id: Int
  id: Int
}

# order by sum() on columns of table "alert_notifications"
input alert_notifications_sum_order_by {
  alert_definition_id: order_by
  id: order_by
}

# update columns of table "alert_notifications"
enum alert_notifications_update_column {
  # column name
  alert_definition_id

  # column name
  compared_val

  # column name
  dimension_info

  # column name
  generation_date

  # column name
  id

  # column name
  query_interval

  # column name
  reported_val
}

input alert_notifications_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: alert_notifications_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: alert_notifications_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: alert_notifications_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: alert_notifications_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: alert_notifications_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: alert_notifications_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: alert_notifications_set_input
  where: alert_notifications_bool_exp!
}

# aggregate var_pop on columns
type alert_notifications_var_pop_fields {
  alert_definition_id: Float
  id: Float
}

# order by var_pop() on columns of table "alert_notifications"
input alert_notifications_var_pop_order_by {
  alert_definition_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type alert_notifications_var_samp_fields {
  alert_definition_id: Float
  id: Float
}

# order by var_samp() on columns of table "alert_notifications"
input alert_notifications_var_samp_order_by {
  alert_definition_id: order_by
  id: order_by
}

# aggregate variance on columns
type alert_notifications_variance_fields {
  alert_definition_id: Float
  id: Float
}

# order by variance() on columns of table "alert_notifications"
input alert_notifications_variance_order_by {
  alert_definition_id: order_by
  id: order_by
}

# A Relay connection object on "alert_notifications"
type alert_notificationsConnection {
  edges: [alert_notificationsEdge!]!
  pageInfo: PageInfo!
}

type alert_notificationsEdge {
  cursor: String!
  node: alert_notifications!
}

# columns and relationships of "banned_raw_pipeline_entity_match"
type banned_raw_pipeline_entity_match implements Node {
  date_changed: timestamp!
  id: ID!

  # An object relationship
  rawPipelineEntityByRawEntityIdA: raw_pipeline_entity!
  raw_entity_id_a: Int!
  raw_entity_id_b: Int!

  # An object relationship
  raw_pipeline_entity: raw_pipeline_entity!
  user_id: Int
}

# aggregated selection of "banned_raw_pipeline_entity_match"
type banned_raw_pipeline_entity_match_aggregate {
  aggregate: banned_raw_pipeline_entity_match_aggregate_fields
  nodes: [banned_raw_pipeline_entity_match!]!
}

# aggregate fields of "banned_raw_pipeline_entity_match"
type banned_raw_pipeline_entity_match_aggregate_fields {
  avg: banned_raw_pipeline_entity_match_avg_fields
  count(columns: [banned_raw_pipeline_entity_match_select_column!], distinct: Boolean): Int!
  max: banned_raw_pipeline_entity_match_max_fields
  min: banned_raw_pipeline_entity_match_min_fields
  stddev: banned_raw_pipeline_entity_match_stddev_fields
  stddev_pop: banned_raw_pipeline_entity_match_stddev_pop_fields
  stddev_samp: banned_raw_pipeline_entity_match_stddev_samp_fields
  sum: banned_raw_pipeline_entity_match_sum_fields
  var_pop: banned_raw_pipeline_entity_match_var_pop_fields
  var_samp: banned_raw_pipeline_entity_match_var_samp_fields
  variance: banned_raw_pipeline_entity_match_variance_fields
}

# order by aggregate values of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_aggregate_order_by {
  avg: banned_raw_pipeline_entity_match_avg_order_by
  count: order_by
  max: banned_raw_pipeline_entity_match_max_order_by
  min: banned_raw_pipeline_entity_match_min_order_by
  stddev: banned_raw_pipeline_entity_match_stddev_order_by
  stddev_pop: banned_raw_pipeline_entity_match_stddev_pop_order_by
  stddev_samp: banned_raw_pipeline_entity_match_stddev_samp_order_by
  sum: banned_raw_pipeline_entity_match_sum_order_by
  var_pop: banned_raw_pipeline_entity_match_var_pop_order_by
  var_samp: banned_raw_pipeline_entity_match_var_samp_order_by
  variance: banned_raw_pipeline_entity_match_variance_order_by
}

# input type for inserting array relation for remote table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_arr_rel_insert_input {
  data: [banned_raw_pipeline_entity_match_insert_input!]!

  # upsert condition
  on_conflict: banned_raw_pipeline_entity_match_on_conflict
}

# aggregate avg on columns
type banned_raw_pipeline_entity_match_avg_fields {
  id: Float
  raw_entity_id_a: Float
  raw_entity_id_b: Float
  user_id: Float
}

# order by avg() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_avg_order_by {
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# Boolean expression to filter rows from the table
# "banned_raw_pipeline_entity_match". All fields are combined with a logical 'AND'.
input banned_raw_pipeline_entity_match_bool_exp {
  _and: [banned_raw_pipeline_entity_match_bool_exp!]
  _not: banned_raw_pipeline_entity_match_bool_exp
  _or: [banned_raw_pipeline_entity_match_bool_exp!]
  date_changed: timestamp_comparison_exp
  id: Int_comparison_exp
  rawPipelineEntityByRawEntityIdA: raw_pipeline_entity_bool_exp
  raw_entity_id_a: Int_comparison_exp
  raw_entity_id_b: Int_comparison_exp
  raw_pipeline_entity: raw_pipeline_entity_bool_exp
  user_id: Int_comparison_exp
}

# unique or primary key constraints on table "banned_raw_pipeline_entity_match"
enum banned_raw_pipeline_entity_match_constraint {
  # unique or primary key constraint on columns "id"
  banned_raw_pipeline_entity_match_pkey

  # unique or primary key constraint on columns "raw_entity_id_a", "raw_entity_id_b"
  unique_banned_match
}

# input type for incrementing numeric columns in table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_inc_input {
  id: Int
  raw_entity_id_a: Int
  raw_entity_id_b: Int
  user_id: Int
}

# input type for inserting data into table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_insert_input {
  date_changed: timestamp
  id: Int
  rawPipelineEntityByRawEntityIdA: raw_pipeline_entity_obj_rel_insert_input
  raw_entity_id_a: Int
  raw_entity_id_b: Int
  raw_pipeline_entity: raw_pipeline_entity_obj_rel_insert_input
  user_id: Int
}

# aggregate max on columns
type banned_raw_pipeline_entity_match_max_fields {
  date_changed: timestamp
  id: Int
  raw_entity_id_a: Int
  raw_entity_id_b: Int
  user_id: Int
}

# order by max() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_max_order_by {
  date_changed: order_by
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# aggregate min on columns
type banned_raw_pipeline_entity_match_min_fields {
  date_changed: timestamp
  id: Int
  raw_entity_id_a: Int
  raw_entity_id_b: Int
  user_id: Int
}

# order by min() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_min_order_by {
  date_changed: order_by
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# response of any mutation on the table "banned_raw_pipeline_entity_match"
type banned_raw_pipeline_entity_match_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [banned_raw_pipeline_entity_match!]!
}

# on_conflict condition type for table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_on_conflict {
  constraint: banned_raw_pipeline_entity_match_constraint!
  update_columns: [banned_raw_pipeline_entity_match_update_column!]! = []
  where: banned_raw_pipeline_entity_match_bool_exp
}

# Ordering options when selecting data from "banned_raw_pipeline_entity_match".
input banned_raw_pipeline_entity_match_order_by {
  date_changed: order_by
  id: order_by
  rawPipelineEntityByRawEntityIdA: raw_pipeline_entity_order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  raw_pipeline_entity: raw_pipeline_entity_order_by
  user_id: order_by
}

# primary key columns input for table: banned_raw_pipeline_entity_match
input banned_raw_pipeline_entity_match_pk_columns_input {
  id: Int!
}

# select columns of table "banned_raw_pipeline_entity_match"
enum banned_raw_pipeline_entity_match_select_column {
  # column name
  date_changed

  # column name
  id

  # column name
  raw_entity_id_a

  # column name
  raw_entity_id_b

  # column name
  user_id
}

# input type for updating data in table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_set_input {
  date_changed: timestamp
  id: Int
  raw_entity_id_a: Int
  raw_entity_id_b: Int
  user_id: Int
}

# aggregate stddev on columns
type banned_raw_pipeline_entity_match_stddev_fields {
  id: Float
  raw_entity_id_a: Float
  raw_entity_id_b: Float
  user_id: Float
}

# order by stddev() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_stddev_order_by {
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# aggregate stddev_pop on columns
type banned_raw_pipeline_entity_match_stddev_pop_fields {
  id: Float
  raw_entity_id_a: Float
  raw_entity_id_b: Float
  user_id: Float
}

# order by stddev_pop() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_stddev_pop_order_by {
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# aggregate stddev_samp on columns
type banned_raw_pipeline_entity_match_stddev_samp_fields {
  id: Float
  raw_entity_id_a: Float
  raw_entity_id_b: Float
  user_id: Float
}

# order by stddev_samp() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_stddev_samp_order_by {
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# aggregate sum on columns
type banned_raw_pipeline_entity_match_sum_fields {
  id: Int
  raw_entity_id_a: Int
  raw_entity_id_b: Int
  user_id: Int
}

# order by sum() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_sum_order_by {
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# update columns of table "banned_raw_pipeline_entity_match"
enum banned_raw_pipeline_entity_match_update_column {
  # column name
  date_changed

  # column name
  id

  # column name
  raw_entity_id_a

  # column name
  raw_entity_id_b

  # column name
  user_id
}

input banned_raw_pipeline_entity_match_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: banned_raw_pipeline_entity_match_inc_input

  # sets the columns of the filtered rows to the given values
  _set: banned_raw_pipeline_entity_match_set_input
  where: banned_raw_pipeline_entity_match_bool_exp!
}

# aggregate var_pop on columns
type banned_raw_pipeline_entity_match_var_pop_fields {
  id: Float
  raw_entity_id_a: Float
  raw_entity_id_b: Float
  user_id: Float
}

# order by var_pop() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_var_pop_order_by {
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# aggregate var_samp on columns
type banned_raw_pipeline_entity_match_var_samp_fields {
  id: Float
  raw_entity_id_a: Float
  raw_entity_id_b: Float
  user_id: Float
}

# order by var_samp() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_var_samp_order_by {
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# aggregate variance on columns
type banned_raw_pipeline_entity_match_variance_fields {
  id: Float
  raw_entity_id_a: Float
  raw_entity_id_b: Float
  user_id: Float
}

# order by variance() on columns of table "banned_raw_pipeline_entity_match"
input banned_raw_pipeline_entity_match_variance_order_by {
  id: order_by
  raw_entity_id_a: order_by
  raw_entity_id_b: order_by
  user_id: order_by
}

# A Relay connection object on "banned_raw_pipeline_entity_match"
type banned_raw_pipeline_entity_matchConnection {
  edges: [banned_raw_pipeline_entity_matchEdge!]!
  pageInfo: PageInfo!
}

type banned_raw_pipeline_entity_matchEdge {
  cursor: String!
  node: banned_raw_pipeline_entity_match!
}

# Boolean expression to compare columns of type "Boolean". All fields are combined with logical 'AND'.
input Boolean_comparison_exp {
  _eq: Boolean
  _gt: Boolean
  _gte: Boolean
  _in: [Boolean!]
  _is_null: Boolean
  _lt: Boolean
  _lte: Boolean
  _neq: Boolean
  _nin: [Boolean!]
}

# columns and relationships of "canonical_pipeline_entity"
type canonical_pipeline_entity implements Node {
  canonical_id: String!
  entity_metadata(
    # JSON select path
    path: String
  ): jsonb!
  entity_type_id: Int!
  id: ID!
  in_latest_datasource: Boolean!

  # An array relationship
  pipeline_entity_matches(
    # distinct select on columns
    distinct_on: [pipeline_entity_match_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_match_order_by!]

    # filter the rows returned
    where: pipeline_entity_match_bool_exp
  ): [pipeline_entity_match!]!

  # An aggregate relationship
  pipeline_entity_matches_aggregate(
    # distinct select on columns
    distinct_on: [pipeline_entity_match_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_match_order_by!]

    # filter the rows returned
    where: pipeline_entity_match_bool_exp
  ): pipeline_entity_match_aggregate!

  # An array relationship connection
  pipeline_entity_matches_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_entity_match_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_match_order_by!]

    # filter the rows returned
    where: pipeline_entity_match_bool_exp
  ): pipeline_entity_matchConnection!

  # An object relationship
  pipeline_entity_type: pipeline_entity_type!
  search_term: String
}

# aggregated selection of "canonical_pipeline_entity"
type canonical_pipeline_entity_aggregate {
  aggregate: canonical_pipeline_entity_aggregate_fields
  nodes: [canonical_pipeline_entity!]!
}

# aggregate fields of "canonical_pipeline_entity"
type canonical_pipeline_entity_aggregate_fields {
  avg: canonical_pipeline_entity_avg_fields
  count(columns: [canonical_pipeline_entity_select_column!], distinct: Boolean): Int!
  max: canonical_pipeline_entity_max_fields
  min: canonical_pipeline_entity_min_fields
  stddev: canonical_pipeline_entity_stddev_fields
  stddev_pop: canonical_pipeline_entity_stddev_pop_fields
  stddev_samp: canonical_pipeline_entity_stddev_samp_fields
  sum: canonical_pipeline_entity_sum_fields
  var_pop: canonical_pipeline_entity_var_pop_fields
  var_samp: canonical_pipeline_entity_var_samp_fields
  variance: canonical_pipeline_entity_variance_fields
}

# order by aggregate values of table "canonical_pipeline_entity"
input canonical_pipeline_entity_aggregate_order_by {
  avg: canonical_pipeline_entity_avg_order_by
  count: order_by
  max: canonical_pipeline_entity_max_order_by
  min: canonical_pipeline_entity_min_order_by
  stddev: canonical_pipeline_entity_stddev_order_by
  stddev_pop: canonical_pipeline_entity_stddev_pop_order_by
  stddev_samp: canonical_pipeline_entity_stddev_samp_order_by
  sum: canonical_pipeline_entity_sum_order_by
  var_pop: canonical_pipeline_entity_var_pop_order_by
  var_samp: canonical_pipeline_entity_var_samp_order_by
  variance: canonical_pipeline_entity_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input canonical_pipeline_entity_append_input {
  entity_metadata: jsonb
}

# input type for inserting array relation for remote table "canonical_pipeline_entity"
input canonical_pipeline_entity_arr_rel_insert_input {
  data: [canonical_pipeline_entity_insert_input!]!

  # upsert condition
  on_conflict: canonical_pipeline_entity_on_conflict
}

# aggregate avg on columns
type canonical_pipeline_entity_avg_fields {
  entity_type_id: Float
  id: Float
}

# order by avg() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_avg_order_by {
  entity_type_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table "canonical_pipeline_entity". All fields are combined with a logical 'AND'.
input canonical_pipeline_entity_bool_exp {
  _and: [canonical_pipeline_entity_bool_exp!]
  _not: canonical_pipeline_entity_bool_exp
  _or: [canonical_pipeline_entity_bool_exp!]
  canonical_id: String_comparison_exp
  entity_metadata: jsonb_comparison_exp
  entity_type_id: Int_comparison_exp
  id: Int_comparison_exp
  in_latest_datasource: Boolean_comparison_exp
  pipeline_entity_matches: pipeline_entity_match_bool_exp
  pipeline_entity_type: pipeline_entity_type_bool_exp
  search_term: String_comparison_exp
}

# unique or primary key constraints on table "canonical_pipeline_entity"
enum canonical_pipeline_entity_constraint {
  # unique or primary key constraint on columns "id"
  canonical_pipeline_entity_pkey

  # unique or primary key constraint on columns "canonical_id", "entity_type_id"
  unique_canonical_pipeline_entity
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input canonical_pipeline_entity_delete_at_path_input {
  entity_metadata: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input canonical_pipeline_entity_delete_elem_input {
  entity_metadata: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input canonical_pipeline_entity_delete_key_input {
  entity_metadata: String
}

# input type for incrementing numeric columns in table "canonical_pipeline_entity"
input canonical_pipeline_entity_inc_input {
  entity_type_id: Int
  id: Int
}

# input type for inserting data into table "canonical_pipeline_entity"
input canonical_pipeline_entity_insert_input {
  canonical_id: String
  entity_metadata: jsonb
  entity_type_id: Int
  id: Int
  in_latest_datasource: Boolean
  pipeline_entity_matches: pipeline_entity_match_arr_rel_insert_input
  pipeline_entity_type: pipeline_entity_type_obj_rel_insert_input
  search_term: String
}

# aggregate max on columns
type canonical_pipeline_entity_max_fields {
  canonical_id: String
  entity_type_id: Int
  id: Int
  search_term: String
}

# order by max() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_max_order_by {
  canonical_id: order_by
  entity_type_id: order_by
  id: order_by
  search_term: order_by
}

# aggregate min on columns
type canonical_pipeline_entity_min_fields {
  canonical_id: String
  entity_type_id: Int
  id: Int
  search_term: String
}

# order by min() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_min_order_by {
  canonical_id: order_by
  entity_type_id: order_by
  id: order_by
  search_term: order_by
}

# response of any mutation on the table "canonical_pipeline_entity"
type canonical_pipeline_entity_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [canonical_pipeline_entity!]!
}

# input type for inserting object relation for remote table "canonical_pipeline_entity"
input canonical_pipeline_entity_obj_rel_insert_input {
  data: canonical_pipeline_entity_insert_input!

  # upsert condition
  on_conflict: canonical_pipeline_entity_on_conflict
}

# on_conflict condition type for table "canonical_pipeline_entity"
input canonical_pipeline_entity_on_conflict {
  constraint: canonical_pipeline_entity_constraint!
  update_columns: [canonical_pipeline_entity_update_column!]! = []
  where: canonical_pipeline_entity_bool_exp
}

# Ordering options when selecting data from "canonical_pipeline_entity".
input canonical_pipeline_entity_order_by {
  canonical_id: order_by
  entity_metadata: order_by
  entity_type_id: order_by
  id: order_by
  in_latest_datasource: order_by
  pipeline_entity_matches_aggregate: pipeline_entity_match_aggregate_order_by
  pipeline_entity_type: pipeline_entity_type_order_by
  search_term: order_by
}

# primary key columns input for table: canonical_pipeline_entity
input canonical_pipeline_entity_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input canonical_pipeline_entity_prepend_input {
  entity_metadata: jsonb
}

# select columns of table "canonical_pipeline_entity"
enum canonical_pipeline_entity_select_column {
  # column name
  canonical_id

  # column name
  entity_metadata

  # column name
  entity_type_id

  # column name
  id

  # column name
  in_latest_datasource

  # column name
  search_term
}

# input type for updating data in table "canonical_pipeline_entity"
input canonical_pipeline_entity_set_input {
  canonical_id: String
  entity_metadata: jsonb
  entity_type_id: Int
  id: Int
  in_latest_datasource: Boolean
  search_term: String
}

# aggregate stddev on columns
type canonical_pipeline_entity_stddev_fields {
  entity_type_id: Float
  id: Float
}

# order by stddev() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_stddev_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type canonical_pipeline_entity_stddev_pop_fields {
  entity_type_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_stddev_pop_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type canonical_pipeline_entity_stddev_samp_fields {
  entity_type_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_stddev_samp_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate sum on columns
type canonical_pipeline_entity_sum_fields {
  entity_type_id: Int
  id: Int
}

# order by sum() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_sum_order_by {
  entity_type_id: order_by
  id: order_by
}

# update columns of table "canonical_pipeline_entity"
enum canonical_pipeline_entity_update_column {
  # column name
  canonical_id

  # column name
  entity_metadata

  # column name
  entity_type_id

  # column name
  id

  # column name
  in_latest_datasource

  # column name
  search_term
}

input canonical_pipeline_entity_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: canonical_pipeline_entity_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: canonical_pipeline_entity_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: canonical_pipeline_entity_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: canonical_pipeline_entity_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: canonical_pipeline_entity_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: canonical_pipeline_entity_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: canonical_pipeline_entity_set_input
  where: canonical_pipeline_entity_bool_exp!
}

# aggregate var_pop on columns
type canonical_pipeline_entity_var_pop_fields {
  entity_type_id: Float
  id: Float
}

# order by var_pop() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_var_pop_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type canonical_pipeline_entity_var_samp_fields {
  entity_type_id: Float
  id: Float
}

# order by var_samp() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_var_samp_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate variance on columns
type canonical_pipeline_entity_variance_fields {
  entity_type_id: Float
  id: Float
}

# order by variance() on columns of table "canonical_pipeline_entity"
input canonical_pipeline_entity_variance_order_by {
  entity_type_id: order_by
  id: order_by
}

# A Relay connection object on "canonical_pipeline_entity"
type canonical_pipeline_entityConnection {
  edges: [canonical_pipeline_entityEdge!]!
  pageInfo: PageInfo!
}

type canonical_pipeline_entityEdge {
  cursor: String!
  node: canonical_pipeline_entity!
}

# columns and relationships of "case"
type case implements Node {
  # An object relationship
  alert_notification: alert_notifications
  alert_notification_id: Int

  # An array relationship
  case_events(
    # distinct select on columns
    distinct_on: [case_event_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_event_order_by!]

    # filter the rows returned
    where: case_event_bool_exp
  ): [case_event!]!

  # An aggregate relationship
  case_events_aggregate(
    # distinct select on columns
    distinct_on: [case_event_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_event_order_by!]

    # filter the rows returned
    where: case_event_bool_exp
  ): case_event_aggregate!

  # An array relationship connection
  case_events_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_event_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_event_order_by!]

    # filter the rows returned
    where: case_event_bool_exp
  ): case_eventConnection!

  # An array relationship
  case_metadata(
    # distinct select on columns
    distinct_on: [case_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_order_by!]

    # filter the rows returned
    where: case_metadata_bool_exp
  ): [case_metadata!]!

  # An aggregate relationship
  case_metadata_aggregate(
    # distinct select on columns
    distinct_on: [case_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_order_by!]

    # filter the rows returned
    where: case_metadata_bool_exp
  ): case_metadata_aggregate!

  # An array relationship connection
  case_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_order_by!]

    # filter the rows returned
    where: case_metadata_bool_exp
  ): case_metadataConnection!

  # An object relationship
  case_status_type: case_status_type!
  case_status_type_id: Int!

  # An object relationship
  case_type: case_type!
  case_type_id: Int!
  id: ID!
  primary_druid_dimension_values(
    # JSON select path
    path: String
  ): jsonb
  spec(
    # JSON select path
    path: String
  ): jsonb
}

# aggregated selection of "case"
type case_aggregate {
  aggregate: case_aggregate_fields
  nodes: [case!]!
}

# aggregate fields of "case"
type case_aggregate_fields {
  avg: case_avg_fields
  count(columns: [case_select_column!], distinct: Boolean): Int!
  max: case_max_fields
  min: case_min_fields
  stddev: case_stddev_fields
  stddev_pop: case_stddev_pop_fields
  stddev_samp: case_stddev_samp_fields
  sum: case_sum_fields
  var_pop: case_var_pop_fields
  var_samp: case_var_samp_fields
  variance: case_variance_fields
}

# order by aggregate values of table "case"
input case_aggregate_order_by {
  avg: case_avg_order_by
  count: order_by
  max: case_max_order_by
  min: case_min_order_by
  stddev: case_stddev_order_by
  stddev_pop: case_stddev_pop_order_by
  stddev_samp: case_stddev_samp_order_by
  sum: case_sum_order_by
  var_pop: case_var_pop_order_by
  var_samp: case_var_samp_order_by
  variance: case_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input case_append_input {
  primary_druid_dimension_values: jsonb
  spec: jsonb
}

# input type for inserting array relation for remote table "case"
input case_arr_rel_insert_input {
  data: [case_insert_input!]!

  # upsert condition
  on_conflict: case_on_conflict
}

# aggregate avg on columns
type case_avg_fields {
  alert_notification_id: Float
  case_status_type_id: Float
  case_type_id: Float
  id: Float
}

# order by avg() on columns of table "case"
input case_avg_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table "case". All fields are combined with a logical 'AND'.
input case_bool_exp {
  _and: [case_bool_exp!]
  _not: case_bool_exp
  _or: [case_bool_exp!]
  alert_notification: alert_notifications_bool_exp
  alert_notification_id: Int_comparison_exp
  case_events: case_event_bool_exp
  case_metadata: case_metadata_bool_exp
  case_status_type: case_status_type_bool_exp
  case_status_type_id: Int_comparison_exp
  case_type: case_type_bool_exp
  case_type_id: Int_comparison_exp
  id: Int_comparison_exp
  primary_druid_dimension_values: jsonb_comparison_exp
  spec: jsonb_comparison_exp
}

# unique or primary key constraints on table "case"
enum case_constraint {
  # unique or primary key constraint on columns "id"
  case_pkey

  # unique or primary key constraint on columns "alert_notification_id"
  unique_alert_notification_id
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input case_delete_at_path_input {
  primary_druid_dimension_values: [String!]
  spec: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input case_delete_elem_input {
  primary_druid_dimension_values: Int
  spec: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input case_delete_key_input {
  primary_druid_dimension_values: String
  spec: String
}

# columns and relationships of "case_event"
type case_event implements Node {
  additional_info(
    # JSON select path
    path: String
  ): jsonb!

  # An object relationship
  case: case
  case_id: Int

  # An array relationship
  case_type_default_events(
    # distinct select on columns
    distinct_on: [case_type_default_event_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_event_order_by!]

    # filter the rows returned
    where: case_type_default_event_bool_exp
  ): [case_type_default_event!]!

  # An aggregate relationship
  case_type_default_events_aggregate(
    # distinct select on columns
    distinct_on: [case_type_default_event_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_event_order_by!]

    # filter the rows returned
    where: case_type_default_event_bool_exp
  ): case_type_default_event_aggregate!

  # An array relationship connection
  case_type_default_events_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_event_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_event_order_by!]

    # filter the rows returned
    where: case_type_default_event_bool_exp
  ): case_type_default_eventConnection!
  created: timestamp!
  description: String
  id: ID!
  name: String!
  type: event_type_enum!
}

# aggregated selection of "case_event"
type case_event_aggregate {
  aggregate: case_event_aggregate_fields
  nodes: [case_event!]!
}

# aggregate fields of "case_event"
type case_event_aggregate_fields {
  avg: case_event_avg_fields
  count(columns: [case_event_select_column!], distinct: Boolean): Int!
  max: case_event_max_fields
  min: case_event_min_fields
  stddev: case_event_stddev_fields
  stddev_pop: case_event_stddev_pop_fields
  stddev_samp: case_event_stddev_samp_fields
  sum: case_event_sum_fields
  var_pop: case_event_var_pop_fields
  var_samp: case_event_var_samp_fields
  variance: case_event_variance_fields
}

# order by aggregate values of table "case_event"
input case_event_aggregate_order_by {
  avg: case_event_avg_order_by
  count: order_by
  max: case_event_max_order_by
  min: case_event_min_order_by
  stddev: case_event_stddev_order_by
  stddev_pop: case_event_stddev_pop_order_by
  stddev_samp: case_event_stddev_samp_order_by
  sum: case_event_sum_order_by
  var_pop: case_event_var_pop_order_by
  var_samp: case_event_var_samp_order_by
  variance: case_event_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input case_event_append_input {
  additional_info: jsonb
}

# input type for inserting array relation for remote table "case_event"
input case_event_arr_rel_insert_input {
  data: [case_event_insert_input!]!

  # upsert condition
  on_conflict: case_event_on_conflict
}

# aggregate avg on columns
type case_event_avg_fields {
  case_id: Float
  id: Float
}

# order by avg() on columns of table "case_event"
input case_event_avg_order_by {
  case_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table "case_event". All fields are combined with a logical 'AND'.
input case_event_bool_exp {
  _and: [case_event_bool_exp!]
  _not: case_event_bool_exp
  _or: [case_event_bool_exp!]
  additional_info: jsonb_comparison_exp
  case: case_bool_exp
  case_id: Int_comparison_exp
  case_type_default_events: case_type_default_event_bool_exp
  created: timestamp_comparison_exp
  description: String_comparison_exp
  id: Int_comparison_exp
  name: String_comparison_exp
  type: event_type_enum_comparison_exp
}

# unique or primary key constraints on table "case_event"
enum case_event_constraint {
  # unique or primary key constraint on columns "id"
  case_event_pkey
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input case_event_delete_at_path_input {
  additional_info: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input case_event_delete_elem_input {
  additional_info: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input case_event_delete_key_input {
  additional_info: String
}

# input type for incrementing numeric columns in table "case_event"
input case_event_inc_input {
  case_id: Int
  id: Int
}

# input type for inserting data into table "case_event"
input case_event_insert_input {
  additional_info: jsonb
  case: case_obj_rel_insert_input
  case_id: Int
  case_type_default_events: case_type_default_event_arr_rel_insert_input
  created: timestamp
  description: String
  id: Int
  name: String
  type: event_type_enum
}

# aggregate max on columns
type case_event_max_fields {
  case_id: Int
  created: timestamp
  description: String
  id: Int
  name: String
  type: event_type_enum
}

# order by max() on columns of table "case_event"
input case_event_max_order_by {
  case_id: order_by
  created: order_by
  description: order_by
  id: order_by
  name: order_by
  type: order_by
}

# aggregate min on columns
type case_event_min_fields {
  case_id: Int
  created: timestamp
  description: String
  id: Int
  name: String
  type: event_type_enum
}

# order by min() on columns of table "case_event"
input case_event_min_order_by {
  case_id: order_by
  created: order_by
  description: order_by
  id: order_by
  name: order_by
  type: order_by
}

# response of any mutation on the table "case_event"
type case_event_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_event!]!
}

# input type for inserting object relation for remote table "case_event"
input case_event_obj_rel_insert_input {
  data: case_event_insert_input!

  # upsert condition
  on_conflict: case_event_on_conflict
}

# on_conflict condition type for table "case_event"
input case_event_on_conflict {
  constraint: case_event_constraint!
  update_columns: [case_event_update_column!]! = []
  where: case_event_bool_exp
}

# Ordering options when selecting data from "case_event".
input case_event_order_by {
  additional_info: order_by
  case: case_order_by
  case_id: order_by
  case_type_default_events_aggregate: case_type_default_event_aggregate_order_by
  created: order_by
  description: order_by
  id: order_by
  name: order_by
  type: order_by
}

# primary key columns input for table: case_event
input case_event_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input case_event_prepend_input {
  additional_info: jsonb
}

# select columns of table "case_event"
enum case_event_select_column {
  # column name
  additional_info

  # column name
  case_id

  # column name
  created

  # column name
  description

  # column name
  id

  # column name
  name

  # column name
  type
}

# input type for updating data in table "case_event"
input case_event_set_input {
  additional_info: jsonb
  case_id: Int
  created: timestamp
  description: String
  id: Int
  name: String
  type: event_type_enum
}

# aggregate stddev on columns
type case_event_stddev_fields {
  case_id: Float
  id: Float
}

# order by stddev() on columns of table "case_event"
input case_event_stddev_order_by {
  case_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type case_event_stddev_pop_fields {
  case_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "case_event"
input case_event_stddev_pop_order_by {
  case_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type case_event_stddev_samp_fields {
  case_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "case_event"
input case_event_stddev_samp_order_by {
  case_id: order_by
  id: order_by
}

# aggregate sum on columns
type case_event_sum_fields {
  case_id: Int
  id: Int
}

# order by sum() on columns of table "case_event"
input case_event_sum_order_by {
  case_id: order_by
  id: order_by
}

# update columns of table "case_event"
enum case_event_update_column {
  # column name
  additional_info

  # column name
  case_id

  # column name
  created

  # column name
  description

  # column name
  id

  # column name
  name

  # column name
  type
}

input case_event_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: case_event_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: case_event_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: case_event_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: case_event_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: case_event_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: case_event_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: case_event_set_input
  where: case_event_bool_exp!
}

# aggregate var_pop on columns
type case_event_var_pop_fields {
  case_id: Float
  id: Float
}

# order by var_pop() on columns of table "case_event"
input case_event_var_pop_order_by {
  case_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type case_event_var_samp_fields {
  case_id: Float
  id: Float
}

# order by var_samp() on columns of table "case_event"
input case_event_var_samp_order_by {
  case_id: order_by
  id: order_by
}

# aggregate variance on columns
type case_event_variance_fields {
  case_id: Float
  id: Float
}

# order by variance() on columns of table "case_event"
input case_event_variance_order_by {
  case_id: order_by
  id: order_by
}

# A Relay connection object on "case_event"
type case_eventConnection {
  edges: [case_eventEdge!]!
  pageInfo: PageInfo!
}

type case_eventEdge {
  cursor: String!
  node: case_event!
}

# input type for incrementing numeric columns in table "case"
input case_inc_input {
  alert_notification_id: Int
  case_status_type_id: Int
  case_type_id: Int
  id: Int
}

# input type for inserting data into table "case"
input case_insert_input {
  alert_notification: alert_notifications_obj_rel_insert_input
  alert_notification_id: Int
  case_events: case_event_arr_rel_insert_input
  case_metadata: case_metadata_arr_rel_insert_input
  case_status_type: case_status_type_obj_rel_insert_input
  case_status_type_id: Int
  case_type: case_type_obj_rel_insert_input
  case_type_id: Int
  id: Int
  primary_druid_dimension_values: jsonb
  spec: jsonb
}

# aggregate max on columns
type case_max_fields {
  alert_notification_id: Int
  case_status_type_id: Int
  case_type_id: Int
  id: Int
}

# order by max() on columns of table "case"
input case_max_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# columns and relationships of "case_metadata"
type case_metadata implements Node {
  # An object relationship
  case: case!
  case_id: Int!

  # An object relationship
  case_metadata_type: case_metadata_type!
  case_metadata_type_id: Int!
  id: ID!
  value: String!
}

# aggregated selection of "case_metadata"
type case_metadata_aggregate {
  aggregate: case_metadata_aggregate_fields
  nodes: [case_metadata!]!
}

# aggregate fields of "case_metadata"
type case_metadata_aggregate_fields {
  avg: case_metadata_avg_fields
  count(columns: [case_metadata_select_column!], distinct: Boolean): Int!
  max: case_metadata_max_fields
  min: case_metadata_min_fields
  stddev: case_metadata_stddev_fields
  stddev_pop: case_metadata_stddev_pop_fields
  stddev_samp: case_metadata_stddev_samp_fields
  sum: case_metadata_sum_fields
  var_pop: case_metadata_var_pop_fields
  var_samp: case_metadata_var_samp_fields
  variance: case_metadata_variance_fields
}

# order by aggregate values of table "case_metadata"
input case_metadata_aggregate_order_by {
  avg: case_metadata_avg_order_by
  count: order_by
  max: case_metadata_max_order_by
  min: case_metadata_min_order_by
  stddev: case_metadata_stddev_order_by
  stddev_pop: case_metadata_stddev_pop_order_by
  stddev_samp: case_metadata_stddev_samp_order_by
  sum: case_metadata_sum_order_by
  var_pop: case_metadata_var_pop_order_by
  var_samp: case_metadata_var_samp_order_by
  variance: case_metadata_variance_order_by
}

# input type for inserting array relation for remote table "case_metadata"
input case_metadata_arr_rel_insert_input {
  data: [case_metadata_insert_input!]!

  # upsert condition
  on_conflict: case_metadata_on_conflict
}

# aggregate avg on columns
type case_metadata_avg_fields {
  case_id: Float
  case_metadata_type_id: Float
}

# order by avg() on columns of table "case_metadata"
input case_metadata_avg_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
}

# Boolean expression to filter rows from the table "case_metadata". All fields are combined with a logical 'AND'.
input case_metadata_bool_exp {
  _and: [case_metadata_bool_exp!]
  _not: case_metadata_bool_exp
  _or: [case_metadata_bool_exp!]
  case: case_bool_exp
  case_id: Int_comparison_exp
  case_metadata_type: case_metadata_type_bool_exp
  case_metadata_type_id: Int_comparison_exp
  value: String_comparison_exp
}

# unique or primary key constraints on table "case_metadata"
enum case_metadata_constraint {
  # unique or primary key constraint on columns "case_id", "case_metadata_type_id"
  case_metadata_pkey
}

# input type for incrementing numeric columns in table "case_metadata"
input case_metadata_inc_input {
  case_id: Int
  case_metadata_type_id: Int
}

# input type for inserting data into table "case_metadata"
input case_metadata_insert_input {
  case: case_obj_rel_insert_input
  case_id: Int
  case_metadata_type: case_metadata_type_obj_rel_insert_input
  case_metadata_type_id: Int
  value: String
}

# aggregate max on columns
type case_metadata_max_fields {
  case_id: Int
  case_metadata_type_id: Int
  value: String
}

# order by max() on columns of table "case_metadata"
input case_metadata_max_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
  value: order_by
}

# aggregate min on columns
type case_metadata_min_fields {
  case_id: Int
  case_metadata_type_id: Int
  value: String
}

# order by min() on columns of table "case_metadata"
input case_metadata_min_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
  value: order_by
}

# response of any mutation on the table "case_metadata"
type case_metadata_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_metadata!]!
}

# on_conflict condition type for table "case_metadata"
input case_metadata_on_conflict {
  constraint: case_metadata_constraint!
  update_columns: [case_metadata_update_column!]! = []
  where: case_metadata_bool_exp
}

# Ordering options when selecting data from "case_metadata".
input case_metadata_order_by {
  case: case_order_by
  case_id: order_by
  case_metadata_type: case_metadata_type_order_by
  case_metadata_type_id: order_by
  value: order_by
}

# primary key columns input for table: case_metadata
input case_metadata_pk_columns_input {
  case_id: Int!
  case_metadata_type_id: Int!
}

# select columns of table "case_metadata"
enum case_metadata_select_column {
  # column name
  case_id

  # column name
  case_metadata_type_id

  # column name
  value
}

# input type for updating data in table "case_metadata"
input case_metadata_set_input {
  case_id: Int
  case_metadata_type_id: Int
  value: String
}

# aggregate stddev on columns
type case_metadata_stddev_fields {
  case_id: Float
  case_metadata_type_id: Float
}

# order by stddev() on columns of table "case_metadata"
input case_metadata_stddev_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
}

# aggregate stddev_pop on columns
type case_metadata_stddev_pop_fields {
  case_id: Float
  case_metadata_type_id: Float
}

# order by stddev_pop() on columns of table "case_metadata"
input case_metadata_stddev_pop_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
}

# aggregate stddev_samp on columns
type case_metadata_stddev_samp_fields {
  case_id: Float
  case_metadata_type_id: Float
}

# order by stddev_samp() on columns of table "case_metadata"
input case_metadata_stddev_samp_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
}

# aggregate sum on columns
type case_metadata_sum_fields {
  case_id: Int
  case_metadata_type_id: Int
}

# order by sum() on columns of table "case_metadata"
input case_metadata_sum_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
}

# columns and relationships of "case_metadata_type"
type case_metadata_type implements Node {
  # An array relationship
  case_metadata(
    # distinct select on columns
    distinct_on: [case_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_order_by!]

    # filter the rows returned
    where: case_metadata_bool_exp
  ): [case_metadata!]!

  # An aggregate relationship
  case_metadata_aggregate(
    # distinct select on columns
    distinct_on: [case_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_order_by!]

    # filter the rows returned
    where: case_metadata_bool_exp
  ): case_metadata_aggregate!

  # An array relationship connection
  case_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_order_by!]

    # filter the rows returned
    where: case_metadata_bool_exp
  ): case_metadataConnection!

  # An object relationship
  case_type: case_type!
  case_type_id: Int!
  dossier_section: String
  empty_display_value: String!
  id: ID!
  is_displayed_empty: Boolean!
  is_editable: Boolean!
  name: String!
  type: metadata_type_enum!
}

# aggregated selection of "case_metadata_type"
type case_metadata_type_aggregate {
  aggregate: case_metadata_type_aggregate_fields
  nodes: [case_metadata_type!]!
}

# aggregate fields of "case_metadata_type"
type case_metadata_type_aggregate_fields {
  avg: case_metadata_type_avg_fields
  count(columns: [case_metadata_type_select_column!], distinct: Boolean): Int!
  max: case_metadata_type_max_fields
  min: case_metadata_type_min_fields
  stddev: case_metadata_type_stddev_fields
  stddev_pop: case_metadata_type_stddev_pop_fields
  stddev_samp: case_metadata_type_stddev_samp_fields
  sum: case_metadata_type_sum_fields
  var_pop: case_metadata_type_var_pop_fields
  var_samp: case_metadata_type_var_samp_fields
  variance: case_metadata_type_variance_fields
}

# order by aggregate values of table "case_metadata_type"
input case_metadata_type_aggregate_order_by {
  avg: case_metadata_type_avg_order_by
  count: order_by
  max: case_metadata_type_max_order_by
  min: case_metadata_type_min_order_by
  stddev: case_metadata_type_stddev_order_by
  stddev_pop: case_metadata_type_stddev_pop_order_by
  stddev_samp: case_metadata_type_stddev_samp_order_by
  sum: case_metadata_type_sum_order_by
  var_pop: case_metadata_type_var_pop_order_by
  var_samp: case_metadata_type_var_samp_order_by
  variance: case_metadata_type_variance_order_by
}

# input type for inserting array relation for remote table "case_metadata_type"
input case_metadata_type_arr_rel_insert_input {
  data: [case_metadata_type_insert_input!]!

  # upsert condition
  on_conflict: case_metadata_type_on_conflict
}

# aggregate avg on columns
type case_metadata_type_avg_fields {
  case_type_id: Float
  id: Float
}

# order by avg() on columns of table "case_metadata_type"
input case_metadata_type_avg_order_by {
  case_type_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table "case_metadata_type". All fields are combined with a logical 'AND'.
input case_metadata_type_bool_exp {
  _and: [case_metadata_type_bool_exp!]
  _not: case_metadata_type_bool_exp
  _or: [case_metadata_type_bool_exp!]
  case_metadata: case_metadata_bool_exp
  case_type: case_type_bool_exp
  case_type_id: Int_comparison_exp
  dossier_section: String_comparison_exp
  empty_display_value: String_comparison_exp
  id: Int_comparison_exp
  is_displayed_empty: Boolean_comparison_exp
  is_editable: Boolean_comparison_exp
  name: String_comparison_exp
  type: metadata_type_enum_comparison_exp
}

# unique or primary key constraints on table "case_metadata_type"
enum case_metadata_type_constraint {
  # unique or primary key constraint on columns "id"
  case_metadata_type_pkey
}

# input type for incrementing numeric columns in table "case_metadata_type"
input case_metadata_type_inc_input {
  case_type_id: Int
  id: Int
}

# input type for inserting data into table "case_metadata_type"
input case_metadata_type_insert_input {
  case_metadata: case_metadata_arr_rel_insert_input
  case_type: case_type_obj_rel_insert_input
  case_type_id: Int
  dossier_section: String
  empty_display_value: String
  id: Int
  is_displayed_empty: Boolean
  is_editable: Boolean
  name: String
  type: metadata_type_enum
}

# aggregate max on columns
type case_metadata_type_max_fields {
  case_type_id: Int
  dossier_section: String
  empty_display_value: String
  id: Int
  name: String
  type: metadata_type_enum
}

# order by max() on columns of table "case_metadata_type"
input case_metadata_type_max_order_by {
  case_type_id: order_by
  dossier_section: order_by
  empty_display_value: order_by
  id: order_by
  name: order_by
  type: order_by
}

# aggregate min on columns
type case_metadata_type_min_fields {
  case_type_id: Int
  dossier_section: String
  empty_display_value: String
  id: Int
  name: String
  type: metadata_type_enum
}

# order by min() on columns of table "case_metadata_type"
input case_metadata_type_min_order_by {
  case_type_id: order_by
  dossier_section: order_by
  empty_display_value: order_by
  id: order_by
  name: order_by
  type: order_by
}

# response of any mutation on the table "case_metadata_type"
type case_metadata_type_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_metadata_type!]!
}

# input type for inserting object relation for remote table "case_metadata_type"
input case_metadata_type_obj_rel_insert_input {
  data: case_metadata_type_insert_input!

  # upsert condition
  on_conflict: case_metadata_type_on_conflict
}

# on_conflict condition type for table "case_metadata_type"
input case_metadata_type_on_conflict {
  constraint: case_metadata_type_constraint!
  update_columns: [case_metadata_type_update_column!]! = []
  where: case_metadata_type_bool_exp
}

# Ordering options when selecting data from "case_metadata_type".
input case_metadata_type_order_by {
  case_metadata_aggregate: case_metadata_aggregate_order_by
  case_type: case_type_order_by
  case_type_id: order_by
  dossier_section: order_by
  empty_display_value: order_by
  id: order_by
  is_displayed_empty: order_by
  is_editable: order_by
  name: order_by
  type: order_by
}

# primary key columns input for table: case_metadata_type
input case_metadata_type_pk_columns_input {
  id: Int!
}

# select columns of table "case_metadata_type"
enum case_metadata_type_select_column {
  # column name
  case_type_id

  # column name
  dossier_section

  # column name
  empty_display_value

  # column name
  id

  # column name
  is_displayed_empty

  # column name
  is_editable

  # column name
  name

  # column name
  type
}

# input type for updating data in table "case_metadata_type"
input case_metadata_type_set_input {
  case_type_id: Int
  dossier_section: String
  empty_display_value: String
  id: Int
  is_displayed_empty: Boolean
  is_editable: Boolean
  name: String
  type: metadata_type_enum
}

# aggregate stddev on columns
type case_metadata_type_stddev_fields {
  case_type_id: Float
  id: Float
}

# order by stddev() on columns of table "case_metadata_type"
input case_metadata_type_stddev_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type case_metadata_type_stddev_pop_fields {
  case_type_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "case_metadata_type"
input case_metadata_type_stddev_pop_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type case_metadata_type_stddev_samp_fields {
  case_type_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "case_metadata_type"
input case_metadata_type_stddev_samp_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate sum on columns
type case_metadata_type_sum_fields {
  case_type_id: Int
  id: Int
}

# order by sum() on columns of table "case_metadata_type"
input case_metadata_type_sum_order_by {
  case_type_id: order_by
  id: order_by
}

# update columns of table "case_metadata_type"
enum case_metadata_type_update_column {
  # column name
  case_type_id

  # column name
  dossier_section

  # column name
  empty_display_value

  # column name
  id

  # column name
  is_displayed_empty

  # column name
  is_editable

  # column name
  name

  # column name
  type
}

input case_metadata_type_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: case_metadata_type_inc_input

  # sets the columns of the filtered rows to the given values
  _set: case_metadata_type_set_input
  where: case_metadata_type_bool_exp!
}

# aggregate var_pop on columns
type case_metadata_type_var_pop_fields {
  case_type_id: Float
  id: Float
}

# order by var_pop() on columns of table "case_metadata_type"
input case_metadata_type_var_pop_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type case_metadata_type_var_samp_fields {
  case_type_id: Float
  id: Float
}

# order by var_samp() on columns of table "case_metadata_type"
input case_metadata_type_var_samp_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate variance on columns
type case_metadata_type_variance_fields {
  case_type_id: Float
  id: Float
}

# order by variance() on columns of table "case_metadata_type"
input case_metadata_type_variance_order_by {
  case_type_id: order_by
  id: order_by
}

# A Relay connection object on "case_metadata_type"
type case_metadata_typeConnection {
  edges: [case_metadata_typeEdge!]!
  pageInfo: PageInfo!
}

type case_metadata_typeEdge {
  cursor: String!
  node: case_metadata_type!
}

# update columns of table "case_metadata"
enum case_metadata_update_column {
  # column name
  case_id

  # column name
  case_metadata_type_id

  # column name
  value
}

input case_metadata_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: case_metadata_inc_input

  # sets the columns of the filtered rows to the given values
  _set: case_metadata_set_input
  where: case_metadata_bool_exp!
}

# aggregate var_pop on columns
type case_metadata_var_pop_fields {
  case_id: Float
  case_metadata_type_id: Float
}

# order by var_pop() on columns of table "case_metadata"
input case_metadata_var_pop_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
}

# aggregate var_samp on columns
type case_metadata_var_samp_fields {
  case_id: Float
  case_metadata_type_id: Float
}

# order by var_samp() on columns of table "case_metadata"
input case_metadata_var_samp_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
}

# aggregate variance on columns
type case_metadata_variance_fields {
  case_id: Float
  case_metadata_type_id: Float
}

# order by variance() on columns of table "case_metadata"
input case_metadata_variance_order_by {
  case_id: order_by
  case_metadata_type_id: order_by
}

# A Relay connection object on "case_metadata"
type case_metadataConnection {
  edges: [case_metadataEdge!]!
  pageInfo: PageInfo!
}

type case_metadataEdge {
  cursor: String!
  node: case_metadata!
}

# aggregate min on columns
type case_min_fields {
  alert_notification_id: Int
  case_status_type_id: Int
  case_type_id: Int
  id: Int
}

# order by min() on columns of table "case"
input case_min_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# response of any mutation on the table "case"
type case_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case!]!
}

# input type for inserting object relation for remote table "case"
input case_obj_rel_insert_input {
  data: case_insert_input!

  # upsert condition
  on_conflict: case_on_conflict
}

# on_conflict condition type for table "case"
input case_on_conflict {
  constraint: case_constraint!
  update_columns: [case_update_column!]! = []
  where: case_bool_exp
}

# Ordering options when selecting data from "case".
input case_order_by {
  alert_notification: alert_notifications_order_by
  alert_notification_id: order_by
  case_events_aggregate: case_event_aggregate_order_by
  case_metadata_aggregate: case_metadata_aggregate_order_by
  case_status_type: case_status_type_order_by
  case_status_type_id: order_by
  case_type: case_type_order_by
  case_type_id: order_by
  id: order_by
  primary_druid_dimension_values: order_by
  spec: order_by
}

# primary key columns input for table: case
input case_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input case_prepend_input {
  primary_druid_dimension_values: jsonb
  spec: jsonb
}

# select columns of table "case"
enum case_select_column {
  # column name
  alert_notification_id

  # column name
  case_status_type_id

  # column name
  case_type_id

  # column name
  id

  # column name
  primary_druid_dimension_values

  # column name
  spec
}

# input type for updating data in table "case"
input case_set_input {
  alert_notification_id: Int
  case_status_type_id: Int
  case_type_id: Int
  id: Int
  primary_druid_dimension_values: jsonb
  spec: jsonb
}

# columns and relationships of "case_status_type"
type case_status_type implements Node {
  # An array relationship
  case_type_default_statuses(
    # distinct select on columns
    distinct_on: [case_type_default_status_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_status_order_by!]

    # filter the rows returned
    where: case_type_default_status_bool_exp
  ): [case_type_default_status!]!

  # An aggregate relationship
  case_type_default_statuses_aggregate(
    # distinct select on columns
    distinct_on: [case_type_default_status_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_status_order_by!]

    # filter the rows returned
    where: case_type_default_status_bool_exp
  ): case_type_default_status_aggregate!

  # An array relationship connection
  case_type_default_statuses_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_status_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_status_order_by!]

    # filter the rows returned
    where: case_type_default_status_bool_exp
  ): case_type_default_statusConnection!

  # An array relationship
  case_types(
    # distinct select on columns
    distinct_on: [case_type_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_order_by!]

    # filter the rows returned
    where: case_type_bool_exp
  ): [case_type!]!

  # An aggregate relationship
  case_types_aggregate(
    # distinct select on columns
    distinct_on: [case_type_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_order_by!]

    # filter the rows returned
    where: case_type_bool_exp
  ): case_type_aggregate!

  # An array relationship connection
  case_types_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_order_by!]

    # filter the rows returned
    where: case_type_bool_exp
  ): case_typeConnection!

  # An array relationship
  cases(
    # distinct select on columns
    distinct_on: [case_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_order_by!]

    # filter the rows returned
    where: case_bool_exp
  ): [case!]!

  # An aggregate relationship
  cases_aggregate(
    # distinct select on columns
    distinct_on: [case_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_order_by!]

    # filter the rows returned
    where: case_bool_exp
  ): case_aggregate!

  # An array relationship connection
  cases_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_order_by!]

    # filter the rows returned
    where: case_bool_exp
  ): caseConnection!
  id: ID!
  is_new: Boolean
  is_open: Boolean
  name: String!
}

# Boolean expression to filter rows from the table "case_status_type". All fields are combined with a logical 'AND'.
input case_status_type_bool_exp {
  _and: [case_status_type_bool_exp!]
  _not: case_status_type_bool_exp
  _or: [case_status_type_bool_exp!]
  case_type_default_statuses: case_type_default_status_bool_exp
  case_types: case_type_bool_exp
  cases: case_bool_exp
  id: Int_comparison_exp
  is_new: Boolean_comparison_exp
  is_open: Boolean_comparison_exp
  name: String_comparison_exp
}

# unique or primary key constraints on table "case_status_type"
enum case_status_type_constraint {
  # unique or primary key constraint on columns "id"
  case_status_type_pkey
}

# input type for incrementing numeric columns in table "case_status_type"
input case_status_type_inc_input {
  id: Int
}

# input type for inserting data into table "case_status_type"
input case_status_type_insert_input {
  case_type_default_statuses: case_type_default_status_arr_rel_insert_input
  case_types: case_type_arr_rel_insert_input
  cases: case_arr_rel_insert_input
  id: Int
  is_new: Boolean
  is_open: Boolean
  name: String
}

# response of any mutation on the table "case_status_type"
type case_status_type_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_status_type!]!
}

# input type for inserting object relation for remote table "case_status_type"
input case_status_type_obj_rel_insert_input {
  data: case_status_type_insert_input!

  # upsert condition
  on_conflict: case_status_type_on_conflict
}

# on_conflict condition type for table "case_status_type"
input case_status_type_on_conflict {
  constraint: case_status_type_constraint!
  update_columns: [case_status_type_update_column!]! = []
  where: case_status_type_bool_exp
}

# Ordering options when selecting data from "case_status_type".
input case_status_type_order_by {
  case_type_default_statuses_aggregate: case_type_default_status_aggregate_order_by
  case_types_aggregate: case_type_aggregate_order_by
  cases_aggregate: case_aggregate_order_by
  id: order_by
  is_new: order_by
  is_open: order_by
  name: order_by
}

# primary key columns input for table: case_status_type
input case_status_type_pk_columns_input {
  id: Int!
}

# select columns of table "case_status_type"
enum case_status_type_select_column {
  # column name
  id

  # column name
  is_new

  # column name
  is_open

  # column name
  name
}

# input type for updating data in table "case_status_type"
input case_status_type_set_input {
  id: Int
  is_new: Boolean
  is_open: Boolean
  name: String
}

# update columns of table "case_status_type"
enum case_status_type_update_column {
  # column name
  id

  # column name
  is_new

  # column name
  is_open

  # column name
  name
}

input case_status_type_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: case_status_type_inc_input

  # sets the columns of the filtered rows to the given values
  _set: case_status_type_set_input
  where: case_status_type_bool_exp!
}

# A Relay connection object on "case_status_type"
type case_status_typeConnection {
  edges: [case_status_typeEdge!]!
  pageInfo: PageInfo!
}

type case_status_typeEdge {
  cursor: String!
  node: case_status_type!
}

# aggregate stddev on columns
type case_stddev_fields {
  alert_notification_id: Float
  case_status_type_id: Float
  case_type_id: Float
  id: Float
}

# order by stddev() on columns of table "case"
input case_stddev_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type case_stddev_pop_fields {
  alert_notification_id: Float
  case_status_type_id: Float
  case_type_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "case"
input case_stddev_pop_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type case_stddev_samp_fields {
  alert_notification_id: Float
  case_status_type_id: Float
  case_type_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "case"
input case_stddev_samp_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# aggregate sum on columns
type case_sum_fields {
  alert_notification_id: Int
  case_status_type_id: Int
  case_type_id: Int
  id: Int
}

# order by sum() on columns of table "case"
input case_sum_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# columns and relationships of "case_type"
type case_type implements Node {
  can_users_add_events: Boolean!

  # An object relationship
  caseTypeMetadataFromDruidDimensionByNamingDruidDimensionId: case_type_metadata_from_druid_dimension

  # An array relationship
  case_metadata_types(
    # distinct select on columns
    distinct_on: [case_metadata_type_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_type_order_by!]

    # filter the rows returned
    where: case_metadata_type_bool_exp
  ): [case_metadata_type!]!

  # An aggregate relationship
  case_metadata_types_aggregate(
    # distinct select on columns
    distinct_on: [case_metadata_type_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_type_order_by!]

    # filter the rows returned
    where: case_metadata_type_bool_exp
  ): case_metadata_type_aggregate!

  # An array relationship connection
  case_metadata_types_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_metadata_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_type_order_by!]

    # filter the rows returned
    where: case_metadata_type_bool_exp
  ): case_metadata_typeConnection!

  # An object relationship
  case_status_type: case_status_type!

  # An array relationship
  case_type_default_events(
    # distinct select on columns
    distinct_on: [case_type_default_event_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_event_order_by!]

    # filter the rows returned
    where: case_type_default_event_bool_exp
  ): [case_type_default_event!]!

  # An aggregate relationship
  case_type_default_events_aggregate(
    # distinct select on columns
    distinct_on: [case_type_default_event_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_event_order_by!]

    # filter the rows returned
    where: case_type_default_event_bool_exp
  ): case_type_default_event_aggregate!

  # An array relationship connection
  case_type_default_events_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_event_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_event_order_by!]

    # filter the rows returned
    where: case_type_default_event_bool_exp
  ): case_type_default_eventConnection!

  # An array relationship
  case_type_default_fields(
    # distinct select on columns
    distinct_on: [case_type_default_field_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_field_order_by!]

    # filter the rows returned
    where: case_type_default_field_bool_exp
  ): [case_type_default_field!]!

  # An aggregate relationship
  case_type_default_fields_aggregate(
    # distinct select on columns
    distinct_on: [case_type_default_field_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_field_order_by!]

    # filter the rows returned
    where: case_type_default_field_bool_exp
  ): case_type_default_field_aggregate!

  # An array relationship connection
  case_type_default_fields_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_field_order_by!]

    # filter the rows returned
    where: case_type_default_field_bool_exp
  ): case_type_default_fieldConnection!

  # An array relationship
  case_type_default_statuses(
    # distinct select on columns
    distinct_on: [case_type_default_status_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_status_order_by!]

    # filter the rows returned
    where: case_type_default_status_bool_exp
  ): [case_type_default_status!]!

  # An aggregate relationship
  case_type_default_statuses_aggregate(
    # distinct select on columns
    distinct_on: [case_type_default_status_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_status_order_by!]

    # filter the rows returned
    where: case_type_default_status_bool_exp
  ): case_type_default_status_aggregate!

  # An array relationship connection
  case_type_default_statuses_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_status_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_status_order_by!]

    # filter the rows returned
    where: case_type_default_status_bool_exp
  ): case_type_default_statusConnection!

  # An object relationship
  case_type_metadata_from_druid_dimension: case_type_metadata_from_druid_dimension

  # An array relationship
  case_type_metadata_from_druid_dimensions(
    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_dimension_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_dimension_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_dimension_bool_exp
  ): [case_type_metadata_from_druid_dimension!]!

  # An aggregate relationship
  case_type_metadata_from_druid_dimensions_aggregate(
    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_dimension_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_dimension_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_dimension_bool_exp
  ): case_type_metadata_from_druid_dimension_aggregate!

  # An array relationship connection
  case_type_metadata_from_druid_dimensions_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_dimension_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_dimension_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_dimension_bool_exp
  ): case_type_metadata_from_druid_dimensionConnection!

  # An array relationship
  case_type_metadata_from_druid_fields(
    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_field_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_field_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_field_bool_exp
  ): [case_type_metadata_from_druid_field!]!

  # An aggregate relationship
  case_type_metadata_from_druid_fields_aggregate(
    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_field_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_field_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_field_bool_exp
  ): case_type_metadata_from_druid_field_aggregate!

  # An array relationship connection
  case_type_metadata_from_druid_fields_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_field_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_field_bool_exp
  ): case_type_metadata_from_druid_fieldConnection!

  # An array relationship
  cases(
    # distinct select on columns
    distinct_on: [case_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_order_by!]

    # filter the rows returned
    where: case_bool_exp
  ): [case!]!

  # An aggregate relationship
  cases_aggregate(
    # distinct select on columns
    distinct_on: [case_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_order_by!]

    # filter the rows returned
    where: case_bool_exp
  ): case_aggregate!

  # An array relationship connection
  cases_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_order_by!]

    # filter the rows returned
    where: case_bool_exp
  ): caseConnection!
  default_case_status_type_id: Int!
  default_dashboard_queries(
    # JSON select path
    path: String
  ): jsonb!

  # An array relationship
  external_alert_types(
    # distinct select on columns
    distinct_on: [external_alert_type_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [external_alert_type_order_by!]

    # filter the rows returned
    where: external_alert_type_bool_exp
  ): [external_alert_type!]!

  # An aggregate relationship
  external_alert_types_aggregate(
    # distinct select on columns
    distinct_on: [external_alert_type_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [external_alert_type_order_by!]

    # filter the rows returned
    where: external_alert_type_bool_exp
  ): external_alert_type_aggregate!

  # An array relationship connection
  external_alert_types_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [external_alert_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [external_alert_type_order_by!]

    # filter the rows returned
    where: external_alert_type_bool_exp
  ): external_alert_typeConnection!
  id: ID!
  is_metadata_expandable: Boolean!
  naming_druid_dimension_id: Int
  primary_druid_dimension_id: Int
  show_case_type_in_dossier: Boolean
  spec(
    # JSON select path
    path: String
  ): jsonb
  type: case_type_enum!
}

# aggregated selection of "case_type"
type case_type_aggregate {
  aggregate: case_type_aggregate_fields
  nodes: [case_type!]!
}

# aggregate fields of "case_type"
type case_type_aggregate_fields {
  avg: case_type_avg_fields
  count(columns: [case_type_select_column!], distinct: Boolean): Int!
  max: case_type_max_fields
  min: case_type_min_fields
  stddev: case_type_stddev_fields
  stddev_pop: case_type_stddev_pop_fields
  stddev_samp: case_type_stddev_samp_fields
  sum: case_type_sum_fields
  var_pop: case_type_var_pop_fields
  var_samp: case_type_var_samp_fields
  variance: case_type_variance_fields
}

# order by aggregate values of table "case_type"
input case_type_aggregate_order_by {
  avg: case_type_avg_order_by
  count: order_by
  max: case_type_max_order_by
  min: case_type_min_order_by
  stddev: case_type_stddev_order_by
  stddev_pop: case_type_stddev_pop_order_by
  stddev_samp: case_type_stddev_samp_order_by
  sum: case_type_sum_order_by
  var_pop: case_type_var_pop_order_by
  var_samp: case_type_var_samp_order_by
  variance: case_type_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input case_type_append_input {
  default_dashboard_queries: jsonb
  spec: jsonb
}

# input type for inserting array relation for remote table "case_type"
input case_type_arr_rel_insert_input {
  data: [case_type_insert_input!]!

  # upsert condition
  on_conflict: case_type_on_conflict
}

# aggregate avg on columns
type case_type_avg_fields {
  default_case_status_type_id: Float
  id: Float
  naming_druid_dimension_id: Float
  primary_druid_dimension_id: Float
}

# order by avg() on columns of table "case_type"
input case_type_avg_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
}

# Boolean expression to filter rows from the table "case_type". All fields are combined with a logical 'AND'.
input case_type_bool_exp {
  _and: [case_type_bool_exp!]
  _not: case_type_bool_exp
  _or: [case_type_bool_exp!]
  can_users_add_events: Boolean_comparison_exp
  caseTypeMetadataFromDruidDimensionByNamingDruidDimensionId: case_type_metadata_from_druid_dimension_bool_exp
  case_metadata_types: case_metadata_type_bool_exp
  case_status_type: case_status_type_bool_exp
  case_type_default_events: case_type_default_event_bool_exp
  case_type_default_fields: case_type_default_field_bool_exp
  case_type_default_statuses: case_type_default_status_bool_exp
  case_type_metadata_from_druid_dimension: case_type_metadata_from_druid_dimension_bool_exp
  case_type_metadata_from_druid_dimensions: case_type_metadata_from_druid_dimension_bool_exp
  case_type_metadata_from_druid_fields: case_type_metadata_from_druid_field_bool_exp
  cases: case_bool_exp
  default_case_status_type_id: Int_comparison_exp
  default_dashboard_queries: jsonb_comparison_exp
  external_alert_types: external_alert_type_bool_exp
  id: Int_comparison_exp
  is_metadata_expandable: Boolean_comparison_exp
  naming_druid_dimension_id: Int_comparison_exp
  primary_druid_dimension_id: Int_comparison_exp
  show_case_type_in_dossier: Boolean_comparison_exp
  spec: jsonb_comparison_exp
  type: case_type_enum_comparison_exp
}

# unique or primary key constraints on table "case_type"
enum case_type_constraint {
  # unique or primary key constraint on columns "id"
  case_type_pkey

  # unique or primary key constraint on columns "primary_druid_dimension_id"
  case_type_primary_druid_dimension_id_key
}

# columns and relationships of "case_type_default_event"
type case_type_default_event implements Node {
  # An object relationship
  case_event: case_event!
  case_event_id: Int!

  # An object relationship
  case_type: case_type!
  case_type_id: Int!
  id: ID!
}

# aggregated selection of "case_type_default_event"
type case_type_default_event_aggregate {
  aggregate: case_type_default_event_aggregate_fields
  nodes: [case_type_default_event!]!
}

# aggregate fields of "case_type_default_event"
type case_type_default_event_aggregate_fields {
  avg: case_type_default_event_avg_fields
  count(columns: [case_type_default_event_select_column!], distinct: Boolean): Int!
  max: case_type_default_event_max_fields
  min: case_type_default_event_min_fields
  stddev: case_type_default_event_stddev_fields
  stddev_pop: case_type_default_event_stddev_pop_fields
  stddev_samp: case_type_default_event_stddev_samp_fields
  sum: case_type_default_event_sum_fields
  var_pop: case_type_default_event_var_pop_fields
  var_samp: case_type_default_event_var_samp_fields
  variance: case_type_default_event_variance_fields
}

# order by aggregate values of table "case_type_default_event"
input case_type_default_event_aggregate_order_by {
  avg: case_type_default_event_avg_order_by
  count: order_by
  max: case_type_default_event_max_order_by
  min: case_type_default_event_min_order_by
  stddev: case_type_default_event_stddev_order_by
  stddev_pop: case_type_default_event_stddev_pop_order_by
  stddev_samp: case_type_default_event_stddev_samp_order_by
  sum: case_type_default_event_sum_order_by
  var_pop: case_type_default_event_var_pop_order_by
  var_samp: case_type_default_event_var_samp_order_by
  variance: case_type_default_event_variance_order_by
}

# input type for inserting array relation for remote table "case_type_default_event"
input case_type_default_event_arr_rel_insert_input {
  data: [case_type_default_event_insert_input!]!

  # upsert condition
  on_conflict: case_type_default_event_on_conflict
}

# aggregate avg on columns
type case_type_default_event_avg_fields {
  case_event_id: Float
  case_type_id: Float
}

# order by avg() on columns of table "case_type_default_event"
input case_type_default_event_avg_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# Boolean expression to filter rows from the table "case_type_default_event". All fields are combined with a logical 'AND'.
input case_type_default_event_bool_exp {
  _and: [case_type_default_event_bool_exp!]
  _not: case_type_default_event_bool_exp
  _or: [case_type_default_event_bool_exp!]
  case_event: case_event_bool_exp
  case_event_id: Int_comparison_exp
  case_type: case_type_bool_exp
  case_type_id: Int_comparison_exp
}

# unique or primary key constraints on table "case_type_default_event"
enum case_type_default_event_constraint {
  # unique or primary key constraint on columns "case_event_id", "case_type_id"
  case_type_default_event_pkey
}

# input type for incrementing numeric columns in table "case_type_default_event"
input case_type_default_event_inc_input {
  case_event_id: Int
  case_type_id: Int
}

# input type for inserting data into table "case_type_default_event"
input case_type_default_event_insert_input {
  case_event: case_event_obj_rel_insert_input
  case_event_id: Int
  case_type: case_type_obj_rel_insert_input
  case_type_id: Int
}

# aggregate max on columns
type case_type_default_event_max_fields {
  case_event_id: Int
  case_type_id: Int
}

# order by max() on columns of table "case_type_default_event"
input case_type_default_event_max_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# aggregate min on columns
type case_type_default_event_min_fields {
  case_event_id: Int
  case_type_id: Int
}

# order by min() on columns of table "case_type_default_event"
input case_type_default_event_min_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# response of any mutation on the table "case_type_default_event"
type case_type_default_event_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_type_default_event!]!
}

# on_conflict condition type for table "case_type_default_event"
input case_type_default_event_on_conflict {
  constraint: case_type_default_event_constraint!
  update_columns: [case_type_default_event_update_column!]! = []
  where: case_type_default_event_bool_exp
}

# Ordering options when selecting data from "case_type_default_event".
input case_type_default_event_order_by {
  case_event: case_event_order_by
  case_event_id: order_by
  case_type: case_type_order_by
  case_type_id: order_by
}

# primary key columns input for table: case_type_default_event
input case_type_default_event_pk_columns_input {
  case_event_id: Int!
  case_type_id: Int!
}

# select columns of table "case_type_default_event"
enum case_type_default_event_select_column {
  # column name
  case_event_id

  # column name
  case_type_id
}

# input type for updating data in table "case_type_default_event"
input case_type_default_event_set_input {
  case_event_id: Int
  case_type_id: Int
}

# aggregate stddev on columns
type case_type_default_event_stddev_fields {
  case_event_id: Float
  case_type_id: Float
}

# order by stddev() on columns of table "case_type_default_event"
input case_type_default_event_stddev_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# aggregate stddev_pop on columns
type case_type_default_event_stddev_pop_fields {
  case_event_id: Float
  case_type_id: Float
}

# order by stddev_pop() on columns of table "case_type_default_event"
input case_type_default_event_stddev_pop_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# aggregate stddev_samp on columns
type case_type_default_event_stddev_samp_fields {
  case_event_id: Float
  case_type_id: Float
}

# order by stddev_samp() on columns of table "case_type_default_event"
input case_type_default_event_stddev_samp_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# aggregate sum on columns
type case_type_default_event_sum_fields {
  case_event_id: Int
  case_type_id: Int
}

# order by sum() on columns of table "case_type_default_event"
input case_type_default_event_sum_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# update columns of table "case_type_default_event"
enum case_type_default_event_update_column {
  # column name
  case_event_id

  # column name
  case_type_id
}

input case_type_default_event_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: case_type_default_event_inc_input

  # sets the columns of the filtered rows to the given values
  _set: case_type_default_event_set_input
  where: case_type_default_event_bool_exp!
}

# aggregate var_pop on columns
type case_type_default_event_var_pop_fields {
  case_event_id: Float
  case_type_id: Float
}

# order by var_pop() on columns of table "case_type_default_event"
input case_type_default_event_var_pop_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# aggregate var_samp on columns
type case_type_default_event_var_samp_fields {
  case_event_id: Float
  case_type_id: Float
}

# order by var_samp() on columns of table "case_type_default_event"
input case_type_default_event_var_samp_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# aggregate variance on columns
type case_type_default_event_variance_fields {
  case_event_id: Float
  case_type_id: Float
}

# order by variance() on columns of table "case_type_default_event"
input case_type_default_event_variance_order_by {
  case_event_id: order_by
  case_type_id: order_by
}

# A Relay connection object on "case_type_default_event"
type case_type_default_eventConnection {
  edges: [case_type_default_eventEdge!]!
  pageInfo: PageInfo!
}

type case_type_default_eventEdge {
  cursor: String!
  node: case_type_default_event!
}

# columns and relationships of "case_type_default_field"
type case_type_default_field implements Node {
  # An object relationship
  case_type: case_type!
  case_type_id: Int!
  field_id: String!
  id: ID!
}

# aggregated selection of "case_type_default_field"
type case_type_default_field_aggregate {
  aggregate: case_type_default_field_aggregate_fields
  nodes: [case_type_default_field!]!
}

# aggregate fields of "case_type_default_field"
type case_type_default_field_aggregate_fields {
  avg: case_type_default_field_avg_fields
  count(columns: [case_type_default_field_select_column!], distinct: Boolean): Int!
  max: case_type_default_field_max_fields
  min: case_type_default_field_min_fields
  stddev: case_type_default_field_stddev_fields
  stddev_pop: case_type_default_field_stddev_pop_fields
  stddev_samp: case_type_default_field_stddev_samp_fields
  sum: case_type_default_field_sum_fields
  var_pop: case_type_default_field_var_pop_fields
  var_samp: case_type_default_field_var_samp_fields
  variance: case_type_default_field_variance_fields
}

# order by aggregate values of table "case_type_default_field"
input case_type_default_field_aggregate_order_by {
  avg: case_type_default_field_avg_order_by
  count: order_by
  max: case_type_default_field_max_order_by
  min: case_type_default_field_min_order_by
  stddev: case_type_default_field_stddev_order_by
  stddev_pop: case_type_default_field_stddev_pop_order_by
  stddev_samp: case_type_default_field_stddev_samp_order_by
  sum: case_type_default_field_sum_order_by
  var_pop: case_type_default_field_var_pop_order_by
  var_samp: case_type_default_field_var_samp_order_by
  variance: case_type_default_field_variance_order_by
}

# input type for inserting array relation for remote table "case_type_default_field"
input case_type_default_field_arr_rel_insert_input {
  data: [case_type_default_field_insert_input!]!

  # upsert condition
  on_conflict: case_type_default_field_on_conflict
}

# aggregate avg on columns
type case_type_default_field_avg_fields {
  case_type_id: Float
}

# order by avg() on columns of table "case_type_default_field"
input case_type_default_field_avg_order_by {
  case_type_id: order_by
}

# Boolean expression to filter rows from the table "case_type_default_field". All fields are combined with a logical 'AND'.
input case_type_default_field_bool_exp {
  _and: [case_type_default_field_bool_exp!]
  _not: case_type_default_field_bool_exp
  _or: [case_type_default_field_bool_exp!]
  case_type: case_type_bool_exp
  case_type_id: Int_comparison_exp
  field_id: String_comparison_exp
}

# unique or primary key constraints on table "case_type_default_field"
enum case_type_default_field_constraint {
  # unique or primary key constraint on columns "case_type_id", "field_id"
  case_type_default_field_pkey
}

# input type for incrementing numeric columns in table "case_type_default_field"
input case_type_default_field_inc_input {
  case_type_id: Int
}

# input type for inserting data into table "case_type_default_field"
input case_type_default_field_insert_input {
  case_type: case_type_obj_rel_insert_input
  case_type_id: Int
  field_id: String
}

# aggregate max on columns
type case_type_default_field_max_fields {
  case_type_id: Int
  field_id: String
}

# order by max() on columns of table "case_type_default_field"
input case_type_default_field_max_order_by {
  case_type_id: order_by
  field_id: order_by
}

# aggregate min on columns
type case_type_default_field_min_fields {
  case_type_id: Int
  field_id: String
}

# order by min() on columns of table "case_type_default_field"
input case_type_default_field_min_order_by {
  case_type_id: order_by
  field_id: order_by
}

# response of any mutation on the table "case_type_default_field"
type case_type_default_field_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_type_default_field!]!
}

# on_conflict condition type for table "case_type_default_field"
input case_type_default_field_on_conflict {
  constraint: case_type_default_field_constraint!
  update_columns: [case_type_default_field_update_column!]! = []
  where: case_type_default_field_bool_exp
}

# Ordering options when selecting data from "case_type_default_field".
input case_type_default_field_order_by {
  case_type: case_type_order_by
  case_type_id: order_by
  field_id: order_by
}

# primary key columns input for table: case_type_default_field
input case_type_default_field_pk_columns_input {
  case_type_id: Int!
  field_id: String!
}

# select columns of table "case_type_default_field"
enum case_type_default_field_select_column {
  # column name
  case_type_id

  # column name
  field_id
}

# input type for updating data in table "case_type_default_field"
input case_type_default_field_set_input {
  case_type_id: Int
  field_id: String
}

# aggregate stddev on columns
type case_type_default_field_stddev_fields {
  case_type_id: Float
}

# order by stddev() on columns of table "case_type_default_field"
input case_type_default_field_stddev_order_by {
  case_type_id: order_by
}

# aggregate stddev_pop on columns
type case_type_default_field_stddev_pop_fields {
  case_type_id: Float
}

# order by stddev_pop() on columns of table "case_type_default_field"
input case_type_default_field_stddev_pop_order_by {
  case_type_id: order_by
}

# aggregate stddev_samp on columns
type case_type_default_field_stddev_samp_fields {
  case_type_id: Float
}

# order by stddev_samp() on columns of table "case_type_default_field"
input case_type_default_field_stddev_samp_order_by {
  case_type_id: order_by
}

# aggregate sum on columns
type case_type_default_field_sum_fields {
  case_type_id: Int
}

# order by sum() on columns of table "case_type_default_field"
input case_type_default_field_sum_order_by {
  case_type_id: order_by
}

# update columns of table "case_type_default_field"
enum case_type_default_field_update_column {
  # column name
  case_type_id

  # column name
  field_id
}

input case_type_default_field_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: case_type_default_field_inc_input

  # sets the columns of the filtered rows to the given values
  _set: case_type_default_field_set_input
  where: case_type_default_field_bool_exp!
}

# aggregate var_pop on columns
type case_type_default_field_var_pop_fields {
  case_type_id: Float
}

# order by var_pop() on columns of table "case_type_default_field"
input case_type_default_field_var_pop_order_by {
  case_type_id: order_by
}

# aggregate var_samp on columns
type case_type_default_field_var_samp_fields {
  case_type_id: Float
}

# order by var_samp() on columns of table "case_type_default_field"
input case_type_default_field_var_samp_order_by {
  case_type_id: order_by
}

# aggregate variance on columns
type case_type_default_field_variance_fields {
  case_type_id: Float
}

# order by variance() on columns of table "case_type_default_field"
input case_type_default_field_variance_order_by {
  case_type_id: order_by
}

# A Relay connection object on "case_type_default_field"
type case_type_default_fieldConnection {
  edges: [case_type_default_fieldEdge!]!
  pageInfo: PageInfo!
}

type case_type_default_fieldEdge {
  cursor: String!
  node: case_type_default_field!
}

# columns and relationships of "case_type_default_status"
type case_type_default_status implements Node {
  # An object relationship
  case_status_type: case_status_type!
  case_status_type_id: Int!

  # An object relationship
  case_type: case_type!
  case_type_id: Int!
  id: ID!
}

# aggregated selection of "case_type_default_status"
type case_type_default_status_aggregate {
  aggregate: case_type_default_status_aggregate_fields
  nodes: [case_type_default_status!]!
}

# aggregate fields of "case_type_default_status"
type case_type_default_status_aggregate_fields {
  avg: case_type_default_status_avg_fields
  count(columns: [case_type_default_status_select_column!], distinct: Boolean): Int!
  max: case_type_default_status_max_fields
  min: case_type_default_status_min_fields
  stddev: case_type_default_status_stddev_fields
  stddev_pop: case_type_default_status_stddev_pop_fields
  stddev_samp: case_type_default_status_stddev_samp_fields
  sum: case_type_default_status_sum_fields
  var_pop: case_type_default_status_var_pop_fields
  var_samp: case_type_default_status_var_samp_fields
  variance: case_type_default_status_variance_fields
}

# order by aggregate values of table "case_type_default_status"
input case_type_default_status_aggregate_order_by {
  avg: case_type_default_status_avg_order_by
  count: order_by
  max: case_type_default_status_max_order_by
  min: case_type_default_status_min_order_by
  stddev: case_type_default_status_stddev_order_by
  stddev_pop: case_type_default_status_stddev_pop_order_by
  stddev_samp: case_type_default_status_stddev_samp_order_by
  sum: case_type_default_status_sum_order_by
  var_pop: case_type_default_status_var_pop_order_by
  var_samp: case_type_default_status_var_samp_order_by
  variance: case_type_default_status_variance_order_by
}

# input type for inserting array relation for remote table "case_type_default_status"
input case_type_default_status_arr_rel_insert_input {
  data: [case_type_default_status_insert_input!]!

  # upsert condition
  on_conflict: case_type_default_status_on_conflict
}

# aggregate avg on columns
type case_type_default_status_avg_fields {
  case_status_type_id: Float
  case_type_id: Float
}

# order by avg() on columns of table "case_type_default_status"
input case_type_default_status_avg_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# Boolean expression to filter rows from the table "case_type_default_status". All fields are combined with a logical 'AND'.
input case_type_default_status_bool_exp {
  _and: [case_type_default_status_bool_exp!]
  _not: case_type_default_status_bool_exp
  _or: [case_type_default_status_bool_exp!]
  case_status_type: case_status_type_bool_exp
  case_status_type_id: Int_comparison_exp
  case_type: case_type_bool_exp
  case_type_id: Int_comparison_exp
}

# unique or primary key constraints on table "case_type_default_status"
enum case_type_default_status_constraint {
  # unique or primary key constraint on columns "case_type_id", "case_status_type_id"
  case_type_default_status_pkey
}

# input type for incrementing numeric columns in table "case_type_default_status"
input case_type_default_status_inc_input {
  case_status_type_id: Int
  case_type_id: Int
}

# input type for inserting data into table "case_type_default_status"
input case_type_default_status_insert_input {
  case_status_type: case_status_type_obj_rel_insert_input
  case_status_type_id: Int
  case_type: case_type_obj_rel_insert_input
  case_type_id: Int
}

# aggregate max on columns
type case_type_default_status_max_fields {
  case_status_type_id: Int
  case_type_id: Int
}

# order by max() on columns of table "case_type_default_status"
input case_type_default_status_max_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# aggregate min on columns
type case_type_default_status_min_fields {
  case_status_type_id: Int
  case_type_id: Int
}

# order by min() on columns of table "case_type_default_status"
input case_type_default_status_min_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# response of any mutation on the table "case_type_default_status"
type case_type_default_status_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_type_default_status!]!
}

# on_conflict condition type for table "case_type_default_status"
input case_type_default_status_on_conflict {
  constraint: case_type_default_status_constraint!
  update_columns: [case_type_default_status_update_column!]! = []
  where: case_type_default_status_bool_exp
}

# Ordering options when selecting data from "case_type_default_status".
input case_type_default_status_order_by {
  case_status_type: case_status_type_order_by
  case_status_type_id: order_by
  case_type: case_type_order_by
  case_type_id: order_by
}

# primary key columns input for table: case_type_default_status
input case_type_default_status_pk_columns_input {
  case_status_type_id: Int!
  case_type_id: Int!
}

# select columns of table "case_type_default_status"
enum case_type_default_status_select_column {
  # column name
  case_status_type_id

  # column name
  case_type_id
}

# input type for updating data in table "case_type_default_status"
input case_type_default_status_set_input {
  case_status_type_id: Int
  case_type_id: Int
}

# aggregate stddev on columns
type case_type_default_status_stddev_fields {
  case_status_type_id: Float
  case_type_id: Float
}

# order by stddev() on columns of table "case_type_default_status"
input case_type_default_status_stddev_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# aggregate stddev_pop on columns
type case_type_default_status_stddev_pop_fields {
  case_status_type_id: Float
  case_type_id: Float
}

# order by stddev_pop() on columns of table "case_type_default_status"
input case_type_default_status_stddev_pop_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# aggregate stddev_samp on columns
type case_type_default_status_stddev_samp_fields {
  case_status_type_id: Float
  case_type_id: Float
}

# order by stddev_samp() on columns of table "case_type_default_status"
input case_type_default_status_stddev_samp_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# aggregate sum on columns
type case_type_default_status_sum_fields {
  case_status_type_id: Int
  case_type_id: Int
}

# order by sum() on columns of table "case_type_default_status"
input case_type_default_status_sum_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# update columns of table "case_type_default_status"
enum case_type_default_status_update_column {
  # column name
  case_status_type_id

  # column name
  case_type_id
}

input case_type_default_status_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: case_type_default_status_inc_input

  # sets the columns of the filtered rows to the given values
  _set: case_type_default_status_set_input
  where: case_type_default_status_bool_exp!
}

# aggregate var_pop on columns
type case_type_default_status_var_pop_fields {
  case_status_type_id: Float
  case_type_id: Float
}

# order by var_pop() on columns of table "case_type_default_status"
input case_type_default_status_var_pop_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# aggregate var_samp on columns
type case_type_default_status_var_samp_fields {
  case_status_type_id: Float
  case_type_id: Float
}

# order by var_samp() on columns of table "case_type_default_status"
input case_type_default_status_var_samp_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# aggregate variance on columns
type case_type_default_status_variance_fields {
  case_status_type_id: Float
  case_type_id: Float
}

# order by variance() on columns of table "case_type_default_status"
input case_type_default_status_variance_order_by {
  case_status_type_id: order_by
  case_type_id: order_by
}

# A Relay connection object on "case_type_default_status"
type case_type_default_statusConnection {
  edges: [case_type_default_statusEdge!]!
  pageInfo: PageInfo!
}

type case_type_default_statusEdge {
  cursor: String!
  node: case_type_default_status!
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input case_type_delete_at_path_input {
  default_dashboard_queries: [String!]
  spec: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input case_type_delete_elem_input {
  default_dashboard_queries: Int
  spec: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input case_type_delete_key_input {
  default_dashboard_queries: String
  spec: String
}

scalar case_type_enum

# Boolean expression to compare columns of type "case_type_enum". All fields are combined with logical 'AND'.
input case_type_enum_comparison_exp {
  _eq: case_type_enum
  _gt: case_type_enum
  _gte: case_type_enum
  _in: [case_type_enum!]
  _is_null: Boolean
  _lt: case_type_enum
  _lte: case_type_enum
  _neq: case_type_enum
  _nin: [case_type_enum!]
}

# input type for incrementing numeric columns in table "case_type"
input case_type_inc_input {
  default_case_status_type_id: Int
  id: Int
  naming_druid_dimension_id: Int
  primary_druid_dimension_id: Int
}

# input type for inserting data into table "case_type"
input case_type_insert_input {
  can_users_add_events: Boolean
  caseTypeMetadataFromDruidDimensionByNamingDruidDimensionId: case_type_metadata_from_druid_dimension_obj_rel_insert_input
  case_metadata_types: case_metadata_type_arr_rel_insert_input
  case_status_type: case_status_type_obj_rel_insert_input
  case_type_default_events: case_type_default_event_arr_rel_insert_input
  case_type_default_fields: case_type_default_field_arr_rel_insert_input
  case_type_default_statuses: case_type_default_status_arr_rel_insert_input
  case_type_metadata_from_druid_dimension: case_type_metadata_from_druid_dimension_obj_rel_insert_input
  case_type_metadata_from_druid_dimensions: case_type_metadata_from_druid_dimension_arr_rel_insert_input
  case_type_metadata_from_druid_fields: case_type_metadata_from_druid_field_arr_rel_insert_input
  cases: case_arr_rel_insert_input
  default_case_status_type_id: Int
  default_dashboard_queries: jsonb
  external_alert_types: external_alert_type_arr_rel_insert_input
  id: Int
  is_metadata_expandable: Boolean
  naming_druid_dimension_id: Int
  primary_druid_dimension_id: Int
  show_case_type_in_dossier: Boolean
  spec: jsonb
  type: case_type_enum
}

# aggregate max on columns
type case_type_max_fields {
  default_case_status_type_id: Int
  id: Int
  naming_druid_dimension_id: Int
  primary_druid_dimension_id: Int
  type: case_type_enum
}

# order by max() on columns of table "case_type"
input case_type_max_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
  type: order_by
}

# columns and relationships of "case_type_metadata_from_druid_dimension"
type case_type_metadata_from_druid_dimension implements Node {
  # An object relationship
  caseTypeById: case_type

  # An object relationship
  case_type: case_type!
  case_type_id: Int!

  # An array relationship
  case_types(
    # distinct select on columns
    distinct_on: [case_type_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_order_by!]

    # filter the rows returned
    where: case_type_bool_exp
  ): [case_type!]!

  # An aggregate relationship
  case_types_aggregate(
    # distinct select on columns
    distinct_on: [case_type_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [case_type_order_by!]

    # filter the rows returned
    where: case_type_bool_exp
  ): case_type_aggregate!

  # An array relationship connection
  case_types_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_order_by!]

    # filter the rows returned
    where: case_type_bool_exp
  ): case_typeConnection!
  dossier_section: String
  druid_dimension_name: String!
  id: ID!
  show_in_overview_table: Boolean!
  treat_as_primary_dimension: Boolean!
}

# aggregated selection of "case_type_metadata_from_druid_dimension"
type case_type_metadata_from_druid_dimension_aggregate {
  aggregate: case_type_metadata_from_druid_dimension_aggregate_fields
  nodes: [case_type_metadata_from_druid_dimension!]!
}

# aggregate fields of "case_type_metadata_from_druid_dimension"
type case_type_metadata_from_druid_dimension_aggregate_fields {
  avg: case_type_metadata_from_druid_dimension_avg_fields
  count(columns: [case_type_metadata_from_druid_dimension_select_column!], distinct: Boolean): Int!
  max: case_type_metadata_from_druid_dimension_max_fields
  min: case_type_metadata_from_druid_dimension_min_fields
  stddev: case_type_metadata_from_druid_dimension_stddev_fields
  stddev_pop: case_type_metadata_from_druid_dimension_stddev_pop_fields
  stddev_samp: case_type_metadata_from_druid_dimension_stddev_samp_fields
  sum: case_type_metadata_from_druid_dimension_sum_fields
  var_pop: case_type_metadata_from_druid_dimension_var_pop_fields
  var_samp: case_type_metadata_from_druid_dimension_var_samp_fields
  variance: case_type_metadata_from_druid_dimension_variance_fields
}

# order by aggregate values of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_aggregate_order_by {
  avg: case_type_metadata_from_druid_dimension_avg_order_by
  count: order_by
  max: case_type_metadata_from_druid_dimension_max_order_by
  min: case_type_metadata_from_druid_dimension_min_order_by
  stddev: case_type_metadata_from_druid_dimension_stddev_order_by
  stddev_pop: case_type_metadata_from_druid_dimension_stddev_pop_order_by
  stddev_samp: case_type_metadata_from_druid_dimension_stddev_samp_order_by
  sum: case_type_metadata_from_druid_dimension_sum_order_by
  var_pop: case_type_metadata_from_druid_dimension_var_pop_order_by
  var_samp: case_type_metadata_from_druid_dimension_var_samp_order_by
  variance: case_type_metadata_from_druid_dimension_variance_order_by
}

# input type for inserting array relation for remote table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_arr_rel_insert_input {
  data: [case_type_metadata_from_druid_dimension_insert_input!]!

  # upsert condition
  on_conflict: case_type_metadata_from_druid_dimension_on_conflict
}

# aggregate avg on columns
type case_type_metadata_from_druid_dimension_avg_fields {
  case_type_id: Float
  id: Float
}

# order by avg() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_avg_order_by {
  case_type_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table
# "case_type_metadata_from_druid_dimension". All fields are combined with a logical 'AND'.
input case_type_metadata_from_druid_dimension_bool_exp {
  _and: [case_type_metadata_from_druid_dimension_bool_exp!]
  _not: case_type_metadata_from_druid_dimension_bool_exp
  _or: [case_type_metadata_from_druid_dimension_bool_exp!]
  caseTypeById: case_type_bool_exp
  case_type: case_type_bool_exp
  case_type_id: Int_comparison_exp
  case_types: case_type_bool_exp
  dossier_section: String_comparison_exp
  druid_dimension_name: String_comparison_exp
  id: Int_comparison_exp
  show_in_overview_table: Boolean_comparison_exp
  treat_as_primary_dimension: Boolean_comparison_exp
}

# unique or primary key constraints on table "case_type_metadata_from_druid_dimension"
enum case_type_metadata_from_druid_dimension_constraint {
  # unique or primary key constraint on columns "id"
  case_type_metadata_from_druid_dimension_pkey
}

# input type for incrementing numeric columns in table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_inc_input {
  case_type_id: Int
  id: Int
}

# input type for inserting data into table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_insert_input {
  caseTypeById: case_type_obj_rel_insert_input
  case_type: case_type_obj_rel_insert_input
  case_type_id: Int
  case_types: case_type_arr_rel_insert_input
  dossier_section: String
  druid_dimension_name: String
  id: Int
  show_in_overview_table: Boolean
  treat_as_primary_dimension: Boolean
}

# aggregate max on columns
type case_type_metadata_from_druid_dimension_max_fields {
  case_type_id: Int
  dossier_section: String
  druid_dimension_name: String
  id: Int
}

# order by max() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_max_order_by {
  case_type_id: order_by
  dossier_section: order_by
  druid_dimension_name: order_by
  id: order_by
}

# aggregate min on columns
type case_type_metadata_from_druid_dimension_min_fields {
  case_type_id: Int
  dossier_section: String
  druid_dimension_name: String
  id: Int
}

# order by min() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_min_order_by {
  case_type_id: order_by
  dossier_section: order_by
  druid_dimension_name: order_by
  id: order_by
}

# response of any mutation on the table "case_type_metadata_from_druid_dimension"
type case_type_metadata_from_druid_dimension_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_type_metadata_from_druid_dimension!]!
}

# input type for inserting object relation for remote table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_obj_rel_insert_input {
  data: case_type_metadata_from_druid_dimension_insert_input!

  # upsert condition
  on_conflict: case_type_metadata_from_druid_dimension_on_conflict
}

# on_conflict condition type for table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_on_conflict {
  constraint: case_type_metadata_from_druid_dimension_constraint!
  update_columns: [case_type_metadata_from_druid_dimension_update_column!]! = []
  where: case_type_metadata_from_druid_dimension_bool_exp
}

# Ordering options when selecting data from "case_type_metadata_from_druid_dimension".
input case_type_metadata_from_druid_dimension_order_by {
  caseTypeById: case_type_order_by
  case_type: case_type_order_by
  case_type_id: order_by
  case_types_aggregate: case_type_aggregate_order_by
  dossier_section: order_by
  druid_dimension_name: order_by
  id: order_by
  show_in_overview_table: order_by
  treat_as_primary_dimension: order_by
}

# primary key columns input for table: case_type_metadata_from_druid_dimension
input case_type_metadata_from_druid_dimension_pk_columns_input {
  id: Int!
}

# select columns of table "case_type_metadata_from_druid_dimension"
enum case_type_metadata_from_druid_dimension_select_column {
  # column name
  case_type_id

  # column name
  dossier_section

  # column name
  druid_dimension_name

  # column name
  id

  # column name
  show_in_overview_table

  # column name
  treat_as_primary_dimension
}

# input type for updating data in table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_set_input {
  case_type_id: Int
  dossier_section: String
  druid_dimension_name: String
  id: Int
  show_in_overview_table: Boolean
  treat_as_primary_dimension: Boolean
}

# aggregate stddev on columns
type case_type_metadata_from_druid_dimension_stddev_fields {
  case_type_id: Float
  id: Float
}

# order by stddev() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_stddev_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type case_type_metadata_from_druid_dimension_stddev_pop_fields {
  case_type_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_stddev_pop_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type case_type_metadata_from_druid_dimension_stddev_samp_fields {
  case_type_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_stddev_samp_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate sum on columns
type case_type_metadata_from_druid_dimension_sum_fields {
  case_type_id: Int
  id: Int
}

# order by sum() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_sum_order_by {
  case_type_id: order_by
  id: order_by
}

# update columns of table "case_type_metadata_from_druid_dimension"
enum case_type_metadata_from_druid_dimension_update_column {
  # column name
  case_type_id

  # column name
  dossier_section

  # column name
  druid_dimension_name

  # column name
  id

  # column name
  show_in_overview_table

  # column name
  treat_as_primary_dimension
}

input case_type_metadata_from_druid_dimension_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: case_type_metadata_from_druid_dimension_inc_input

  # sets the columns of the filtered rows to the given values
  _set: case_type_metadata_from_druid_dimension_set_input
  where: case_type_metadata_from_druid_dimension_bool_exp!
}

# aggregate var_pop on columns
type case_type_metadata_from_druid_dimension_var_pop_fields {
  case_type_id: Float
  id: Float
}

# order by var_pop() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_var_pop_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type case_type_metadata_from_druid_dimension_var_samp_fields {
  case_type_id: Float
  id: Float
}

# order by var_samp() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_var_samp_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate variance on columns
type case_type_metadata_from_druid_dimension_variance_fields {
  case_type_id: Float
  id: Float
}

# order by variance() on columns of table "case_type_metadata_from_druid_dimension"
input case_type_metadata_from_druid_dimension_variance_order_by {
  case_type_id: order_by
  id: order_by
}

# A Relay connection object on "case_type_metadata_from_druid_dimension"
type case_type_metadata_from_druid_dimensionConnection {
  edges: [case_type_metadata_from_druid_dimensionEdge!]!
  pageInfo: PageInfo!
}

type case_type_metadata_from_druid_dimensionEdge {
  cursor: String!
  node: case_type_metadata_from_druid_dimension!
}

# columns and relationships of "case_type_metadata_from_druid_field"
type case_type_metadata_from_druid_field implements Node {
  # An object relationship
  case_type: case_type!
  case_type_id: Int!
  display_name: String!
  dossier_section: String
  field_id: String!
  id: ID!
  show_in_overview_table: Boolean!
  type: druid_field_metadata_type_enum!
}

# aggregated selection of "case_type_metadata_from_druid_field"
type case_type_metadata_from_druid_field_aggregate {
  aggregate: case_type_metadata_from_druid_field_aggregate_fields
  nodes: [case_type_metadata_from_druid_field!]!
}

# aggregate fields of "case_type_metadata_from_druid_field"
type case_type_metadata_from_druid_field_aggregate_fields {
  avg: case_type_metadata_from_druid_field_avg_fields
  count(columns: [case_type_metadata_from_druid_field_select_column!], distinct: Boolean): Int!
  max: case_type_metadata_from_druid_field_max_fields
  min: case_type_metadata_from_druid_field_min_fields
  stddev: case_type_metadata_from_druid_field_stddev_fields
  stddev_pop: case_type_metadata_from_druid_field_stddev_pop_fields
  stddev_samp: case_type_metadata_from_druid_field_stddev_samp_fields
  sum: case_type_metadata_from_druid_field_sum_fields
  var_pop: case_type_metadata_from_druid_field_var_pop_fields
  var_samp: case_type_metadata_from_druid_field_var_samp_fields
  variance: case_type_metadata_from_druid_field_variance_fields
}

# order by aggregate values of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_aggregate_order_by {
  avg: case_type_metadata_from_druid_field_avg_order_by
  count: order_by
  max: case_type_metadata_from_druid_field_max_order_by
  min: case_type_metadata_from_druid_field_min_order_by
  stddev: case_type_metadata_from_druid_field_stddev_order_by
  stddev_pop: case_type_metadata_from_druid_field_stddev_pop_order_by
  stddev_samp: case_type_metadata_from_druid_field_stddev_samp_order_by
  sum: case_type_metadata_from_druid_field_sum_order_by
  var_pop: case_type_metadata_from_druid_field_var_pop_order_by
  var_samp: case_type_metadata_from_druid_field_var_samp_order_by
  variance: case_type_metadata_from_druid_field_variance_order_by
}

# input type for inserting array relation for remote table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_arr_rel_insert_input {
  data: [case_type_metadata_from_druid_field_insert_input!]!

  # upsert condition
  on_conflict: case_type_metadata_from_druid_field_on_conflict
}

# aggregate avg on columns
type case_type_metadata_from_druid_field_avg_fields {
  case_type_id: Float
  id: Float
}

# order by avg() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_avg_order_by {
  case_type_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table
# "case_type_metadata_from_druid_field". All fields are combined with a logical 'AND'.
input case_type_metadata_from_druid_field_bool_exp {
  _and: [case_type_metadata_from_druid_field_bool_exp!]
  _not: case_type_metadata_from_druid_field_bool_exp
  _or: [case_type_metadata_from_druid_field_bool_exp!]
  case_type: case_type_bool_exp
  case_type_id: Int_comparison_exp
  display_name: String_comparison_exp
  dossier_section: String_comparison_exp
  field_id: String_comparison_exp
  id: Int_comparison_exp
  show_in_overview_table: Boolean_comparison_exp
  type: druid_field_metadata_type_enum_comparison_exp
}

# unique or primary key constraints on table "case_type_metadata_from_druid_field"
enum case_type_metadata_from_druid_field_constraint {
  # unique or primary key constraint on columns "id"
  case_type_metadata_from_druid_field_pkey
}

# input type for incrementing numeric columns in table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_inc_input {
  case_type_id: Int
  id: Int
}

# input type for inserting data into table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_insert_input {
  case_type: case_type_obj_rel_insert_input
  case_type_id: Int
  display_name: String
  dossier_section: String
  field_id: String
  id: Int
  show_in_overview_table: Boolean
  type: druid_field_metadata_type_enum
}

# aggregate max on columns
type case_type_metadata_from_druid_field_max_fields {
  case_type_id: Int
  display_name: String
  dossier_section: String
  field_id: String
  id: Int
  type: druid_field_metadata_type_enum
}

# order by max() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_max_order_by {
  case_type_id: order_by
  display_name: order_by
  dossier_section: order_by
  field_id: order_by
  id: order_by
  type: order_by
}

# aggregate min on columns
type case_type_metadata_from_druid_field_min_fields {
  case_type_id: Int
  display_name: String
  dossier_section: String
  field_id: String
  id: Int
  type: druid_field_metadata_type_enum
}

# order by min() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_min_order_by {
  case_type_id: order_by
  display_name: order_by
  dossier_section: order_by
  field_id: order_by
  id: order_by
  type: order_by
}

# response of any mutation on the table "case_type_metadata_from_druid_field"
type case_type_metadata_from_druid_field_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_type_metadata_from_druid_field!]!
}

# on_conflict condition type for table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_on_conflict {
  constraint: case_type_metadata_from_druid_field_constraint!
  update_columns: [case_type_metadata_from_druid_field_update_column!]! = []
  where: case_type_metadata_from_druid_field_bool_exp
}

# Ordering options when selecting data from "case_type_metadata_from_druid_field".
input case_type_metadata_from_druid_field_order_by {
  case_type: case_type_order_by
  case_type_id: order_by
  display_name: order_by
  dossier_section: order_by
  field_id: order_by
  id: order_by
  show_in_overview_table: order_by
  type: order_by
}

# primary key columns input for table: case_type_metadata_from_druid_field
input case_type_metadata_from_druid_field_pk_columns_input {
  id: Int!
}

# select columns of table "case_type_metadata_from_druid_field"
enum case_type_metadata_from_druid_field_select_column {
  # column name
  case_type_id

  # column name
  display_name

  # column name
  dossier_section

  # column name
  field_id

  # column name
  id

  # column name
  show_in_overview_table

  # column name
  type
}

# input type for updating data in table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_set_input {
  case_type_id: Int
  display_name: String
  dossier_section: String
  field_id: String
  id: Int
  show_in_overview_table: Boolean
  type: druid_field_metadata_type_enum
}

# aggregate stddev on columns
type case_type_metadata_from_druid_field_stddev_fields {
  case_type_id: Float
  id: Float
}

# order by stddev() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_stddev_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type case_type_metadata_from_druid_field_stddev_pop_fields {
  case_type_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_stddev_pop_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type case_type_metadata_from_druid_field_stddev_samp_fields {
  case_type_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_stddev_samp_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate sum on columns
type case_type_metadata_from_druid_field_sum_fields {
  case_type_id: Int
  id: Int
}

# order by sum() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_sum_order_by {
  case_type_id: order_by
  id: order_by
}

# update columns of table "case_type_metadata_from_druid_field"
enum case_type_metadata_from_druid_field_update_column {
  # column name
  case_type_id

  # column name
  display_name

  # column name
  dossier_section

  # column name
  field_id

  # column name
  id

  # column name
  show_in_overview_table

  # column name
  type
}

input case_type_metadata_from_druid_field_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: case_type_metadata_from_druid_field_inc_input

  # sets the columns of the filtered rows to the given values
  _set: case_type_metadata_from_druid_field_set_input
  where: case_type_metadata_from_druid_field_bool_exp!
}

# aggregate var_pop on columns
type case_type_metadata_from_druid_field_var_pop_fields {
  case_type_id: Float
  id: Float
}

# order by var_pop() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_var_pop_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type case_type_metadata_from_druid_field_var_samp_fields {
  case_type_id: Float
  id: Float
}

# order by var_samp() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_var_samp_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate variance on columns
type case_type_metadata_from_druid_field_variance_fields {
  case_type_id: Float
  id: Float
}

# order by variance() on columns of table "case_type_metadata_from_druid_field"
input case_type_metadata_from_druid_field_variance_order_by {
  case_type_id: order_by
  id: order_by
}

# A Relay connection object on "case_type_metadata_from_druid_field"
type case_type_metadata_from_druid_fieldConnection {
  edges: [case_type_metadata_from_druid_fieldEdge!]!
  pageInfo: PageInfo!
}

type case_type_metadata_from_druid_fieldEdge {
  cursor: String!
  node: case_type_metadata_from_druid_field!
}

# aggregate min on columns
type case_type_min_fields {
  default_case_status_type_id: Int
  id: Int
  naming_druid_dimension_id: Int
  primary_druid_dimension_id: Int
  type: case_type_enum
}

# order by min() on columns of table "case_type"
input case_type_min_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
  type: order_by
}

# response of any mutation on the table "case_type"
type case_type_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [case_type!]!
}

# input type for inserting object relation for remote table "case_type"
input case_type_obj_rel_insert_input {
  data: case_type_insert_input!

  # upsert condition
  on_conflict: case_type_on_conflict
}

# on_conflict condition type for table "case_type"
input case_type_on_conflict {
  constraint: case_type_constraint!
  update_columns: [case_type_update_column!]! = []
  where: case_type_bool_exp
}

# Ordering options when selecting data from "case_type".
input case_type_order_by {
  can_users_add_events: order_by
  caseTypeMetadataFromDruidDimensionByNamingDruidDimensionId: case_type_metadata_from_druid_dimension_order_by
  case_metadata_types_aggregate: case_metadata_type_aggregate_order_by
  case_status_type: case_status_type_order_by
  case_type_default_events_aggregate: case_type_default_event_aggregate_order_by
  case_type_default_fields_aggregate: case_type_default_field_aggregate_order_by
  case_type_default_statuses_aggregate: case_type_default_status_aggregate_order_by
  case_type_metadata_from_druid_dimension: case_type_metadata_from_druid_dimension_order_by
  case_type_metadata_from_druid_dimensions_aggregate: case_type_metadata_from_druid_dimension_aggregate_order_by
  case_type_metadata_from_druid_fields_aggregate: case_type_metadata_from_druid_field_aggregate_order_by
  cases_aggregate: case_aggregate_order_by
  default_case_status_type_id: order_by
  default_dashboard_queries: order_by
  external_alert_types_aggregate: external_alert_type_aggregate_order_by
  id: order_by
  is_metadata_expandable: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
  show_case_type_in_dossier: order_by
  spec: order_by
  type: order_by
}

# primary key columns input for table: case_type
input case_type_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input case_type_prepend_input {
  default_dashboard_queries: jsonb
  spec: jsonb
}

# select columns of table "case_type"
enum case_type_select_column {
  # column name
  can_users_add_events

  # column name
  default_case_status_type_id

  # column name
  default_dashboard_queries

  # column name
  id

  # column name
  is_metadata_expandable

  # column name
  naming_druid_dimension_id

  # column name
  primary_druid_dimension_id

  # column name
  show_case_type_in_dossier

  # column name
  spec

  # column name
  type
}

# input type for updating data in table "case_type"
input case_type_set_input {
  can_users_add_events: Boolean
  default_case_status_type_id: Int
  default_dashboard_queries: jsonb
  id: Int
  is_metadata_expandable: Boolean
  naming_druid_dimension_id: Int
  primary_druid_dimension_id: Int
  show_case_type_in_dossier: Boolean
  spec: jsonb
  type: case_type_enum
}

# aggregate stddev on columns
type case_type_stddev_fields {
  default_case_status_type_id: Float
  id: Float
  naming_druid_dimension_id: Float
  primary_druid_dimension_id: Float
}

# order by stddev() on columns of table "case_type"
input case_type_stddev_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
}

# aggregate stddev_pop on columns
type case_type_stddev_pop_fields {
  default_case_status_type_id: Float
  id: Float
  naming_druid_dimension_id: Float
  primary_druid_dimension_id: Float
}

# order by stddev_pop() on columns of table "case_type"
input case_type_stddev_pop_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
}

# aggregate stddev_samp on columns
type case_type_stddev_samp_fields {
  default_case_status_type_id: Float
  id: Float
  naming_druid_dimension_id: Float
  primary_druid_dimension_id: Float
}

# order by stddev_samp() on columns of table "case_type"
input case_type_stddev_samp_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
}

# aggregate sum on columns
type case_type_sum_fields {
  default_case_status_type_id: Int
  id: Int
  naming_druid_dimension_id: Int
  primary_druid_dimension_id: Int
}

# order by sum() on columns of table "case_type"
input case_type_sum_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
}

# update columns of table "case_type"
enum case_type_update_column {
  # column name
  can_users_add_events

  # column name
  default_case_status_type_id

  # column name
  default_dashboard_queries

  # column name
  id

  # column name
  is_metadata_expandable

  # column name
  naming_druid_dimension_id

  # column name
  primary_druid_dimension_id

  # column name
  show_case_type_in_dossier

  # column name
  spec

  # column name
  type
}

input case_type_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: case_type_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: case_type_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: case_type_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: case_type_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: case_type_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: case_type_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: case_type_set_input
  where: case_type_bool_exp!
}

# aggregate var_pop on columns
type case_type_var_pop_fields {
  default_case_status_type_id: Float
  id: Float
  naming_druid_dimension_id: Float
  primary_druid_dimension_id: Float
}

# order by var_pop() on columns of table "case_type"
input case_type_var_pop_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
}

# aggregate var_samp on columns
type case_type_var_samp_fields {
  default_case_status_type_id: Float
  id: Float
  naming_druid_dimension_id: Float
  primary_druid_dimension_id: Float
}

# order by var_samp() on columns of table "case_type"
input case_type_var_samp_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
}

# aggregate variance on columns
type case_type_variance_fields {
  default_case_status_type_id: Float
  id: Float
  naming_druid_dimension_id: Float
  primary_druid_dimension_id: Float
}

# order by variance() on columns of table "case_type"
input case_type_variance_order_by {
  default_case_status_type_id: order_by
  id: order_by
  naming_druid_dimension_id: order_by
  primary_druid_dimension_id: order_by
}

# A Relay connection object on "case_type"
type case_typeConnection {
  edges: [case_typeEdge!]!
  pageInfo: PageInfo!
}

type case_typeEdge {
  cursor: String!
  node: case_type!
}

# update columns of table "case"
enum case_update_column {
  # column name
  alert_notification_id

  # column name
  case_status_type_id

  # column name
  case_type_id

  # column name
  id

  # column name
  primary_druid_dimension_values

  # column name
  spec
}

input case_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: case_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: case_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: case_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: case_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: case_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: case_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: case_set_input
  where: case_bool_exp!
}

# aggregate var_pop on columns
type case_var_pop_fields {
  alert_notification_id: Float
  case_status_type_id: Float
  case_type_id: Float
  id: Float
}

# order by var_pop() on columns of table "case"
input case_var_pop_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type case_var_samp_fields {
  alert_notification_id: Float
  case_status_type_id: Float
  case_type_id: Float
  id: Float
}

# order by var_samp() on columns of table "case"
input case_var_samp_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# aggregate variance on columns
type case_variance_fields {
  alert_notification_id: Float
  case_status_type_id: Float
  case_type_id: Float
  id: Float
}

# order by variance() on columns of table "case"
input case_variance_order_by {
  alert_notification_id: order_by
  case_status_type_id: order_by
  case_type_id: order_by
  id: order_by
}

# A Relay connection object on "case"
type caseConnection {
  edges: [caseEdge!]!
  pageInfo: PageInfo!
}

type caseEdge {
  cursor: String!
  node: case!
}

# columns and relationships of "category"
type category implements Node {
  # An array relationship
  children(
    # distinct select on columns
    distinct_on: [category_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [category_order_by!]

    # filter the rows returned
    where: category_bool_exp
  ): [category!]!

  # An aggregate relationship
  children_aggregate(
    # distinct select on columns
    distinct_on: [category_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [category_order_by!]

    # filter the rows returned
    where: category_bool_exp
  ): category_aggregate!

  # An array relationship connection
  children_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [category_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [category_order_by!]

    # filter the rows returned
    where: category_bool_exp
  ): categoryConnection!
  created: timestamp!

  # An array relationship
  field_category_mappings(
    # distinct select on columns
    distinct_on: [field_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_category_mapping_order_by!]

    # filter the rows returned
    where: field_category_mapping_bool_exp
  ): [field_category_mapping!]!

  # An aggregate relationship
  field_category_mappings_aggregate(
    # distinct select on columns
    distinct_on: [field_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_category_mapping_order_by!]

    # filter the rows returned
    where: field_category_mapping_bool_exp
  ): field_category_mapping_aggregate!

  # An array relationship connection
  field_category_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_category_mapping_order_by!]

    # filter the rows returned
    where: field_category_mapping_bool_exp
  ): field_category_mappingConnection!
  id: ID!
  last_modified: timestamp!
  name: String!

  # An object relationship
  parent: category
  parent_id: String

  # An array relationship
  unpublished_field_category_mappings(
    # distinct select on columns
    distinct_on: [unpublished_field_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_category_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_category_mapping_bool_exp
  ): [unpublished_field_category_mapping!]!

  # An aggregate relationship
  unpublished_field_category_mappings_aggregate(
    # distinct select on columns
    distinct_on: [unpublished_field_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_category_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_category_mapping_bool_exp
  ): unpublished_field_category_mapping_aggregate!

  # An array relationship connection
  unpublished_field_category_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_category_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_category_mapping_bool_exp
  ): unpublished_field_category_mappingConnection!
  visibility_status: visibility_status_enum!
}

# aggregated selection of "category"
type category_aggregate {
  aggregate: category_aggregate_fields
  nodes: [category!]!
}

# aggregate fields of "category"
type category_aggregate_fields {
  count(columns: [category_select_column!], distinct: Boolean): Int!
  max: category_max_fields
  min: category_min_fields
}

# order by aggregate values of table "category"
input category_aggregate_order_by {
  count: order_by
  max: category_max_order_by
  min: category_min_order_by
}

# input type for inserting array relation for remote table "category"
input category_arr_rel_insert_input {
  data: [category_insert_input!]!

  # upsert condition
  on_conflict: category_on_conflict
}

# Boolean expression to filter rows from the table "category". All fields are combined with a logical 'AND'.
input category_bool_exp {
  _and: [category_bool_exp!]
  _not: category_bool_exp
  _or: [category_bool_exp!]
  children: category_bool_exp
  created: timestamp_comparison_exp
  field_category_mappings: field_category_mapping_bool_exp
  id: String_comparison_exp
  last_modified: timestamp_comparison_exp
  name: String_comparison_exp
  parent: category_bool_exp
  parent_id: String_comparison_exp
  unpublished_field_category_mappings: unpublished_field_category_mapping_bool_exp
  visibility_status: visibility_status_enum_comparison_exp
}

# unique or primary key constraints on table "category"
enum category_constraint {
  # unique or primary key constraint on columns "id"
  category_pkey
}

# input type for inserting data into table "category"
input category_insert_input {
  children: category_arr_rel_insert_input
  created: timestamp
  field_category_mappings: field_category_mapping_arr_rel_insert_input
  id: String
  last_modified: timestamp
  name: String
  parent: category_obj_rel_insert_input
  parent_id: String
  unpublished_field_category_mappings: unpublished_field_category_mapping_arr_rel_insert_input
  visibility_status: visibility_status_enum
}

# aggregate max on columns
type category_max_fields {
  created: timestamp
  id: String
  last_modified: timestamp
  name: String
  parent_id: String
  visibility_status: visibility_status_enum
}

# order by max() on columns of table "category"
input category_max_order_by {
  created: order_by
  id: order_by
  last_modified: order_by
  name: order_by
  parent_id: order_by
  visibility_status: order_by
}

# aggregate min on columns
type category_min_fields {
  created: timestamp
  id: String
  last_modified: timestamp
  name: String
  parent_id: String
  visibility_status: visibility_status_enum
}

# order by min() on columns of table "category"
input category_min_order_by {
  created: order_by
  id: order_by
  last_modified: order_by
  name: order_by
  parent_id: order_by
  visibility_status: order_by
}

# response of any mutation on the table "category"
type category_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [category!]!
}

# input type for inserting object relation for remote table "category"
input category_obj_rel_insert_input {
  data: category_insert_input!

  # upsert condition
  on_conflict: category_on_conflict
}

# on_conflict condition type for table "category"
input category_on_conflict {
  constraint: category_constraint!
  update_columns: [category_update_column!]! = []
  where: category_bool_exp
}

# Ordering options when selecting data from "category".
input category_order_by {
  children_aggregate: category_aggregate_order_by
  created: order_by
  field_category_mappings_aggregate: field_category_mapping_aggregate_order_by
  id: order_by
  last_modified: order_by
  name: order_by
  parent: category_order_by
  parent_id: order_by
  unpublished_field_category_mappings_aggregate: unpublished_field_category_mapping_aggregate_order_by
  visibility_status: order_by
}

# primary key columns input for table: category
input category_pk_columns_input {
  id: String!
}

# select columns of table "category"
enum category_select_column {
  # column name
  created

  # column name
  id

  # column name
  last_modified

  # column name
  name

  # column name
  parent_id

  # column name
  visibility_status
}

# input type for updating data in table "category"
input category_set_input {
  created: timestamp
  id: String
  last_modified: timestamp
  name: String
  parent_id: String
  visibility_status: visibility_status_enum
}

# update columns of table "category"
enum category_update_column {
  # column name
  created

  # column name
  id

  # column name
  last_modified

  # column name
  name

  # column name
  parent_id

  # column name
  visibility_status
}

input category_updates {
  # sets the columns of the filtered rows to the given values
  _set: category_set_input
  where: category_bool_exp!
}

# A Relay connection object on "category"
type categoryConnection {
  edges: [categoryEdge!]!
  pageInfo: PageInfo!
}

type categoryEdge {
  cursor: String!
  node: category!
}

# columns and relationships of "dashboard"
type dashboard implements Node {
  author_id: Int!
  created: timestamp!

  # An array relationship
  dashboard_sessions(
    # distinct select on columns
    distinct_on: [dashboard_session_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dashboard_session_order_by!]

    # filter the rows returned
    where: dashboard_session_bool_exp
  ): [dashboard_session!]!

  # An aggregate relationship
  dashboard_sessions_aggregate(
    # distinct select on columns
    distinct_on: [dashboard_session_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dashboard_session_order_by!]

    # filter the rows returned
    where: dashboard_session_bool_exp
  ): dashboard_session_aggregate!

  # An array relationship connection
  dashboard_sessions_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dashboard_session_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dashboard_session_order_by!]

    # filter the rows returned
    where: dashboard_session_bool_exp
  ): dashboard_sessionConnection!
  description: String
  id: ID!
  is_official: Boolean!
  last_modified: timestamp!
  legacy_specification(
    # JSON select path
    path: String
  ): jsonb
  registered_users_can_download_data: Boolean!
  resource_id: Int!
  slug: String
  specification(
    # JSON select path
    path: String
  ): jsonb!
  total_views: Int!
  unregistered_users_can_download_data: Boolean!
}

# append existing jsonb value of filtered columns with new jsonb value
input dashboard_append_input {
  legacy_specification: jsonb
  specification: jsonb
}

# Boolean expression to filter rows from the table "dashboard". All fields are combined with a logical 'AND'.
input dashboard_bool_exp {
  _and: [dashboard_bool_exp!]
  _not: dashboard_bool_exp
  _or: [dashboard_bool_exp!]
  author_id: Int_comparison_exp
  created: timestamp_comparison_exp
  dashboard_sessions: dashboard_session_bool_exp
  description: String_comparison_exp
  id: Int_comparison_exp
  is_official: Boolean_comparison_exp
  last_modified: timestamp_comparison_exp
  legacy_specification: jsonb_comparison_exp
  registered_users_can_download_data: Boolean_comparison_exp
  resource_id: Int_comparison_exp
  slug: String_comparison_exp
  specification: jsonb_comparison_exp
  total_views: Int_comparison_exp
  unregistered_users_can_download_data: Boolean_comparison_exp
}

# unique or primary key constraints on table "dashboard"
enum dashboard_constraint {
  # unique or primary key constraint on columns "id"
  dashboard_pkey

  # unique or primary key constraint on columns "resource_id"
  dashboard_resource_id_key

  # unique or primary key constraint on columns "slug"
  dashboard_slug_key
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input dashboard_delete_at_path_input {
  legacy_specification: [String!]
  specification: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input dashboard_delete_elem_input {
  legacy_specification: Int
  specification: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input dashboard_delete_key_input {
  legacy_specification: String
  specification: String
}

# input type for incrementing numeric columns in table "dashboard"
input dashboard_inc_input {
  author_id: Int
  id: Int
  resource_id: Int
  total_views: Int
}

# input type for inserting data into table "dashboard"
input dashboard_insert_input {
  author_id: Int
  created: timestamp
  dashboard_sessions: dashboard_session_arr_rel_insert_input
  description: String
  id: Int
  is_official: Boolean
  last_modified: timestamp
  legacy_specification: jsonb
  registered_users_can_download_data: Boolean
  resource_id: Int
  slug: String
  specification: jsonb
  total_views: Int
  unregistered_users_can_download_data: Boolean
}

# response of any mutation on the table "dashboard"
type dashboard_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [dashboard!]!
}

# input type for inserting object relation for remote table "dashboard"
input dashboard_obj_rel_insert_input {
  data: dashboard_insert_input!

  # upsert condition
  on_conflict: dashboard_on_conflict
}

# on_conflict condition type for table "dashboard"
input dashboard_on_conflict {
  constraint: dashboard_constraint!
  update_columns: [dashboard_update_column!]! = []
  where: dashboard_bool_exp
}

# Ordering options when selecting data from "dashboard".
input dashboard_order_by {
  author_id: order_by
  created: order_by
  dashboard_sessions_aggregate: dashboard_session_aggregate_order_by
  description: order_by
  id: order_by
  is_official: order_by
  last_modified: order_by
  legacy_specification: order_by
  registered_users_can_download_data: order_by
  resource_id: order_by
  slug: order_by
  specification: order_by
  total_views: order_by
  unregistered_users_can_download_data: order_by
}

# primary key columns input for table: dashboard
input dashboard_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input dashboard_prepend_input {
  legacy_specification: jsonb
  specification: jsonb
}

# select columns of table "dashboard"
enum dashboard_select_column {
  # column name
  author_id

  # column name
  created

  # column name
  description

  # column name
  id

  # column name
  is_official

  # column name
  last_modified

  # column name
  legacy_specification

  # column name
  registered_users_can_download_data

  # column name
  resource_id

  # column name
  slug

  # column name
  specification

  # column name
  total_views

  # column name
  unregistered_users_can_download_data
}

# columns and relationships of "dashboard_session"
type dashboard_session implements Node {
  # An object relationship
  dashboard: dashboard!
  dashboard_id: Int!
  data_blob(
    # JSON select path
    path: String
  ): jsonb
  id: ID!
  uuid: String!
}

# aggregated selection of "dashboard_session"
type dashboard_session_aggregate {
  aggregate: dashboard_session_aggregate_fields
  nodes: [dashboard_session!]!
}

# aggregate fields of "dashboard_session"
type dashboard_session_aggregate_fields {
  avg: dashboard_session_avg_fields
  count(columns: [dashboard_session_select_column!], distinct: Boolean): Int!
  max: dashboard_session_max_fields
  min: dashboard_session_min_fields
  stddev: dashboard_session_stddev_fields
  stddev_pop: dashboard_session_stddev_pop_fields
  stddev_samp: dashboard_session_stddev_samp_fields
  sum: dashboard_session_sum_fields
  var_pop: dashboard_session_var_pop_fields
  var_samp: dashboard_session_var_samp_fields
  variance: dashboard_session_variance_fields
}

# order by aggregate values of table "dashboard_session"
input dashboard_session_aggregate_order_by {
  avg: dashboard_session_avg_order_by
  count: order_by
  max: dashboard_session_max_order_by
  min: dashboard_session_min_order_by
  stddev: dashboard_session_stddev_order_by
  stddev_pop: dashboard_session_stddev_pop_order_by
  stddev_samp: dashboard_session_stddev_samp_order_by
  sum: dashboard_session_sum_order_by
  var_pop: dashboard_session_var_pop_order_by
  var_samp: dashboard_session_var_samp_order_by
  variance: dashboard_session_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input dashboard_session_append_input {
  data_blob: jsonb
}

# input type for inserting array relation for remote table "dashboard_session"
input dashboard_session_arr_rel_insert_input {
  data: [dashboard_session_insert_input!]!

  # upsert condition
  on_conflict: dashboard_session_on_conflict
}

# aggregate avg on columns
type dashboard_session_avg_fields {
  dashboard_id: Float
}

# order by avg() on columns of table "dashboard_session"
input dashboard_session_avg_order_by {
  dashboard_id: order_by
}

# Boolean expression to filter rows from the table "dashboard_session". All fields are combined with a logical 'AND'.
input dashboard_session_bool_exp {
  _and: [dashboard_session_bool_exp!]
  _not: dashboard_session_bool_exp
  _or: [dashboard_session_bool_exp!]
  dashboard: dashboard_bool_exp
  dashboard_id: Int_comparison_exp
  data_blob: jsonb_comparison_exp
  uuid: String_comparison_exp
}

# unique or primary key constraints on table "dashboard_session"
enum dashboard_session_constraint {
  # unique or primary key constraint on columns "uuid"
  dashboard_session_pkey
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input dashboard_session_delete_at_path_input {
  data_blob: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input dashboard_session_delete_elem_input {
  data_blob: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input dashboard_session_delete_key_input {
  data_blob: String
}

# input type for incrementing numeric columns in table "dashboard_session"
input dashboard_session_inc_input {
  dashboard_id: Int
}

# input type for inserting data into table "dashboard_session"
input dashboard_session_insert_input {
  dashboard: dashboard_obj_rel_insert_input
  dashboard_id: Int
  data_blob: jsonb
  uuid: String
}

# aggregate max on columns
type dashboard_session_max_fields {
  dashboard_id: Int
  uuid: String
}

# order by max() on columns of table "dashboard_session"
input dashboard_session_max_order_by {
  dashboard_id: order_by
  uuid: order_by
}

# aggregate min on columns
type dashboard_session_min_fields {
  dashboard_id: Int
  uuid: String
}

# order by min() on columns of table "dashboard_session"
input dashboard_session_min_order_by {
  dashboard_id: order_by
  uuid: order_by
}

# response of any mutation on the table "dashboard_session"
type dashboard_session_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [dashboard_session!]!
}

# on_conflict condition type for table "dashboard_session"
input dashboard_session_on_conflict {
  constraint: dashboard_session_constraint!
  update_columns: [dashboard_session_update_column!]! = []
  where: dashboard_session_bool_exp
}

# Ordering options when selecting data from "dashboard_session".
input dashboard_session_order_by {
  dashboard: dashboard_order_by
  dashboard_id: order_by
  data_blob: order_by
  uuid: order_by
}

# primary key columns input for table: dashboard_session
input dashboard_session_pk_columns_input {
  uuid: String!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input dashboard_session_prepend_input {
  data_blob: jsonb
}

# select columns of table "dashboard_session"
enum dashboard_session_select_column {
  # column name
  dashboard_id

  # column name
  data_blob

  # column name
  uuid
}

# input type for updating data in table "dashboard_session"
input dashboard_session_set_input {
  dashboard_id: Int
  data_blob: jsonb
  uuid: String
}

# aggregate stddev on columns
type dashboard_session_stddev_fields {
  dashboard_id: Float
}

# order by stddev() on columns of table "dashboard_session"
input dashboard_session_stddev_order_by {
  dashboard_id: order_by
}

# aggregate stddev_pop on columns
type dashboard_session_stddev_pop_fields {
  dashboard_id: Float
}

# order by stddev_pop() on columns of table "dashboard_session"
input dashboard_session_stddev_pop_order_by {
  dashboard_id: order_by
}

# aggregate stddev_samp on columns
type dashboard_session_stddev_samp_fields {
  dashboard_id: Float
}

# order by stddev_samp() on columns of table "dashboard_session"
input dashboard_session_stddev_samp_order_by {
  dashboard_id: order_by
}

# aggregate sum on columns
type dashboard_session_sum_fields {
  dashboard_id: Int
}

# order by sum() on columns of table "dashboard_session"
input dashboard_session_sum_order_by {
  dashboard_id: order_by
}

# update columns of table "dashboard_session"
enum dashboard_session_update_column {
  # column name
  dashboard_id

  # column name
  data_blob

  # column name
  uuid
}

input dashboard_session_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: dashboard_session_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: dashboard_session_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: dashboard_session_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: dashboard_session_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: dashboard_session_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: dashboard_session_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: dashboard_session_set_input
  where: dashboard_session_bool_exp!
}

# aggregate var_pop on columns
type dashboard_session_var_pop_fields {
  dashboard_id: Float
}

# order by var_pop() on columns of table "dashboard_session"
input dashboard_session_var_pop_order_by {
  dashboard_id: order_by
}

# aggregate var_samp on columns
type dashboard_session_var_samp_fields {
  dashboard_id: Float
}

# order by var_samp() on columns of table "dashboard_session"
input dashboard_session_var_samp_order_by {
  dashboard_id: order_by
}

# aggregate variance on columns
type dashboard_session_variance_fields {
  dashboard_id: Float
}

# order by variance() on columns of table "dashboard_session"
input dashboard_session_variance_order_by {
  dashboard_id: order_by
}

# A Relay connection object on "dashboard_session"
type dashboard_sessionConnection {
  edges: [dashboard_sessionEdge!]!
  pageInfo: PageInfo!
}

type dashboard_sessionEdge {
  cursor: String!
  node: dashboard_session!
}

# input type for updating data in table "dashboard"
input dashboard_set_input {
  author_id: Int
  created: timestamp
  description: String
  id: Int
  is_official: Boolean
  last_modified: timestamp
  legacy_specification: jsonb
  registered_users_can_download_data: Boolean
  resource_id: Int
  slug: String
  specification: jsonb
  total_views: Int
  unregistered_users_can_download_data: Boolean
}

# update columns of table "dashboard"
enum dashboard_update_column {
  # column name
  author_id

  # column name
  created

  # column name
  description

  # column name
  id

  # column name
  is_official

  # column name
  last_modified

  # column name
  legacy_specification

  # column name
  registered_users_can_download_data

  # column name
  resource_id

  # column name
  slug

  # column name
  specification

  # column name
  total_views

  # column name
  unregistered_users_can_download_data
}

input dashboard_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: dashboard_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: dashboard_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: dashboard_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: dashboard_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: dashboard_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: dashboard_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: dashboard_set_input
  where: dashboard_bool_exp!
}

# A Relay connection object on "dashboard"
type dashboardConnection {
  edges: [dashboardEdge!]!
  pageInfo: PageInfo!
}

type dashboardEdge {
  cursor: String!
  node: dashboard!
}

# columns and relationships of "data_upload_file_summary"
type data_upload_file_summary implements Node {
  column_mapping(
    # JSON select path
    path: String
  ): jsonb!
  created: timestamp!
  file_path: String!
  id: ID!
  last_modified: timestamp!

  # An object relationship
  self_serve_source: self_serve_source
  self_serve_source_id: Int
  source_id: String!
  user_file_name: String!
}

# aggregated selection of "data_upload_file_summary"
type data_upload_file_summary_aggregate {
  aggregate: data_upload_file_summary_aggregate_fields
  nodes: [data_upload_file_summary!]!
}

# aggregate fields of "data_upload_file_summary"
type data_upload_file_summary_aggregate_fields {
  avg: data_upload_file_summary_avg_fields
  count(columns: [data_upload_file_summary_select_column!], distinct: Boolean): Int!
  max: data_upload_file_summary_max_fields
  min: data_upload_file_summary_min_fields
  stddev: data_upload_file_summary_stddev_fields
  stddev_pop: data_upload_file_summary_stddev_pop_fields
  stddev_samp: data_upload_file_summary_stddev_samp_fields
  sum: data_upload_file_summary_sum_fields
  var_pop: data_upload_file_summary_var_pop_fields
  var_samp: data_upload_file_summary_var_samp_fields
  variance: data_upload_file_summary_variance_fields
}

# order by aggregate values of table "data_upload_file_summary"
input data_upload_file_summary_aggregate_order_by {
  avg: data_upload_file_summary_avg_order_by
  count: order_by
  max: data_upload_file_summary_max_order_by
  min: data_upload_file_summary_min_order_by
  stddev: data_upload_file_summary_stddev_order_by
  stddev_pop: data_upload_file_summary_stddev_pop_order_by
  stddev_samp: data_upload_file_summary_stddev_samp_order_by
  sum: data_upload_file_summary_sum_order_by
  var_pop: data_upload_file_summary_var_pop_order_by
  var_samp: data_upload_file_summary_var_samp_order_by
  variance: data_upload_file_summary_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input data_upload_file_summary_append_input {
  column_mapping: jsonb
}

# input type for inserting array relation for remote table "data_upload_file_summary"
input data_upload_file_summary_arr_rel_insert_input {
  data: [data_upload_file_summary_insert_input!]!

  # upsert condition
  on_conflict: data_upload_file_summary_on_conflict
}

# aggregate avg on columns
type data_upload_file_summary_avg_fields {
  id: Float
  self_serve_source_id: Float
}

# order by avg() on columns of table "data_upload_file_summary"
input data_upload_file_summary_avg_order_by {
  id: order_by
  self_serve_source_id: order_by
}

# Boolean expression to filter rows from the table "data_upload_file_summary". All fields are combined with a logical 'AND'.
input data_upload_file_summary_bool_exp {
  _and: [data_upload_file_summary_bool_exp!]
  _not: data_upload_file_summary_bool_exp
  _or: [data_upload_file_summary_bool_exp!]
  column_mapping: jsonb_comparison_exp
  created: timestamp_comparison_exp
  file_path: String_comparison_exp
  id: Int_comparison_exp
  last_modified: timestamp_comparison_exp
  self_serve_source: self_serve_source_bool_exp
  self_serve_source_id: Int_comparison_exp
  source_id: String_comparison_exp
  user_file_name: String_comparison_exp
}

# unique or primary key constraints on table "data_upload_file_summary"
enum data_upload_file_summary_constraint {
  # unique or primary key constraint on columns "id"
  data_upload_file_summary_pkey
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input data_upload_file_summary_delete_at_path_input {
  column_mapping: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input data_upload_file_summary_delete_elem_input {
  column_mapping: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input data_upload_file_summary_delete_key_input {
  column_mapping: String
}

# input type for incrementing numeric columns in table "data_upload_file_summary"
input data_upload_file_summary_inc_input {
  id: Int
  self_serve_source_id: Int
}

# input type for inserting data into table "data_upload_file_summary"
input data_upload_file_summary_insert_input {
  column_mapping: jsonb
  created: timestamp
  file_path: String
  id: Int
  last_modified: timestamp
  self_serve_source: self_serve_source_obj_rel_insert_input
  self_serve_source_id: Int
  source_id: String
  user_file_name: String
}

# aggregate max on columns
type data_upload_file_summary_max_fields {
  created: timestamp
  file_path: String
  id: Int
  last_modified: timestamp
  self_serve_source_id: Int
  source_id: String
  user_file_name: String
}

# order by max() on columns of table "data_upload_file_summary"
input data_upload_file_summary_max_order_by {
  created: order_by
  file_path: order_by
  id: order_by
  last_modified: order_by
  self_serve_source_id: order_by
  source_id: order_by
  user_file_name: order_by
}

# aggregate min on columns
type data_upload_file_summary_min_fields {
  created: timestamp
  file_path: String
  id: Int
  last_modified: timestamp
  self_serve_source_id: Int
  source_id: String
  user_file_name: String
}

# order by min() on columns of table "data_upload_file_summary"
input data_upload_file_summary_min_order_by {
  created: order_by
  file_path: order_by
  id: order_by
  last_modified: order_by
  self_serve_source_id: order_by
  source_id: order_by
  user_file_name: order_by
}

# response of any mutation on the table "data_upload_file_summary"
type data_upload_file_summary_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [data_upload_file_summary!]!
}

# on_conflict condition type for table "data_upload_file_summary"
input data_upload_file_summary_on_conflict {
  constraint: data_upload_file_summary_constraint!
  update_columns: [data_upload_file_summary_update_column!]! = []
  where: data_upload_file_summary_bool_exp
}

# Ordering options when selecting data from "data_upload_file_summary".
input data_upload_file_summary_order_by {
  column_mapping: order_by
  created: order_by
  file_path: order_by
  id: order_by
  last_modified: order_by
  self_serve_source: self_serve_source_order_by
  self_serve_source_id: order_by
  source_id: order_by
  user_file_name: order_by
}

# primary key columns input for table: data_upload_file_summary
input data_upload_file_summary_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input data_upload_file_summary_prepend_input {
  column_mapping: jsonb
}

# select columns of table "data_upload_file_summary"
enum data_upload_file_summary_select_column {
  # column name
  column_mapping

  # column name
  created

  # column name
  file_path

  # column name
  id

  # column name
  last_modified

  # column name
  self_serve_source_id

  # column name
  source_id

  # column name
  user_file_name
}

# input type for updating data in table "data_upload_file_summary"
input data_upload_file_summary_set_input {
  column_mapping: jsonb
  created: timestamp
  file_path: String
  id: Int
  last_modified: timestamp
  self_serve_source_id: Int
  source_id: String
  user_file_name: String
}

# aggregate stddev on columns
type data_upload_file_summary_stddev_fields {
  id: Float
  self_serve_source_id: Float
}

# order by stddev() on columns of table "data_upload_file_summary"
input data_upload_file_summary_stddev_order_by {
  id: order_by
  self_serve_source_id: order_by
}

# aggregate stddev_pop on columns
type data_upload_file_summary_stddev_pop_fields {
  id: Float
  self_serve_source_id: Float
}

# order by stddev_pop() on columns of table "data_upload_file_summary"
input data_upload_file_summary_stddev_pop_order_by {
  id: order_by
  self_serve_source_id: order_by
}

# aggregate stddev_samp on columns
type data_upload_file_summary_stddev_samp_fields {
  id: Float
  self_serve_source_id: Float
}

# order by stddev_samp() on columns of table "data_upload_file_summary"
input data_upload_file_summary_stddev_samp_order_by {
  id: order_by
  self_serve_source_id: order_by
}

# aggregate sum on columns
type data_upload_file_summary_sum_fields {
  id: Int
  self_serve_source_id: Int
}

# order by sum() on columns of table "data_upload_file_summary"
input data_upload_file_summary_sum_order_by {
  id: order_by
  self_serve_source_id: order_by
}

# update columns of table "data_upload_file_summary"
enum data_upload_file_summary_update_column {
  # column name
  column_mapping

  # column name
  created

  # column name
  file_path

  # column name
  id

  # column name
  last_modified

  # column name
  self_serve_source_id

  # column name
  source_id

  # column name
  user_file_name
}

input data_upload_file_summary_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: data_upload_file_summary_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: data_upload_file_summary_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: data_upload_file_summary_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: data_upload_file_summary_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: data_upload_file_summary_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: data_upload_file_summary_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: data_upload_file_summary_set_input
  where: data_upload_file_summary_bool_exp!
}

# aggregate var_pop on columns
type data_upload_file_summary_var_pop_fields {
  id: Float
  self_serve_source_id: Float
}

# order by var_pop() on columns of table "data_upload_file_summary"
input data_upload_file_summary_var_pop_order_by {
  id: order_by
  self_serve_source_id: order_by
}

# aggregate var_samp on columns
type data_upload_file_summary_var_samp_fields {
  id: Float
  self_serve_source_id: Float
}

# order by var_samp() on columns of table "data_upload_file_summary"
input data_upload_file_summary_var_samp_order_by {
  id: order_by
  self_serve_source_id: order_by
}

# aggregate variance on columns
type data_upload_file_summary_variance_fields {
  id: Float
  self_serve_source_id: Float
}

# order by variance() on columns of table "data_upload_file_summary"
input data_upload_file_summary_variance_order_by {
  id: order_by
  self_serve_source_id: order_by
}

# A Relay connection object on "data_upload_file_summary"
type data_upload_file_summaryConnection {
  edges: [data_upload_file_summaryEdge!]!
  pageInfo: PageInfo!
}

type data_upload_file_summaryEdge {
  cursor: String!
  node: data_upload_file_summary!
}

# columns and relationships of "dataprep_flow"
type dataprep_flow implements Node {
  appendable: Boolean!
  created: timestamp!

  # An array relationship
  dataprep_jobs(
    # distinct select on columns
    distinct_on: [dataprep_job_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dataprep_job_order_by!]

    # filter the rows returned
    where: dataprep_job_bool_exp
  ): [dataprep_job!]!

  # An aggregate relationship
  dataprep_jobs_aggregate(
    # distinct select on columns
    distinct_on: [dataprep_job_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dataprep_job_order_by!]

    # filter the rows returned
    where: dataprep_job_bool_exp
  ): dataprep_job_aggregate!

  # An array relationship connection
  dataprep_jobs_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dataprep_job_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dataprep_job_order_by!]

    # filter the rows returned
    where: dataprep_job_bool_exp
  ): dataprep_jobConnection!
  expected_columns(
    # JSON select path
    path: String
  ): jsonb!
  id: ID!
  last_modified: timestamp!
  recipe_id: Int!

  # An array relationship
  self_serve_sources(
    # distinct select on columns
    distinct_on: [self_serve_source_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [self_serve_source_order_by!]

    # filter the rows returned
    where: self_serve_source_bool_exp
  ): [self_serve_source!]!

  # An aggregate relationship
  self_serve_sources_aggregate(
    # distinct select on columns
    distinct_on: [self_serve_source_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [self_serve_source_order_by!]

    # filter the rows returned
    where: self_serve_source_bool_exp
  ): self_serve_source_aggregate!

  # An array relationship connection
  self_serve_sources_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [self_serve_source_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [self_serve_source_order_by!]

    # filter the rows returned
    where: self_serve_source_bool_exp
  ): self_serve_sourceConnection!
}

# append existing jsonb value of filtered columns with new jsonb value
input dataprep_flow_append_input {
  expected_columns: jsonb
}

# Boolean expression to filter rows from the table "dataprep_flow". All fields are combined with a logical 'AND'.
input dataprep_flow_bool_exp {
  _and: [dataprep_flow_bool_exp!]
  _not: dataprep_flow_bool_exp
  _or: [dataprep_flow_bool_exp!]
  appendable: Boolean_comparison_exp
  created: timestamp_comparison_exp
  dataprep_jobs: dataprep_job_bool_exp
  expected_columns: jsonb_comparison_exp
  id: Int_comparison_exp
  last_modified: timestamp_comparison_exp
  recipe_id: Int_comparison_exp
  self_serve_sources: self_serve_source_bool_exp
}

# unique or primary key constraints on table "dataprep_flow"
enum dataprep_flow_constraint {
  # unique or primary key constraint on columns "id"
  dataprep_flow_pkey

  # unique or primary key constraint on columns "recipe_id"
  dataprep_flow_recipe_id_key
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input dataprep_flow_delete_at_path_input {
  expected_columns: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input dataprep_flow_delete_elem_input {
  expected_columns: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input dataprep_flow_delete_key_input {
  expected_columns: String
}

# input type for incrementing numeric columns in table "dataprep_flow"
input dataprep_flow_inc_input {
  id: Int
  recipe_id: Int
}

# input type for inserting data into table "dataprep_flow"
input dataprep_flow_insert_input {
  appendable: Boolean
  created: timestamp
  dataprep_jobs: dataprep_job_arr_rel_insert_input
  expected_columns: jsonb
  id: Int
  last_modified: timestamp
  recipe_id: Int
  self_serve_sources: self_serve_source_arr_rel_insert_input
}

# response of any mutation on the table "dataprep_flow"
type dataprep_flow_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [dataprep_flow!]!
}

# input type for inserting object relation for remote table "dataprep_flow"
input dataprep_flow_obj_rel_insert_input {
  data: dataprep_flow_insert_input!

  # upsert condition
  on_conflict: dataprep_flow_on_conflict
}

# on_conflict condition type for table "dataprep_flow"
input dataprep_flow_on_conflict {
  constraint: dataprep_flow_constraint!
  update_columns: [dataprep_flow_update_column!]! = []
  where: dataprep_flow_bool_exp
}

# Ordering options when selecting data from "dataprep_flow".
input dataprep_flow_order_by {
  appendable: order_by
  created: order_by
  dataprep_jobs_aggregate: dataprep_job_aggregate_order_by
  expected_columns: order_by
  id: order_by
  last_modified: order_by
  recipe_id: order_by
  self_serve_sources_aggregate: self_serve_source_aggregate_order_by
}

# primary key columns input for table: dataprep_flow
input dataprep_flow_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input dataprep_flow_prepend_input {
  expected_columns: jsonb
}

# select columns of table "dataprep_flow"
enum dataprep_flow_select_column {
  # column name
  appendable

  # column name
  created

  # column name
  expected_columns

  # column name
  id

  # column name
  last_modified

  # column name
  recipe_id
}

# input type for updating data in table "dataprep_flow"
input dataprep_flow_set_input {
  appendable: Boolean
  created: timestamp
  expected_columns: jsonb
  id: Int
  last_modified: timestamp
  recipe_id: Int
}

# update columns of table "dataprep_flow"
enum dataprep_flow_update_column {
  # column name
  appendable

  # column name
  created

  # column name
  expected_columns

  # column name
  id

  # column name
  last_modified

  # column name
  recipe_id
}

input dataprep_flow_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: dataprep_flow_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: dataprep_flow_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: dataprep_flow_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: dataprep_flow_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: dataprep_flow_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: dataprep_flow_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: dataprep_flow_set_input
  where: dataprep_flow_bool_exp!
}

# A Relay connection object on "dataprep_flow"
type dataprep_flowConnection {
  edges: [dataprep_flowEdge!]!
  pageInfo: PageInfo!
}

type dataprep_flowEdge {
  cursor: String!
  node: dataprep_flow!
}

# columns and relationships of "dataprep_job"
type dataprep_job implements Node {
  created: timestamp!
  created_on_dataprep: timestamp

  # An object relationship
  dataprep_flow: dataprep_flow!
  dataprep_flow_id: Int!
  id: ID!
  job_id: Int
  last_modified: timestamp!
  last_modified_on_dataprep: timestamp
  status: String
}

# aggregated selection of "dataprep_job"
type dataprep_job_aggregate {
  aggregate: dataprep_job_aggregate_fields
  nodes: [dataprep_job!]!
}

# aggregate fields of "dataprep_job"
type dataprep_job_aggregate_fields {
  avg: dataprep_job_avg_fields
  count(columns: [dataprep_job_select_column!], distinct: Boolean): Int!
  max: dataprep_job_max_fields
  min: dataprep_job_min_fields
  stddev: dataprep_job_stddev_fields
  stddev_pop: dataprep_job_stddev_pop_fields
  stddev_samp: dataprep_job_stddev_samp_fields
  sum: dataprep_job_sum_fields
  var_pop: dataprep_job_var_pop_fields
  var_samp: dataprep_job_var_samp_fields
  variance: dataprep_job_variance_fields
}

# order by aggregate values of table "dataprep_job"
input dataprep_job_aggregate_order_by {
  avg: dataprep_job_avg_order_by
  count: order_by
  max: dataprep_job_max_order_by
  min: dataprep_job_min_order_by
  stddev: dataprep_job_stddev_order_by
  stddev_pop: dataprep_job_stddev_pop_order_by
  stddev_samp: dataprep_job_stddev_samp_order_by
  sum: dataprep_job_sum_order_by
  var_pop: dataprep_job_var_pop_order_by
  var_samp: dataprep_job_var_samp_order_by
  variance: dataprep_job_variance_order_by
}

# input type for inserting array relation for remote table "dataprep_job"
input dataprep_job_arr_rel_insert_input {
  data: [dataprep_job_insert_input!]!

  # upsert condition
  on_conflict: dataprep_job_on_conflict
}

# aggregate avg on columns
type dataprep_job_avg_fields {
  dataprep_flow_id: Float
  id: Float
  job_id: Float
}

# order by avg() on columns of table "dataprep_job"
input dataprep_job_avg_order_by {
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
}

# Boolean expression to filter rows from the table "dataprep_job". All fields are combined with a logical 'AND'.
input dataprep_job_bool_exp {
  _and: [dataprep_job_bool_exp!]
  _not: dataprep_job_bool_exp
  _or: [dataprep_job_bool_exp!]
  created: timestamp_comparison_exp
  created_on_dataprep: timestamp_comparison_exp
  dataprep_flow: dataprep_flow_bool_exp
  dataprep_flow_id: Int_comparison_exp
  id: Int_comparison_exp
  job_id: Int_comparison_exp
  last_modified: timestamp_comparison_exp
  last_modified_on_dataprep: timestamp_comparison_exp
  status: String_comparison_exp
}

# unique or primary key constraints on table "dataprep_job"
enum dataprep_job_constraint {
  # unique or primary key constraint on columns "id"
  dataprep_job_pkey
}

# input type for incrementing numeric columns in table "dataprep_job"
input dataprep_job_inc_input {
  dataprep_flow_id: Int
  id: Int
  job_id: Int
}

# input type for inserting data into table "dataprep_job"
input dataprep_job_insert_input {
  created: timestamp
  created_on_dataprep: timestamp
  dataprep_flow: dataprep_flow_obj_rel_insert_input
  dataprep_flow_id: Int
  id: Int
  job_id: Int
  last_modified: timestamp
  last_modified_on_dataprep: timestamp
  status: String
}

# aggregate max on columns
type dataprep_job_max_fields {
  created: timestamp
  created_on_dataprep: timestamp
  dataprep_flow_id: Int
  id: Int
  job_id: Int
  last_modified: timestamp
  last_modified_on_dataprep: timestamp
  status: String
}

# order by max() on columns of table "dataprep_job"
input dataprep_job_max_order_by {
  created: order_by
  created_on_dataprep: order_by
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
  last_modified: order_by
  last_modified_on_dataprep: order_by
  status: order_by
}

# aggregate min on columns
type dataprep_job_min_fields {
  created: timestamp
  created_on_dataprep: timestamp
  dataprep_flow_id: Int
  id: Int
  job_id: Int
  last_modified: timestamp
  last_modified_on_dataprep: timestamp
  status: String
}

# order by min() on columns of table "dataprep_job"
input dataprep_job_min_order_by {
  created: order_by
  created_on_dataprep: order_by
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
  last_modified: order_by
  last_modified_on_dataprep: order_by
  status: order_by
}

# response of any mutation on the table "dataprep_job"
type dataprep_job_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [dataprep_job!]!
}

# on_conflict condition type for table "dataprep_job"
input dataprep_job_on_conflict {
  constraint: dataprep_job_constraint!
  update_columns: [dataprep_job_update_column!]! = []
  where: dataprep_job_bool_exp
}

# Ordering options when selecting data from "dataprep_job".
input dataprep_job_order_by {
  created: order_by
  created_on_dataprep: order_by
  dataprep_flow: dataprep_flow_order_by
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
  last_modified: order_by
  last_modified_on_dataprep: order_by
  status: order_by
}

# primary key columns input for table: dataprep_job
input dataprep_job_pk_columns_input {
  id: Int!
}

# select columns of table "dataprep_job"
enum dataprep_job_select_column {
  # column name
  created

  # column name
  created_on_dataprep

  # column name
  dataprep_flow_id

  # column name
  id

  # column name
  job_id

  # column name
  last_modified

  # column name
  last_modified_on_dataprep

  # column name
  status
}

# input type for updating data in table "dataprep_job"
input dataprep_job_set_input {
  created: timestamp
  created_on_dataprep: timestamp
  dataprep_flow_id: Int
  id: Int
  job_id: Int
  last_modified: timestamp
  last_modified_on_dataprep: timestamp
  status: String
}

# aggregate stddev on columns
type dataprep_job_stddev_fields {
  dataprep_flow_id: Float
  id: Float
  job_id: Float
}

# order by stddev() on columns of table "dataprep_job"
input dataprep_job_stddev_order_by {
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
}

# aggregate stddev_pop on columns
type dataprep_job_stddev_pop_fields {
  dataprep_flow_id: Float
  id: Float
  job_id: Float
}

# order by stddev_pop() on columns of table "dataprep_job"
input dataprep_job_stddev_pop_order_by {
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
}

# aggregate stddev_samp on columns
type dataprep_job_stddev_samp_fields {
  dataprep_flow_id: Float
  id: Float
  job_id: Float
}

# order by stddev_samp() on columns of table "dataprep_job"
input dataprep_job_stddev_samp_order_by {
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
}

# aggregate sum on columns
type dataprep_job_sum_fields {
  dataprep_flow_id: Int
  id: Int
  job_id: Int
}

# order by sum() on columns of table "dataprep_job"
input dataprep_job_sum_order_by {
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
}

# update columns of table "dataprep_job"
enum dataprep_job_update_column {
  # column name
  created

  # column name
  created_on_dataprep

  # column name
  dataprep_flow_id

  # column name
  id

  # column name
  job_id

  # column name
  last_modified

  # column name
  last_modified_on_dataprep

  # column name
  status
}

input dataprep_job_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: dataprep_job_inc_input

  # sets the columns of the filtered rows to the given values
  _set: dataprep_job_set_input
  where: dataprep_job_bool_exp!
}

# aggregate var_pop on columns
type dataprep_job_var_pop_fields {
  dataprep_flow_id: Float
  id: Float
  job_id: Float
}

# order by var_pop() on columns of table "dataprep_job"
input dataprep_job_var_pop_order_by {
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
}

# aggregate var_samp on columns
type dataprep_job_var_samp_fields {
  dataprep_flow_id: Float
  id: Float
  job_id: Float
}

# order by var_samp() on columns of table "dataprep_job"
input dataprep_job_var_samp_order_by {
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
}

# aggregate variance on columns
type dataprep_job_variance_fields {
  dataprep_flow_id: Float
  id: Float
  job_id: Float
}

# order by variance() on columns of table "dataprep_job"
input dataprep_job_variance_order_by {
  dataprep_flow_id: order_by
  id: order_by
  job_id: order_by
}

# A Relay connection object on "dataprep_job"
type dataprep_jobConnection {
  edges: [dataprep_jobEdge!]!
  pageInfo: PageInfo!
}

type dataprep_jobEdge {
  cursor: String!
  node: dataprep_job!
}

# columns and relationships of "dimension"
type dimension implements Node {
  authorizable: Boolean!
  created: timestamp!
  description: String

  # An array relationship
  dimension_category_mappings(
    # distinct select on columns
    distinct_on: [dimension_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_mapping_order_by!]

    # filter the rows returned
    where: dimension_category_mapping_bool_exp
  ): [dimension_category_mapping!]!

  # An aggregate relationship
  dimension_category_mappings_aggregate(
    # distinct select on columns
    distinct_on: [dimension_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_mapping_order_by!]

    # filter the rows returned
    where: dimension_category_mapping_bool_exp
  ): dimension_category_mapping_aggregate!

  # An array relationship connection
  dimension_category_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_mapping_order_by!]

    # filter the rows returned
    where: dimension_category_mapping_bool_exp
  ): dimension_category_mappingConnection!

  # An array relationship
  field_dimension_mappings(
    # distinct select on columns
    distinct_on: [field_dimension_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_dimension_mapping_order_by!]

    # filter the rows returned
    where: field_dimension_mapping_bool_exp
  ): [field_dimension_mapping!]!

  # An aggregate relationship
  field_dimension_mappings_aggregate(
    # distinct select on columns
    distinct_on: [field_dimension_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_dimension_mapping_order_by!]

    # filter the rows returned
    where: field_dimension_mapping_bool_exp
  ): field_dimension_mapping_aggregate!

  # An array relationship connection
  field_dimension_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_dimension_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_dimension_mapping_order_by!]

    # filter the rows returned
    where: field_dimension_mapping_bool_exp
  ): field_dimension_mappingConnection!
  filterable: Boolean!

  # An array relationship
  geoDimensionMetadataByLatId(
    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): [geo_dimension_metadata!]!

  # An aggregate relationship
  geoDimensionMetadataByLatId_aggregate(
    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): geo_dimension_metadata_aggregate!

  # An array relationship connection
  geoDimensionMetadataByLatId_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): geo_dimension_metadataConnection!

  # An array relationship
  geoDimensionMetadataByLonId(
    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): [geo_dimension_metadata!]!

  # An aggregate relationship
  geoDimensionMetadataByLonId_aggregate(
    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): geo_dimension_metadata_aggregate!

  # An array relationship connection
  geoDimensionMetadataByLonId_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): geo_dimension_metadataConnection!

  # An array relationship
  geo_dimension_metadata(
    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): [geo_dimension_metadata!]!

  # An aggregate relationship
  geo_dimension_metadata_aggregate(
    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): geo_dimension_metadata_aggregate!

  # An array relationship connection
  geo_dimension_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): geo_dimension_metadataConnection!

  # An array relationship
  hierarchicalDimensionMetadataByUniqueIdentifierDimensionId(
    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): [hierarchical_dimension_metadata!]!

  # An aggregate relationship
  hierarchicalDimensionMetadataByUniqueIdentifierDimensionId_aggregate(
    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): hierarchical_dimension_metadata_aggregate!

  # An array relationship connection
  hierarchicalDimensionMetadataByUniqueIdentifierDimensionId_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): hierarchical_dimension_metadataConnection!

  # An array relationship
  hierarchical_dimension_metadata(
    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): [hierarchical_dimension_metadata!]!

  # An aggregate relationship
  hierarchical_dimension_metadata_aggregate(
    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): hierarchical_dimension_metadata_aggregate!

  # An array relationship connection
  hierarchical_dimension_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): hierarchical_dimension_metadataConnection!
  id: ID!
  last_modified: timestamp!
  name: String!

  # An array relationship
  non_hierarchical_dimensions(
    # distinct select on columns
    distinct_on: [non_hierarchical_dimension_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [non_hierarchical_dimension_order_by!]

    # filter the rows returned
    where: non_hierarchical_dimension_bool_exp
  ): [non_hierarchical_dimension!]!

  # An aggregate relationship
  non_hierarchical_dimensions_aggregate(
    # distinct select on columns
    distinct_on: [non_hierarchical_dimension_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [non_hierarchical_dimension_order_by!]

    # filter the rows returned
    where: non_hierarchical_dimension_bool_exp
  ): non_hierarchical_dimension_aggregate!

  # An array relationship connection
  non_hierarchical_dimensions_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [non_hierarchical_dimension_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [non_hierarchical_dimension_order_by!]

    # filter the rows returned
    where: non_hierarchical_dimension_bool_exp
  ): non_hierarchical_dimensionConnection!

  # An array relationship
  unpublished_field_dimension_mappings(
    # distinct select on columns
    distinct_on: [unpublished_field_dimension_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_dimension_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_dimension_mapping_bool_exp
  ): [unpublished_field_dimension_mapping!]!

  # An aggregate relationship
  unpublished_field_dimension_mappings_aggregate(
    # distinct select on columns
    distinct_on: [unpublished_field_dimension_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_dimension_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_dimension_mapping_bool_exp
  ): unpublished_field_dimension_mapping_aggregate!

  # An array relationship connection
  unpublished_field_dimension_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_dimension_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_dimension_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_dimension_mapping_bool_exp
  ): unpublished_field_dimension_mappingConnection!
}

# Boolean expression to filter rows from the table "dimension". All fields are combined with a logical 'AND'.
input dimension_bool_exp {
  _and: [dimension_bool_exp!]
  _not: dimension_bool_exp
  _or: [dimension_bool_exp!]
  authorizable: Boolean_comparison_exp
  created: timestamp_comparison_exp
  description: String_comparison_exp
  dimension_category_mappings: dimension_category_mapping_bool_exp
  field_dimension_mappings: field_dimension_mapping_bool_exp
  filterable: Boolean_comparison_exp
  geoDimensionMetadataByLatId: geo_dimension_metadata_bool_exp
  geoDimensionMetadataByLonId: geo_dimension_metadata_bool_exp
  geo_dimension_metadata: geo_dimension_metadata_bool_exp
  hierarchicalDimensionMetadataByUniqueIdentifierDimensionId: hierarchical_dimension_metadata_bool_exp
  hierarchical_dimension_metadata: hierarchical_dimension_metadata_bool_exp
  id: String_comparison_exp
  last_modified: timestamp_comparison_exp
  name: String_comparison_exp
  non_hierarchical_dimensions: non_hierarchical_dimension_bool_exp
  unpublished_field_dimension_mappings: unpublished_field_dimension_mapping_bool_exp
}

# columns and relationships of "dimension_category"
type dimension_category implements Node {
  # An array relationship
  children(
    # distinct select on columns
    distinct_on: [dimension_category_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_order_by!]

    # filter the rows returned
    where: dimension_category_bool_exp
  ): [dimension_category!]!

  # An aggregate relationship
  children_aggregate(
    # distinct select on columns
    distinct_on: [dimension_category_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_order_by!]

    # filter the rows returned
    where: dimension_category_bool_exp
  ): dimension_category_aggregate!

  # An array relationship connection
  children_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_category_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_order_by!]

    # filter the rows returned
    where: dimension_category_bool_exp
  ): dimension_categoryConnection!
  created: timestamp!

  # An array relationship
  dimension_category_mappings(
    # distinct select on columns
    distinct_on: [dimension_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_mapping_order_by!]

    # filter the rows returned
    where: dimension_category_mapping_bool_exp
  ): [dimension_category_mapping!]!

  # An aggregate relationship
  dimension_category_mappings_aggregate(
    # distinct select on columns
    distinct_on: [dimension_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_mapping_order_by!]

    # filter the rows returned
    where: dimension_category_mapping_bool_exp
  ): dimension_category_mapping_aggregate!

  # An array relationship connection
  dimension_category_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_mapping_order_by!]

    # filter the rows returned
    where: dimension_category_mapping_bool_exp
  ): dimension_category_mappingConnection!
  id: ID!
  last_modified: timestamp!
  name: String!

  # An object relationship
  parent: dimension_category
  parent_id: String
}

# aggregated selection of "dimension_category"
type dimension_category_aggregate {
  aggregate: dimension_category_aggregate_fields
  nodes: [dimension_category!]!
}

# aggregate fields of "dimension_category"
type dimension_category_aggregate_fields {
  count(columns: [dimension_category_select_column!], distinct: Boolean): Int!
  max: dimension_category_max_fields
  min: dimension_category_min_fields
}

# order by aggregate values of table "dimension_category"
input dimension_category_aggregate_order_by {
  count: order_by
  max: dimension_category_max_order_by
  min: dimension_category_min_order_by
}

# input type for inserting array relation for remote table "dimension_category"
input dimension_category_arr_rel_insert_input {
  data: [dimension_category_insert_input!]!

  # upsert condition
  on_conflict: dimension_category_on_conflict
}

# Boolean expression to filter rows from the table "dimension_category". All fields are combined with a logical 'AND'.
input dimension_category_bool_exp {
  _and: [dimension_category_bool_exp!]
  _not: dimension_category_bool_exp
  _or: [dimension_category_bool_exp!]
  children: dimension_category_bool_exp
  created: timestamp_comparison_exp
  dimension_category_mappings: dimension_category_mapping_bool_exp
  id: String_comparison_exp
  last_modified: timestamp_comparison_exp
  name: String_comparison_exp
  parent: dimension_category_bool_exp
  parent_id: String_comparison_exp
}

# unique or primary key constraints on table "dimension_category"
enum dimension_category_constraint {
  # unique or primary key constraint on columns "id"
  dimension_category_pkey
}

# input type for inserting data into table "dimension_category"
input dimension_category_insert_input {
  children: dimension_category_arr_rel_insert_input
  created: timestamp
  dimension_category_mappings: dimension_category_mapping_arr_rel_insert_input
  id: String
  last_modified: timestamp
  name: String
  parent: dimension_category_obj_rel_insert_input
  parent_id: String
}

# columns and relationships of "dimension_category_mapping"
type dimension_category_mapping implements Node {
  category_id: String!
  created: timestamp!

  # An object relationship
  dimension: dimension!

  # An object relationship
  dimension_category: dimension_category!
  dimension_id: String!
  id: ID!
  last_modified: timestamp!
}

# aggregated selection of "dimension_category_mapping"
type dimension_category_mapping_aggregate {
  aggregate: dimension_category_mapping_aggregate_fields
  nodes: [dimension_category_mapping!]!
}

# aggregate fields of "dimension_category_mapping"
type dimension_category_mapping_aggregate_fields {
  avg: dimension_category_mapping_avg_fields
  count(columns: [dimension_category_mapping_select_column!], distinct: Boolean): Int!
  max: dimension_category_mapping_max_fields
  min: dimension_category_mapping_min_fields
  stddev: dimension_category_mapping_stddev_fields
  stddev_pop: dimension_category_mapping_stddev_pop_fields
  stddev_samp: dimension_category_mapping_stddev_samp_fields
  sum: dimension_category_mapping_sum_fields
  var_pop: dimension_category_mapping_var_pop_fields
  var_samp: dimension_category_mapping_var_samp_fields
  variance: dimension_category_mapping_variance_fields
}

# order by aggregate values of table "dimension_category_mapping"
input dimension_category_mapping_aggregate_order_by {
  avg: dimension_category_mapping_avg_order_by
  count: order_by
  max: dimension_category_mapping_max_order_by
  min: dimension_category_mapping_min_order_by
  stddev: dimension_category_mapping_stddev_order_by
  stddev_pop: dimension_category_mapping_stddev_pop_order_by
  stddev_samp: dimension_category_mapping_stddev_samp_order_by
  sum: dimension_category_mapping_sum_order_by
  var_pop: dimension_category_mapping_var_pop_order_by
  var_samp: dimension_category_mapping_var_samp_order_by
  variance: dimension_category_mapping_variance_order_by
}

# input type for inserting array relation for remote table "dimension_category_mapping"
input dimension_category_mapping_arr_rel_insert_input {
  data: [dimension_category_mapping_insert_input!]!

  # upsert condition
  on_conflict: dimension_category_mapping_on_conflict
}

# aggregate avg on columns
type dimension_category_mapping_avg_fields {
  id: Float
}

# order by avg() on columns of table "dimension_category_mapping"
input dimension_category_mapping_avg_order_by {
  id: order_by
}

# Boolean expression to filter rows from the table "dimension_category_mapping". All fields are combined with a logical 'AND'.
input dimension_category_mapping_bool_exp {
  _and: [dimension_category_mapping_bool_exp!]
  _not: dimension_category_mapping_bool_exp
  _or: [dimension_category_mapping_bool_exp!]
  category_id: String_comparison_exp
  created: timestamp_comparison_exp
  dimension: dimension_bool_exp
  dimension_category: dimension_category_bool_exp
  dimension_id: String_comparison_exp
  id: Int_comparison_exp
  last_modified: timestamp_comparison_exp
}

# unique or primary key constraints on table "dimension_category_mapping"
enum dimension_category_mapping_constraint {
  # unique or primary key constraint on columns "dimension_id", "category_id"
  dimension_category_mapping_dimension_id_category_id_key

  # unique or primary key constraint on columns "id"
  dimension_category_mapping_pkey
}

# input type for incrementing numeric columns in table "dimension_category_mapping"
input dimension_category_mapping_inc_input {
  id: Int
}

# input type for inserting data into table "dimension_category_mapping"
input dimension_category_mapping_insert_input {
  category_id: String
  created: timestamp
  dimension: dimension_obj_rel_insert_input
  dimension_category: dimension_category_obj_rel_insert_input
  dimension_id: String
  id: Int
  last_modified: timestamp
}

# aggregate max on columns
type dimension_category_mapping_max_fields {
  category_id: String
  created: timestamp
  dimension_id: String
  id: Int
  last_modified: timestamp
}

# order by max() on columns of table "dimension_category_mapping"
input dimension_category_mapping_max_order_by {
  category_id: order_by
  created: order_by
  dimension_id: order_by
  id: order_by
  last_modified: order_by
}

# aggregate min on columns
type dimension_category_mapping_min_fields {
  category_id: String
  created: timestamp
  dimension_id: String
  id: Int
  last_modified: timestamp
}

# order by min() on columns of table "dimension_category_mapping"
input dimension_category_mapping_min_order_by {
  category_id: order_by
  created: order_by
  dimension_id: order_by
  id: order_by
  last_modified: order_by
}

# response of any mutation on the table "dimension_category_mapping"
type dimension_category_mapping_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [dimension_category_mapping!]!
}

# on_conflict condition type for table "dimension_category_mapping"
input dimension_category_mapping_on_conflict {
  constraint: dimension_category_mapping_constraint!
  update_columns: [dimension_category_mapping_update_column!]! = []
  where: dimension_category_mapping_bool_exp
}

# Ordering options when selecting data from "dimension_category_mapping".
input dimension_category_mapping_order_by {
  category_id: order_by
  created: order_by
  dimension: dimension_order_by
  dimension_category: dimension_category_order_by
  dimension_id: order_by
  id: order_by
  last_modified: order_by
}

# primary key columns input for table: dimension_category_mapping
input dimension_category_mapping_pk_columns_input {
  id: Int!
}

# select columns of table "dimension_category_mapping"
enum dimension_category_mapping_select_column {
  # column name
  category_id

  # column name
  created

  # column name
  dimension_id

  # column name
  id

  # column name
  last_modified
}

# input type for updating data in table "dimension_category_mapping"
input dimension_category_mapping_set_input {
  category_id: String
  created: timestamp
  dimension_id: String
  id: Int
  last_modified: timestamp
}

# aggregate stddev on columns
type dimension_category_mapping_stddev_fields {
  id: Float
}

# order by stddev() on columns of table "dimension_category_mapping"
input dimension_category_mapping_stddev_order_by {
  id: order_by
}

# aggregate stddev_pop on columns
type dimension_category_mapping_stddev_pop_fields {
  id: Float
}

# order by stddev_pop() on columns of table "dimension_category_mapping"
input dimension_category_mapping_stddev_pop_order_by {
  id: order_by
}

# aggregate stddev_samp on columns
type dimension_category_mapping_stddev_samp_fields {
  id: Float
}

# order by stddev_samp() on columns of table "dimension_category_mapping"
input dimension_category_mapping_stddev_samp_order_by {
  id: order_by
}

# aggregate sum on columns
type dimension_category_mapping_sum_fields {
  id: Int
}

# order by sum() on columns of table "dimension_category_mapping"
input dimension_category_mapping_sum_order_by {
  id: order_by
}

# update columns of table "dimension_category_mapping"
enum dimension_category_mapping_update_column {
  # column name
  category_id

  # column name
  created

  # column name
  dimension_id

  # column name
  id

  # column name
  last_modified
}

input dimension_category_mapping_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: dimension_category_mapping_inc_input

  # sets the columns of the filtered rows to the given values
  _set: dimension_category_mapping_set_input
  where: dimension_category_mapping_bool_exp!
}

# aggregate var_pop on columns
type dimension_category_mapping_var_pop_fields {
  id: Float
}

# order by var_pop() on columns of table "dimension_category_mapping"
input dimension_category_mapping_var_pop_order_by {
  id: order_by
}

# aggregate var_samp on columns
type dimension_category_mapping_var_samp_fields {
  id: Float
}

# order by var_samp() on columns of table "dimension_category_mapping"
input dimension_category_mapping_var_samp_order_by {
  id: order_by
}

# aggregate variance on columns
type dimension_category_mapping_variance_fields {
  id: Float
}

# order by variance() on columns of table "dimension_category_mapping"
input dimension_category_mapping_variance_order_by {
  id: order_by
}

# A Relay connection object on "dimension_category_mapping"
type dimension_category_mappingConnection {
  edges: [dimension_category_mappingEdge!]!
  pageInfo: PageInfo!
}

type dimension_category_mappingEdge {
  cursor: String!
  node: dimension_category_mapping!
}

# aggregate max on columns
type dimension_category_max_fields {
  created: timestamp
  id: String
  last_modified: timestamp
  name: String
  parent_id: String
}

# order by max() on columns of table "dimension_category"
input dimension_category_max_order_by {
  created: order_by
  id: order_by
  last_modified: order_by
  name: order_by
  parent_id: order_by
}

# aggregate min on columns
type dimension_category_min_fields {
  created: timestamp
  id: String
  last_modified: timestamp
  name: String
  parent_id: String
}

# order by min() on columns of table "dimension_category"
input dimension_category_min_order_by {
  created: order_by
  id: order_by
  last_modified: order_by
  name: order_by
  parent_id: order_by
}

# response of any mutation on the table "dimension_category"
type dimension_category_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [dimension_category!]!
}

# input type for inserting object relation for remote table "dimension_category"
input dimension_category_obj_rel_insert_input {
  data: dimension_category_insert_input!

  # upsert condition
  on_conflict: dimension_category_on_conflict
}

# on_conflict condition type for table "dimension_category"
input dimension_category_on_conflict {
  constraint: dimension_category_constraint!
  update_columns: [dimension_category_update_column!]! = []
  where: dimension_category_bool_exp
}

# Ordering options when selecting data from "dimension_category".
input dimension_category_order_by {
  children_aggregate: dimension_category_aggregate_order_by
  created: order_by
  dimension_category_mappings_aggregate: dimension_category_mapping_aggregate_order_by
  id: order_by
  last_modified: order_by
  name: order_by
  parent: dimension_category_order_by
  parent_id: order_by
}

# primary key columns input for table: dimension_category
input dimension_category_pk_columns_input {
  id: String!
}

# select columns of table "dimension_category"
enum dimension_category_select_column {
  # column name
  created

  # column name
  id

  # column name
  last_modified

  # column name
  name

  # column name
  parent_id
}

# input type for updating data in table "dimension_category"
input dimension_category_set_input {
  created: timestamp
  id: String
  last_modified: timestamp
  name: String
  parent_id: String
}

# update columns of table "dimension_category"
enum dimension_category_update_column {
  # column name
  created

  # column name
  id

  # column name
  last_modified

  # column name
  name

  # column name
  parent_id
}

input dimension_category_updates {
  # sets the columns of the filtered rows to the given values
  _set: dimension_category_set_input
  where: dimension_category_bool_exp!
}

# A Relay connection object on "dimension_category"
type dimension_categoryConnection {
  edges: [dimension_categoryEdge!]!
  pageInfo: PageInfo!
}

type dimension_categoryEdge {
  cursor: String!
  node: dimension_category!
}

# unique or primary key constraints on table "dimension"
enum dimension_constraint {
  # unique or primary key constraint on columns "id"
  dimension_pkey
}

# input type for inserting data into table "dimension"
input dimension_insert_input {
  authorizable: Boolean
  created: timestamp
  description: String
  dimension_category_mappings: dimension_category_mapping_arr_rel_insert_input
  field_dimension_mappings: field_dimension_mapping_arr_rel_insert_input
  filterable: Boolean
  geoDimensionMetadataByLatId: geo_dimension_metadata_arr_rel_insert_input
  geoDimensionMetadataByLonId: geo_dimension_metadata_arr_rel_insert_input
  geo_dimension_metadata: geo_dimension_metadata_arr_rel_insert_input
  hierarchicalDimensionMetadataByUniqueIdentifierDimensionId: hierarchical_dimension_metadata_arr_rel_insert_input
  hierarchical_dimension_metadata: hierarchical_dimension_metadata_arr_rel_insert_input
  id: String
  last_modified: timestamp
  name: String
  non_hierarchical_dimensions: non_hierarchical_dimension_arr_rel_insert_input
  unpublished_field_dimension_mappings: unpublished_field_dimension_mapping_arr_rel_insert_input
}

# response of any mutation on the table "dimension"
type dimension_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [dimension!]!
}

# input type for inserting object relation for remote table "dimension"
input dimension_obj_rel_insert_input {
  data: dimension_insert_input!

  # upsert condition
  on_conflict: dimension_on_conflict
}

# on_conflict condition type for table "dimension"
input dimension_on_conflict {
  constraint: dimension_constraint!
  update_columns: [dimension_update_column!]! = []
  where: dimension_bool_exp
}

# Ordering options when selecting data from "dimension".
input dimension_order_by {
  authorizable: order_by
  created: order_by
  description: order_by
  dimension_category_mappings_aggregate: dimension_category_mapping_aggregate_order_by
  field_dimension_mappings_aggregate: field_dimension_mapping_aggregate_order_by
  filterable: order_by
  geoDimensionMetadataByLatId_aggregate: geo_dimension_metadata_aggregate_order_by
  geoDimensionMetadataByLonId_aggregate: geo_dimension_metadata_aggregate_order_by
  geo_dimension_metadata_aggregate: geo_dimension_metadata_aggregate_order_by
  hierarchicalDimensionMetadataByUniqueIdentifierDimensionId_aggregate: hierarchical_dimension_metadata_aggregate_order_by
  hierarchical_dimension_metadata_aggregate: hierarchical_dimension_metadata_aggregate_order_by
  id: order_by
  last_modified: order_by
  name: order_by
  non_hierarchical_dimensions_aggregate: non_hierarchical_dimension_aggregate_order_by
  unpublished_field_dimension_mappings_aggregate: unpublished_field_dimension_mapping_aggregate_order_by
}

# primary key columns input for table: dimension
input dimension_pk_columns_input {
  id: String!
}

# select columns of table "dimension"
enum dimension_select_column {
  # column name
  authorizable

  # column name
  created

  # column name
  description

  # column name
  filterable

  # column name
  id

  # column name
  last_modified

  # column name
  name
}

# input type for updating data in table "dimension"
input dimension_set_input {
  authorizable: Boolean
  created: timestamp
  description: String
  filterable: Boolean
  id: String
  last_modified: timestamp
  name: String
}

# update columns of table "dimension"
enum dimension_update_column {
  # column name
  authorizable

  # column name
  created

  # column name
  description

  # column name
  filterable

  # column name
  id

  # column name
  last_modified

  # column name
  name
}

input dimension_updates {
  # sets the columns of the filtered rows to the given values
  _set: dimension_set_input
  where: dimension_bool_exp!
}

# A Relay connection object on "dimension"
type dimensionConnection {
  edges: [dimensionEdge!]!
  pageInfo: PageInfo!
}

type dimensionEdge {
  cursor: String!
  node: dimension!
}

scalar druid_field_metadata_type_enum

# Boolean expression to compare columns of type "druid_field_metadata_type_enum". All fields are combined with logical 'AND'.
input druid_field_metadata_type_enum_comparison_exp {
  _eq: druid_field_metadata_type_enum
  _gt: druid_field_metadata_type_enum
  _gte: druid_field_metadata_type_enum
  _in: [druid_field_metadata_type_enum!]
  _is_null: Boolean
  _lt: druid_field_metadata_type_enum
  _lte: druid_field_metadata_type_enum
  _neq: druid_field_metadata_type_enum
  _nin: [druid_field_metadata_type_enum!]
}

scalar event_type_enum

# Boolean expression to compare columns of type "event_type_enum". All fields are combined with logical 'AND'.
input event_type_enum_comparison_exp {
  _eq: event_type_enum
  _gt: event_type_enum
  _gte: event_type_enum
  _in: [event_type_enum!]
  _is_null: Boolean
  _lt: event_type_enum
  _lte: event_type_enum
  _neq: event_type_enum
  _nin: [event_type_enum!]
}

# columns and relationships of "external_alert_activity_to_ignore"
type external_alert_activity_to_ignore implements Node {
  activity: String!

  # An object relationship
  external_alert_type: external_alert_type!
  external_alert_type_id: Int!
  id: ID!
}

# aggregated selection of "external_alert_activity_to_ignore"
type external_alert_activity_to_ignore_aggregate {
  aggregate: external_alert_activity_to_ignore_aggregate_fields
  nodes: [external_alert_activity_to_ignore!]!
}

# aggregate fields of "external_alert_activity_to_ignore"
type external_alert_activity_to_ignore_aggregate_fields {
  avg: external_alert_activity_to_ignore_avg_fields
  count(columns: [external_alert_activity_to_ignore_select_column!], distinct: Boolean): Int!
  max: external_alert_activity_to_ignore_max_fields
  min: external_alert_activity_to_ignore_min_fields
  stddev: external_alert_activity_to_ignore_stddev_fields
  stddev_pop: external_alert_activity_to_ignore_stddev_pop_fields
  stddev_samp: external_alert_activity_to_ignore_stddev_samp_fields
  sum: external_alert_activity_to_ignore_sum_fields
  var_pop: external_alert_activity_to_ignore_var_pop_fields
  var_samp: external_alert_activity_to_ignore_var_samp_fields
  variance: external_alert_activity_to_ignore_variance_fields
}

# order by aggregate values of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_aggregate_order_by {
  avg: external_alert_activity_to_ignore_avg_order_by
  count: order_by
  max: external_alert_activity_to_ignore_max_order_by
  min: external_alert_activity_to_ignore_min_order_by
  stddev: external_alert_activity_to_ignore_stddev_order_by
  stddev_pop: external_alert_activity_to_ignore_stddev_pop_order_by
  stddev_samp: external_alert_activity_to_ignore_stddev_samp_order_by
  sum: external_alert_activity_to_ignore_sum_order_by
  var_pop: external_alert_activity_to_ignore_var_pop_order_by
  var_samp: external_alert_activity_to_ignore_var_samp_order_by
  variance: external_alert_activity_to_ignore_variance_order_by
}

# input type for inserting array relation for remote table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_arr_rel_insert_input {
  data: [external_alert_activity_to_ignore_insert_input!]!

  # upsert condition
  on_conflict: external_alert_activity_to_ignore_on_conflict
}

# aggregate avg on columns
type external_alert_activity_to_ignore_avg_fields {
  external_alert_type_id: Float
}

# order by avg() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_avg_order_by {
  external_alert_type_id: order_by
}

# Boolean expression to filter rows from the table
# "external_alert_activity_to_ignore". All fields are combined with a logical 'AND'.
input external_alert_activity_to_ignore_bool_exp {
  _and: [external_alert_activity_to_ignore_bool_exp!]
  _not: external_alert_activity_to_ignore_bool_exp
  _or: [external_alert_activity_to_ignore_bool_exp!]
  activity: String_comparison_exp
  external_alert_type: external_alert_type_bool_exp
  external_alert_type_id: Int_comparison_exp
}

# unique or primary key constraints on table "external_alert_activity_to_ignore"
enum external_alert_activity_to_ignore_constraint {
  # unique or primary key constraint on columns "external_alert_type_id", "activity"
  external_alert_activity_to_ignore_pkey
}

# input type for incrementing numeric columns in table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_inc_input {
  external_alert_type_id: Int
}

# input type for inserting data into table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_insert_input {
  activity: String
  external_alert_type: external_alert_type_obj_rel_insert_input
  external_alert_type_id: Int
}

# aggregate max on columns
type external_alert_activity_to_ignore_max_fields {
  activity: String
  external_alert_type_id: Int
}

# order by max() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_max_order_by {
  activity: order_by
  external_alert_type_id: order_by
}

# aggregate min on columns
type external_alert_activity_to_ignore_min_fields {
  activity: String
  external_alert_type_id: Int
}

# order by min() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_min_order_by {
  activity: order_by
  external_alert_type_id: order_by
}

# response of any mutation on the table "external_alert_activity_to_ignore"
type external_alert_activity_to_ignore_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [external_alert_activity_to_ignore!]!
}

# on_conflict condition type for table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_on_conflict {
  constraint: external_alert_activity_to_ignore_constraint!
  update_columns: [external_alert_activity_to_ignore_update_column!]! = []
  where: external_alert_activity_to_ignore_bool_exp
}

# Ordering options when selecting data from "external_alert_activity_to_ignore".
input external_alert_activity_to_ignore_order_by {
  activity: order_by
  external_alert_type: external_alert_type_order_by
  external_alert_type_id: order_by
}

# primary key columns input for table: external_alert_activity_to_ignore
input external_alert_activity_to_ignore_pk_columns_input {
  activity: String!
  external_alert_type_id: Int!
}

# select columns of table "external_alert_activity_to_ignore"
enum external_alert_activity_to_ignore_select_column {
  # column name
  activity

  # column name
  external_alert_type_id
}

# input type for updating data in table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_set_input {
  activity: String
  external_alert_type_id: Int
}

# aggregate stddev on columns
type external_alert_activity_to_ignore_stddev_fields {
  external_alert_type_id: Float
}

# order by stddev() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_stddev_order_by {
  external_alert_type_id: order_by
}

# aggregate stddev_pop on columns
type external_alert_activity_to_ignore_stddev_pop_fields {
  external_alert_type_id: Float
}

# order by stddev_pop() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_stddev_pop_order_by {
  external_alert_type_id: order_by
}

# aggregate stddev_samp on columns
type external_alert_activity_to_ignore_stddev_samp_fields {
  external_alert_type_id: Float
}

# order by stddev_samp() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_stddev_samp_order_by {
  external_alert_type_id: order_by
}

# aggregate sum on columns
type external_alert_activity_to_ignore_sum_fields {
  external_alert_type_id: Int
}

# order by sum() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_sum_order_by {
  external_alert_type_id: order_by
}

# update columns of table "external_alert_activity_to_ignore"
enum external_alert_activity_to_ignore_update_column {
  # column name
  activity

  # column name
  external_alert_type_id
}

input external_alert_activity_to_ignore_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: external_alert_activity_to_ignore_inc_input

  # sets the columns of the filtered rows to the given values
  _set: external_alert_activity_to_ignore_set_input
  where: external_alert_activity_to_ignore_bool_exp!
}

# aggregate var_pop on columns
type external_alert_activity_to_ignore_var_pop_fields {
  external_alert_type_id: Float
}

# order by var_pop() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_var_pop_order_by {
  external_alert_type_id: order_by
}

# aggregate var_samp on columns
type external_alert_activity_to_ignore_var_samp_fields {
  external_alert_type_id: Float
}

# order by var_samp() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_var_samp_order_by {
  external_alert_type_id: order_by
}

# aggregate variance on columns
type external_alert_activity_to_ignore_variance_fields {
  external_alert_type_id: Float
}

# order by variance() on columns of table "external_alert_activity_to_ignore"
input external_alert_activity_to_ignore_variance_order_by {
  external_alert_type_id: order_by
}

# A Relay connection object on "external_alert_activity_to_ignore"
type external_alert_activity_to_ignoreConnection {
  edges: [external_alert_activity_to_ignoreEdge!]!
  pageInfo: PageInfo!
}

type external_alert_activity_to_ignoreEdge {
  cursor: String!
  node: external_alert_activity_to_ignore!
}

# columns and relationships of "external_alert_type"
type external_alert_type implements Node {
  # An object relationship
  case_type: case_type!
  case_type_id: Int!
  data_dimension: String!
  druid_dimension: String!

  # An array relationship
  external_alert_activity_to_ignores(
    # distinct select on columns
    distinct_on: [external_alert_activity_to_ignore_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [external_alert_activity_to_ignore_order_by!]

    # filter the rows returned
    where: external_alert_activity_to_ignore_bool_exp
  ): [external_alert_activity_to_ignore!]!

  # An aggregate relationship
  external_alert_activity_to_ignores_aggregate(
    # distinct select on columns
    distinct_on: [external_alert_activity_to_ignore_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [external_alert_activity_to_ignore_order_by!]

    # filter the rows returned
    where: external_alert_activity_to_ignore_bool_exp
  ): external_alert_activity_to_ignore_aggregate!

  # An array relationship connection
  external_alert_activity_to_ignores_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [external_alert_activity_to_ignore_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [external_alert_activity_to_ignore_order_by!]

    # filter the rows returned
    where: external_alert_activity_to_ignore_bool_exp
  ): external_alert_activity_to_ignoreConnection!
  field_id: String!
  id: ID!
  name: String!
}

# aggregated selection of "external_alert_type"
type external_alert_type_aggregate {
  aggregate: external_alert_type_aggregate_fields
  nodes: [external_alert_type!]!
}

# aggregate fields of "external_alert_type"
type external_alert_type_aggregate_fields {
  avg: external_alert_type_avg_fields
  count(columns: [external_alert_type_select_column!], distinct: Boolean): Int!
  max: external_alert_type_max_fields
  min: external_alert_type_min_fields
  stddev: external_alert_type_stddev_fields
  stddev_pop: external_alert_type_stddev_pop_fields
  stddev_samp: external_alert_type_stddev_samp_fields
  sum: external_alert_type_sum_fields
  var_pop: external_alert_type_var_pop_fields
  var_samp: external_alert_type_var_samp_fields
  variance: external_alert_type_variance_fields
}

# order by aggregate values of table "external_alert_type"
input external_alert_type_aggregate_order_by {
  avg: external_alert_type_avg_order_by
  count: order_by
  max: external_alert_type_max_order_by
  min: external_alert_type_min_order_by
  stddev: external_alert_type_stddev_order_by
  stddev_pop: external_alert_type_stddev_pop_order_by
  stddev_samp: external_alert_type_stddev_samp_order_by
  sum: external_alert_type_sum_order_by
  var_pop: external_alert_type_var_pop_order_by
  var_samp: external_alert_type_var_samp_order_by
  variance: external_alert_type_variance_order_by
}

# input type for inserting array relation for remote table "external_alert_type"
input external_alert_type_arr_rel_insert_input {
  data: [external_alert_type_insert_input!]!

  # upsert condition
  on_conflict: external_alert_type_on_conflict
}

# aggregate avg on columns
type external_alert_type_avg_fields {
  case_type_id: Float
  id: Float
}

# order by avg() on columns of table "external_alert_type"
input external_alert_type_avg_order_by {
  case_type_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table "external_alert_type". All fields are combined with a logical 'AND'.
input external_alert_type_bool_exp {
  _and: [external_alert_type_bool_exp!]
  _not: external_alert_type_bool_exp
  _or: [external_alert_type_bool_exp!]
  case_type: case_type_bool_exp
  case_type_id: Int_comparison_exp
  data_dimension: String_comparison_exp
  druid_dimension: String_comparison_exp
  external_alert_activity_to_ignores: external_alert_activity_to_ignore_bool_exp
  field_id: String_comparison_exp
  id: Int_comparison_exp
  name: String_comparison_exp
}

# unique or primary key constraints on table "external_alert_type"
enum external_alert_type_constraint {
  # unique or primary key constraint on columns "name"
  external_alert_type_name_key

  # unique or primary key constraint on columns "id"
  external_alert_type_pkey
}

# input type for incrementing numeric columns in table "external_alert_type"
input external_alert_type_inc_input {
  case_type_id: Int
  id: Int
}

# input type for inserting data into table "external_alert_type"
input external_alert_type_insert_input {
  case_type: case_type_obj_rel_insert_input
  case_type_id: Int
  data_dimension: String
  druid_dimension: String
  external_alert_activity_to_ignores: external_alert_activity_to_ignore_arr_rel_insert_input
  field_id: String
  id: Int
  name: String
}

# aggregate max on columns
type external_alert_type_max_fields {
  case_type_id: Int
  data_dimension: String
  druid_dimension: String
  field_id: String
  id: Int
  name: String
}

# order by max() on columns of table "external_alert_type"
input external_alert_type_max_order_by {
  case_type_id: order_by
  data_dimension: order_by
  druid_dimension: order_by
  field_id: order_by
  id: order_by
  name: order_by
}

# aggregate min on columns
type external_alert_type_min_fields {
  case_type_id: Int
  data_dimension: String
  druid_dimension: String
  field_id: String
  id: Int
  name: String
}

# order by min() on columns of table "external_alert_type"
input external_alert_type_min_order_by {
  case_type_id: order_by
  data_dimension: order_by
  druid_dimension: order_by
  field_id: order_by
  id: order_by
  name: order_by
}

# response of any mutation on the table "external_alert_type"
type external_alert_type_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [external_alert_type!]!
}

# input type for inserting object relation for remote table "external_alert_type"
input external_alert_type_obj_rel_insert_input {
  data: external_alert_type_insert_input!

  # upsert condition
  on_conflict: external_alert_type_on_conflict
}

# on_conflict condition type for table "external_alert_type"
input external_alert_type_on_conflict {
  constraint: external_alert_type_constraint!
  update_columns: [external_alert_type_update_column!]! = []
  where: external_alert_type_bool_exp
}

# Ordering options when selecting data from "external_alert_type".
input external_alert_type_order_by {
  case_type: case_type_order_by
  case_type_id: order_by
  data_dimension: order_by
  druid_dimension: order_by
  external_alert_activity_to_ignores_aggregate: external_alert_activity_to_ignore_aggregate_order_by
  field_id: order_by
  id: order_by
  name: order_by
}

# primary key columns input for table: external_alert_type
input external_alert_type_pk_columns_input {
  id: Int!
}

# select columns of table "external_alert_type"
enum external_alert_type_select_column {
  # column name
  case_type_id

  # column name
  data_dimension

  # column name
  druid_dimension

  # column name
  field_id

  # column name
  id

  # column name
  name
}

# input type for updating data in table "external_alert_type"
input external_alert_type_set_input {
  case_type_id: Int
  data_dimension: String
  druid_dimension: String
  field_id: String
  id: Int
  name: String
}

# aggregate stddev on columns
type external_alert_type_stddev_fields {
  case_type_id: Float
  id: Float
}

# order by stddev() on columns of table "external_alert_type"
input external_alert_type_stddev_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type external_alert_type_stddev_pop_fields {
  case_type_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "external_alert_type"
input external_alert_type_stddev_pop_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type external_alert_type_stddev_samp_fields {
  case_type_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "external_alert_type"
input external_alert_type_stddev_samp_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate sum on columns
type external_alert_type_sum_fields {
  case_type_id: Int
  id: Int
}

# order by sum() on columns of table "external_alert_type"
input external_alert_type_sum_order_by {
  case_type_id: order_by
  id: order_by
}

# update columns of table "external_alert_type"
enum external_alert_type_update_column {
  # column name
  case_type_id

  # column name
  data_dimension

  # column name
  druid_dimension

  # column name
  field_id

  # column name
  id

  # column name
  name
}

input external_alert_type_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: external_alert_type_inc_input

  # sets the columns of the filtered rows to the given values
  _set: external_alert_type_set_input
  where: external_alert_type_bool_exp!
}

# aggregate var_pop on columns
type external_alert_type_var_pop_fields {
  case_type_id: Float
  id: Float
}

# order by var_pop() on columns of table "external_alert_type"
input external_alert_type_var_pop_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type external_alert_type_var_samp_fields {
  case_type_id: Float
  id: Float
}

# order by var_samp() on columns of table "external_alert_type"
input external_alert_type_var_samp_order_by {
  case_type_id: order_by
  id: order_by
}

# aggregate variance on columns
type external_alert_type_variance_fields {
  case_type_id: Float
  id: Float
}

# order by variance() on columns of table "external_alert_type"
input external_alert_type_variance_order_by {
  case_type_id: order_by
  id: order_by
}

# A Relay connection object on "external_alert_type"
type external_alert_typeConnection {
  edges: [external_alert_typeEdge!]!
  pageInfo: PageInfo!
}

type external_alert_typeEdge {
  cursor: String!
  node: external_alert_type!
}

# columns and relationships of "feed_update"
type feed_update implements Node {
  datasource: String

  # An object relationship
  feed_update_type: feed_update_type
  feed_update_type_id: Int
  generation_datetime: timestamp
  id: ID!
  notification_payload(
    # JSON select path
    path: String
  ): jsonb
  user_id: Int
}

# aggregated selection of "feed_update"
type feed_update_aggregate {
  aggregate: feed_update_aggregate_fields
  nodes: [feed_update!]!
}

# aggregate fields of "feed_update"
type feed_update_aggregate_fields {
  avg: feed_update_avg_fields
  count(columns: [feed_update_select_column!], distinct: Boolean): Int!
  max: feed_update_max_fields
  min: feed_update_min_fields
  stddev: feed_update_stddev_fields
  stddev_pop: feed_update_stddev_pop_fields
  stddev_samp: feed_update_stddev_samp_fields
  sum: feed_update_sum_fields
  var_pop: feed_update_var_pop_fields
  var_samp: feed_update_var_samp_fields
  variance: feed_update_variance_fields
}

# order by aggregate values of table "feed_update"
input feed_update_aggregate_order_by {
  avg: feed_update_avg_order_by
  count: order_by
  max: feed_update_max_order_by
  min: feed_update_min_order_by
  stddev: feed_update_stddev_order_by
  stddev_pop: feed_update_stddev_pop_order_by
  stddev_samp: feed_update_stddev_samp_order_by
  sum: feed_update_sum_order_by
  var_pop: feed_update_var_pop_order_by
  var_samp: feed_update_var_samp_order_by
  variance: feed_update_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input feed_update_append_input {
  notification_payload: jsonb
}

# input type for inserting array relation for remote table "feed_update"
input feed_update_arr_rel_insert_input {
  data: [feed_update_insert_input!]!

  # upsert condition
  on_conflict: feed_update_on_conflict
}

# aggregate avg on columns
type feed_update_avg_fields {
  feed_update_type_id: Float
  id: Float
  user_id: Float
}

# order by avg() on columns of table "feed_update"
input feed_update_avg_order_by {
  feed_update_type_id: order_by
  id: order_by
  user_id: order_by
}

# Boolean expression to filter rows from the table "feed_update". All fields are combined with a logical 'AND'.
input feed_update_bool_exp {
  _and: [feed_update_bool_exp!]
  _not: feed_update_bool_exp
  _or: [feed_update_bool_exp!]
  datasource: String_comparison_exp
  feed_update_type: feed_update_type_bool_exp
  feed_update_type_id: Int_comparison_exp
  generation_datetime: timestamp_comparison_exp
  id: Int_comparison_exp
  notification_payload: jsonb_comparison_exp
  user_id: Int_comparison_exp
}

# unique or primary key constraints on table "feed_update"
enum feed_update_constraint {
  # unique or primary key constraint on columns "id"
  feed_update_pkey
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input feed_update_delete_at_path_input {
  notification_payload: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input feed_update_delete_elem_input {
  notification_payload: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input feed_update_delete_key_input {
  notification_payload: String
}

# input type for incrementing numeric columns in table "feed_update"
input feed_update_inc_input {
  feed_update_type_id: Int
  id: Int
  user_id: Int
}

# input type for inserting data into table "feed_update"
input feed_update_insert_input {
  datasource: String
  feed_update_type: feed_update_type_obj_rel_insert_input
  feed_update_type_id: Int
  generation_datetime: timestamp
  id: Int
  notification_payload: jsonb
  user_id: Int
}

# aggregate max on columns
type feed_update_max_fields {
  datasource: String
  feed_update_type_id: Int
  generation_datetime: timestamp
  id: Int
  user_id: Int
}

# order by max() on columns of table "feed_update"
input feed_update_max_order_by {
  datasource: order_by
  feed_update_type_id: order_by
  generation_datetime: order_by
  id: order_by
  user_id: order_by
}

# aggregate min on columns
type feed_update_min_fields {
  datasource: String
  feed_update_type_id: Int
  generation_datetime: timestamp
  id: Int
  user_id: Int
}

# order by min() on columns of table "feed_update"
input feed_update_min_order_by {
  datasource: order_by
  feed_update_type_id: order_by
  generation_datetime: order_by
  id: order_by
  user_id: order_by
}

# response of any mutation on the table "feed_update"
type feed_update_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [feed_update!]!
}

# on_conflict condition type for table "feed_update"
input feed_update_on_conflict {
  constraint: feed_update_constraint!
  update_columns: [feed_update_update_column!]! = []
  where: feed_update_bool_exp
}

# Ordering options when selecting data from "feed_update".
input feed_update_order_by {
  datasource: order_by
  feed_update_type: feed_update_type_order_by
  feed_update_type_id: order_by
  generation_datetime: order_by
  id: order_by
  notification_payload: order_by
  user_id: order_by
}

# primary key columns input for table: feed_update
input feed_update_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input feed_update_prepend_input {
  notification_payload: jsonb
}

# select columns of table "feed_update"
enum feed_update_select_column {
  # column name
  datasource

  # column name
  feed_update_type_id

  # column name
  generation_datetime

  # column name
  id

  # column name
  notification_payload

  # column name
  user_id
}

# input type for updating data in table "feed_update"
input feed_update_set_input {
  datasource: String
  feed_update_type_id: Int
  generation_datetime: timestamp
  id: Int
  notification_payload: jsonb
  user_id: Int
}

# aggregate stddev on columns
type feed_update_stddev_fields {
  feed_update_type_id: Float
  id: Float
  user_id: Float
}

# order by stddev() on columns of table "feed_update"
input feed_update_stddev_order_by {
  feed_update_type_id: order_by
  id: order_by
  user_id: order_by
}

# aggregate stddev_pop on columns
type feed_update_stddev_pop_fields {
  feed_update_type_id: Float
  id: Float
  user_id: Float
}

# order by stddev_pop() on columns of table "feed_update"
input feed_update_stddev_pop_order_by {
  feed_update_type_id: order_by
  id: order_by
  user_id: order_by
}

# aggregate stddev_samp on columns
type feed_update_stddev_samp_fields {
  feed_update_type_id: Float
  id: Float
  user_id: Float
}

# order by stddev_samp() on columns of table "feed_update"
input feed_update_stddev_samp_order_by {
  feed_update_type_id: order_by
  id: order_by
  user_id: order_by
}

# aggregate sum on columns
type feed_update_sum_fields {
  feed_update_type_id: Int
  id: Int
  user_id: Int
}

# order by sum() on columns of table "feed_update"
input feed_update_sum_order_by {
  feed_update_type_id: order_by
  id: order_by
  user_id: order_by
}

# columns and relationships of "feed_update_type"
type feed_update_type implements Node {
  # An array relationship
  feed_updates(
    # distinct select on columns
    distinct_on: [feed_update_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [feed_update_order_by!]

    # filter the rows returned
    where: feed_update_bool_exp
  ): [feed_update!]!

  # An aggregate relationship
  feed_updates_aggregate(
    # distinct select on columns
    distinct_on: [feed_update_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [feed_update_order_by!]

    # filter the rows returned
    where: feed_update_bool_exp
  ): feed_update_aggregate!

  # An array relationship connection
  feed_updates_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [feed_update_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [feed_update_order_by!]

    # filter the rows returned
    where: feed_update_bool_exp
  ): feed_updateConnection!
  id: ID!
  name: String
}

# Boolean expression to filter rows from the table "feed_update_type". All fields are combined with a logical 'AND'.
input feed_update_type_bool_exp {
  _and: [feed_update_type_bool_exp!]
  _not: feed_update_type_bool_exp
  _or: [feed_update_type_bool_exp!]
  feed_updates: feed_update_bool_exp
  id: Int_comparison_exp
  name: String_comparison_exp
}

# unique or primary key constraints on table "feed_update_type"
enum feed_update_type_constraint {
  # unique or primary key constraint on columns "id"
  feed_update_type_pkey
}

# input type for incrementing numeric columns in table "feed_update_type"
input feed_update_type_inc_input {
  id: Int
}

# input type for inserting data into table "feed_update_type"
input feed_update_type_insert_input {
  feed_updates: feed_update_arr_rel_insert_input
  id: Int
  name: String
}

# response of any mutation on the table "feed_update_type"
type feed_update_type_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [feed_update_type!]!
}

# input type for inserting object relation for remote table "feed_update_type"
input feed_update_type_obj_rel_insert_input {
  data: feed_update_type_insert_input!

  # upsert condition
  on_conflict: feed_update_type_on_conflict
}

# on_conflict condition type for table "feed_update_type"
input feed_update_type_on_conflict {
  constraint: feed_update_type_constraint!
  update_columns: [feed_update_type_update_column!]! = []
  where: feed_update_type_bool_exp
}

# Ordering options when selecting data from "feed_update_type".
input feed_update_type_order_by {
  feed_updates_aggregate: feed_update_aggregate_order_by
  id: order_by
  name: order_by
}

# primary key columns input for table: feed_update_type
input feed_update_type_pk_columns_input {
  id: Int!
}

# select columns of table "feed_update_type"
enum feed_update_type_select_column {
  # column name
  id

  # column name
  name
}

# input type for updating data in table "feed_update_type"
input feed_update_type_set_input {
  id: Int
  name: String
}

# update columns of table "feed_update_type"
enum feed_update_type_update_column {
  # column name
  id

  # column name
  name
}

input feed_update_type_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: feed_update_type_inc_input

  # sets the columns of the filtered rows to the given values
  _set: feed_update_type_set_input
  where: feed_update_type_bool_exp!
}

# A Relay connection object on "feed_update_type"
type feed_update_typeConnection {
  edges: [feed_update_typeEdge!]!
  pageInfo: PageInfo!
}

type feed_update_typeEdge {
  cursor: String!
  node: feed_update_type!
}

# update columns of table "feed_update"
enum feed_update_update_column {
  # column name
  datasource

  # column name
  feed_update_type_id

  # column name
  generation_datetime

  # column name
  id

  # column name
  notification_payload

  # column name
  user_id
}

input feed_update_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: feed_update_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: feed_update_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: feed_update_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: feed_update_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: feed_update_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: feed_update_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: feed_update_set_input
  where: feed_update_bool_exp!
}

# aggregate var_pop on columns
type feed_update_var_pop_fields {
  feed_update_type_id: Float
  id: Float
  user_id: Float
}

# order by var_pop() on columns of table "feed_update"
input feed_update_var_pop_order_by {
  feed_update_type_id: order_by
  id: order_by
  user_id: order_by
}

# aggregate var_samp on columns
type feed_update_var_samp_fields {
  feed_update_type_id: Float
  id: Float
  user_id: Float
}

# order by var_samp() on columns of table "feed_update"
input feed_update_var_samp_order_by {
  feed_update_type_id: order_by
  id: order_by
  user_id: order_by
}

# aggregate variance on columns
type feed_update_variance_fields {
  feed_update_type_id: Float
  id: Float
  user_id: Float
}

# order by variance() on columns of table "feed_update"
input feed_update_variance_order_by {
  feed_update_type_id: order_by
  id: order_by
  user_id: order_by
}

# A Relay connection object on "feed_update"
type feed_updateConnection {
  edges: [feed_updateEdge!]!
  pageInfo: PageInfo!
}

type feed_updateEdge {
  cursor: String!
  node: feed_update!
}

# columns and relationships of "field"
type field implements Node {
  calculation(
    # JSON select path
    path: String
  ): jsonb!

  # An object relationship
  copied_from_field: field
  copied_from_field_id: String
  created: timestamp!
  description: String

  # An array relationship
  field_category_mappings(
    # distinct select on columns
    distinct_on: [field_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_category_mapping_order_by!]

    # filter the rows returned
    where: field_category_mapping_bool_exp
  ): [field_category_mapping!]!

  # An aggregate relationship
  field_category_mappings_aggregate(
    # distinct select on columns
    distinct_on: [field_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_category_mapping_order_by!]

    # filter the rows returned
    where: field_category_mapping_bool_exp
  ): field_category_mapping_aggregate!

  # An array relationship connection
  field_category_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_category_mapping_order_by!]

    # filter the rows returned
    where: field_category_mapping_bool_exp
  ): field_category_mappingConnection!

  # An array relationship
  field_copies(
    # distinct select on columns
    distinct_on: [field_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_order_by!]

    # filter the rows returned
    where: field_bool_exp
  ): [field!]!

  # An aggregate relationship
  field_copies_aggregate(
    # distinct select on columns
    distinct_on: [field_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_order_by!]

    # filter the rows returned
    where: field_bool_exp
  ): field_aggregate!

  # An array relationship connection
  field_copies_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_order_by!]

    # filter the rows returned
    where: field_bool_exp
  ): fieldConnection!

  # An array relationship
  field_dimension_mappings(
    # distinct select on columns
    distinct_on: [field_dimension_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_dimension_mapping_order_by!]

    # filter the rows returned
    where: field_dimension_mapping_bool_exp
  ): [field_dimension_mapping!]!

  # An aggregate relationship
  field_dimension_mappings_aggregate(
    # distinct select on columns
    distinct_on: [field_dimension_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_dimension_mapping_order_by!]

    # filter the rows returned
    where: field_dimension_mapping_bool_exp
  ): field_dimension_mapping_aggregate!

  # An array relationship connection
  field_dimension_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_dimension_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_dimension_mapping_order_by!]

    # filter the rows returned
    where: field_dimension_mapping_bool_exp
  ): field_dimension_mappingConnection!

  # An array relationship
  field_pipeline_datasource_mappings(
    # distinct select on columns
    distinct_on: [field_pipeline_datasource_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: field_pipeline_datasource_mapping_bool_exp
  ): [field_pipeline_datasource_mapping!]!

  # An aggregate relationship
  field_pipeline_datasource_mappings_aggregate(
    # distinct select on columns
    distinct_on: [field_pipeline_datasource_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: field_pipeline_datasource_mapping_bool_exp
  ): field_pipeline_datasource_mapping_aggregate!

  # An array relationship connection
  field_pipeline_datasource_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_pipeline_datasource_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: field_pipeline_datasource_mapping_bool_exp
  ): field_pipeline_datasource_mappingConnection!
  id: ID!
  last_modified: timestamp!
  name: String!
  short_name: String!
}

# aggregated selection of "field"
type field_aggregate {
  aggregate: field_aggregate_fields
  nodes: [field!]!
}

# aggregate fields of "field"
type field_aggregate_fields {
  count(columns: [field_select_column!], distinct: Boolean): Int!
  max: field_max_fields
  min: field_min_fields
}

# order by aggregate values of table "field"
input field_aggregate_order_by {
  count: order_by
  max: field_max_order_by
  min: field_min_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input field_append_input {
  calculation: jsonb
}

# input type for inserting array relation for remote table "field"
input field_arr_rel_insert_input {
  data: [field_insert_input!]!

  # upsert condition
  on_conflict: field_on_conflict
}

# Boolean expression to filter rows from the table "field". All fields are combined with a logical 'AND'.
input field_bool_exp {
  _and: [field_bool_exp!]
  _not: field_bool_exp
  _or: [field_bool_exp!]
  calculation: jsonb_comparison_exp
  copied_from_field: field_bool_exp
  copied_from_field_id: String_comparison_exp
  created: timestamp_comparison_exp
  description: String_comparison_exp
  field_category_mappings: field_category_mapping_bool_exp
  field_copies: field_bool_exp
  field_dimension_mappings: field_dimension_mapping_bool_exp
  field_pipeline_datasource_mappings: field_pipeline_datasource_mapping_bool_exp
  id: String_comparison_exp
  last_modified: timestamp_comparison_exp
  name: String_comparison_exp
  short_name: String_comparison_exp
}

# columns and relationships of "field_category_mapping"
type field_category_mapping implements Node {
  # An object relationship
  category: category!
  category_id: String!
  created: timestamp!

  # An object relationship
  field: field!
  field_id: String!
  id: ID!
  last_modified: timestamp!
  visibility_status: visibility_status_enum!
}

# aggregated selection of "field_category_mapping"
type field_category_mapping_aggregate {
  aggregate: field_category_mapping_aggregate_fields
  nodes: [field_category_mapping!]!
}

# aggregate fields of "field_category_mapping"
type field_category_mapping_aggregate_fields {
  avg: field_category_mapping_avg_fields
  count(columns: [field_category_mapping_select_column!], distinct: Boolean): Int!
  max: field_category_mapping_max_fields
  min: field_category_mapping_min_fields
  stddev: field_category_mapping_stddev_fields
  stddev_pop: field_category_mapping_stddev_pop_fields
  stddev_samp: field_category_mapping_stddev_samp_fields
  sum: field_category_mapping_sum_fields
  var_pop: field_category_mapping_var_pop_fields
  var_samp: field_category_mapping_var_samp_fields
  variance: field_category_mapping_variance_fields
}

# order by aggregate values of table "field_category_mapping"
input field_category_mapping_aggregate_order_by {
  avg: field_category_mapping_avg_order_by
  count: order_by
  max: field_category_mapping_max_order_by
  min: field_category_mapping_min_order_by
  stddev: field_category_mapping_stddev_order_by
  stddev_pop: field_category_mapping_stddev_pop_order_by
  stddev_samp: field_category_mapping_stddev_samp_order_by
  sum: field_category_mapping_sum_order_by
  var_pop: field_category_mapping_var_pop_order_by
  var_samp: field_category_mapping_var_samp_order_by
  variance: field_category_mapping_variance_order_by
}

# input type for inserting array relation for remote table "field_category_mapping"
input field_category_mapping_arr_rel_insert_input {
  data: [field_category_mapping_insert_input!]!

  # upsert condition
  on_conflict: field_category_mapping_on_conflict
}

# aggregate avg on columns
type field_category_mapping_avg_fields {
  id: Float
}

# order by avg() on columns of table "field_category_mapping"
input field_category_mapping_avg_order_by {
  id: order_by
}

# Boolean expression to filter rows from the table "field_category_mapping". All fields are combined with a logical 'AND'.
input field_category_mapping_bool_exp {
  _and: [field_category_mapping_bool_exp!]
  _not: field_category_mapping_bool_exp
  _or: [field_category_mapping_bool_exp!]
  category: category_bool_exp
  category_id: String_comparison_exp
  created: timestamp_comparison_exp
  field: field_bool_exp
  field_id: String_comparison_exp
  id: Int_comparison_exp
  last_modified: timestamp_comparison_exp
  visibility_status: visibility_status_enum_comparison_exp
}

# unique or primary key constraints on table "field_category_mapping"
enum field_category_mapping_constraint {
  # unique or primary key constraint on columns "category_id", "field_id"
  field_category_mapping_field_id_category_id_key

  # unique or primary key constraint on columns "id"
  field_category_mapping_pkey
}

# input type for incrementing numeric columns in table "field_category_mapping"
input field_category_mapping_inc_input {
  id: Int
}

# input type for inserting data into table "field_category_mapping"
input field_category_mapping_insert_input {
  category: category_obj_rel_insert_input
  category_id: String
  created: timestamp
  field: field_obj_rel_insert_input
  field_id: String
  id: Int
  last_modified: timestamp
  visibility_status: visibility_status_enum
}

# aggregate max on columns
type field_category_mapping_max_fields {
  category_id: String
  created: timestamp
  field_id: String
  id: Int
  last_modified: timestamp
  visibility_status: visibility_status_enum
}

# order by max() on columns of table "field_category_mapping"
input field_category_mapping_max_order_by {
  category_id: order_by
  created: order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
  visibility_status: order_by
}

# aggregate min on columns
type field_category_mapping_min_fields {
  category_id: String
  created: timestamp
  field_id: String
  id: Int
  last_modified: timestamp
  visibility_status: visibility_status_enum
}

# order by min() on columns of table "field_category_mapping"
input field_category_mapping_min_order_by {
  category_id: order_by
  created: order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
  visibility_status: order_by
}

# response of any mutation on the table "field_category_mapping"
type field_category_mapping_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [field_category_mapping!]!
}

# on_conflict condition type for table "field_category_mapping"
input field_category_mapping_on_conflict {
  constraint: field_category_mapping_constraint!
  update_columns: [field_category_mapping_update_column!]! = []
  where: field_category_mapping_bool_exp
}

# Ordering options when selecting data from "field_category_mapping".
input field_category_mapping_order_by {
  category: category_order_by
  category_id: order_by
  created: order_by
  field: field_order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
  visibility_status: order_by
}

# primary key columns input for table: field_category_mapping
input field_category_mapping_pk_columns_input {
  id: Int!
}

# select columns of table "field_category_mapping"
enum field_category_mapping_select_column {
  # column name
  category_id

  # column name
  created

  # column name
  field_id

  # column name
  id

  # column name
  last_modified

  # column name
  visibility_status
}

# input type for updating data in table "field_category_mapping"
input field_category_mapping_set_input {
  category_id: String
  created: timestamp
  field_id: String
  id: Int
  last_modified: timestamp
  visibility_status: visibility_status_enum
}

# aggregate stddev on columns
type field_category_mapping_stddev_fields {
  id: Float
}

# order by stddev() on columns of table "field_category_mapping"
input field_category_mapping_stddev_order_by {
  id: order_by
}

# aggregate stddev_pop on columns
type field_category_mapping_stddev_pop_fields {
  id: Float
}

# order by stddev_pop() on columns of table "field_category_mapping"
input field_category_mapping_stddev_pop_order_by {
  id: order_by
}

# aggregate stddev_samp on columns
type field_category_mapping_stddev_samp_fields {
  id: Float
}

# order by stddev_samp() on columns of table "field_category_mapping"
input field_category_mapping_stddev_samp_order_by {
  id: order_by
}

# aggregate sum on columns
type field_category_mapping_sum_fields {
  id: Int
}

# order by sum() on columns of table "field_category_mapping"
input field_category_mapping_sum_order_by {
  id: order_by
}

# update columns of table "field_category_mapping"
enum field_category_mapping_update_column {
  # column name
  category_id

  # column name
  created

  # column name
  field_id

  # column name
  id

  # column name
  last_modified

  # column name
  visibility_status
}

input field_category_mapping_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: field_category_mapping_inc_input

  # sets the columns of the filtered rows to the given values
  _set: field_category_mapping_set_input
  where: field_category_mapping_bool_exp!
}

# aggregate var_pop on columns
type field_category_mapping_var_pop_fields {
  id: Float
}

# order by var_pop() on columns of table "field_category_mapping"
input field_category_mapping_var_pop_order_by {
  id: order_by
}

# aggregate var_samp on columns
type field_category_mapping_var_samp_fields {
  id: Float
}

# order by var_samp() on columns of table "field_category_mapping"
input field_category_mapping_var_samp_order_by {
  id: order_by
}

# aggregate variance on columns
type field_category_mapping_variance_fields {
  id: Float
}

# order by variance() on columns of table "field_category_mapping"
input field_category_mapping_variance_order_by {
  id: order_by
}

# A Relay connection object on "field_category_mapping"
type field_category_mappingConnection {
  edges: [field_category_mappingEdge!]!
  pageInfo: PageInfo!
}

type field_category_mappingEdge {
  cursor: String!
  node: field_category_mapping!
}

# unique or primary key constraints on table "field"
enum field_constraint {
  # unique or primary key constraint on columns "id"
  field_pkey
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input field_delete_at_path_input {
  calculation: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input field_delete_elem_input {
  calculation: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input field_delete_key_input {
  calculation: String
}

# columns and relationships of "field_dimension_mapping"
type field_dimension_mapping implements Node {
  created: timestamp!

  # An object relationship
  dimension: dimension!
  dimension_id: String!

  # An object relationship
  field: field!
  field_id: String!
  id: ID!
  last_modified: timestamp!
}

# aggregated selection of "field_dimension_mapping"
type field_dimension_mapping_aggregate {
  aggregate: field_dimension_mapping_aggregate_fields
  nodes: [field_dimension_mapping!]!
}

# aggregate fields of "field_dimension_mapping"
type field_dimension_mapping_aggregate_fields {
  avg: field_dimension_mapping_avg_fields
  count(columns: [field_dimension_mapping_select_column!], distinct: Boolean): Int!
  max: field_dimension_mapping_max_fields
  min: field_dimension_mapping_min_fields
  stddev: field_dimension_mapping_stddev_fields
  stddev_pop: field_dimension_mapping_stddev_pop_fields
  stddev_samp: field_dimension_mapping_stddev_samp_fields
  sum: field_dimension_mapping_sum_fields
  var_pop: field_dimension_mapping_var_pop_fields
  var_samp: field_dimension_mapping_var_samp_fields
  variance: field_dimension_mapping_variance_fields
}

# order by aggregate values of table "field_dimension_mapping"
input field_dimension_mapping_aggregate_order_by {
  avg: field_dimension_mapping_avg_order_by
  count: order_by
  max: field_dimension_mapping_max_order_by
  min: field_dimension_mapping_min_order_by
  stddev: field_dimension_mapping_stddev_order_by
  stddev_pop: field_dimension_mapping_stddev_pop_order_by
  stddev_samp: field_dimension_mapping_stddev_samp_order_by
  sum: field_dimension_mapping_sum_order_by
  var_pop: field_dimension_mapping_var_pop_order_by
  var_samp: field_dimension_mapping_var_samp_order_by
  variance: field_dimension_mapping_variance_order_by
}

# input type for inserting array relation for remote table "field_dimension_mapping"
input field_dimension_mapping_arr_rel_insert_input {
  data: [field_dimension_mapping_insert_input!]!

  # upsert condition
  on_conflict: field_dimension_mapping_on_conflict
}

# aggregate avg on columns
type field_dimension_mapping_avg_fields {
  id: Float
}

# order by avg() on columns of table "field_dimension_mapping"
input field_dimension_mapping_avg_order_by {
  id: order_by
}

# Boolean expression to filter rows from the table "field_dimension_mapping". All fields are combined with a logical 'AND'.
input field_dimension_mapping_bool_exp {
  _and: [field_dimension_mapping_bool_exp!]
  _not: field_dimension_mapping_bool_exp
  _or: [field_dimension_mapping_bool_exp!]
  created: timestamp_comparison_exp
  dimension: dimension_bool_exp
  dimension_id: String_comparison_exp
  field: field_bool_exp
  field_id: String_comparison_exp
  id: Int_comparison_exp
  last_modified: timestamp_comparison_exp
}

# unique or primary key constraints on table "field_dimension_mapping"
enum field_dimension_mapping_constraint {
  # unique or primary key constraint on columns "dimension_id", "field_id"
  field_dimension_mapping_field_id_dimension_id_key

  # unique or primary key constraint on columns "id"
  field_dimension_mapping_pkey
}

# input type for incrementing numeric columns in table "field_dimension_mapping"
input field_dimension_mapping_inc_input {
  id: Int
}

# input type for inserting data into table "field_dimension_mapping"
input field_dimension_mapping_insert_input {
  created: timestamp
  dimension: dimension_obj_rel_insert_input
  dimension_id: String
  field: field_obj_rel_insert_input
  field_id: String
  id: Int
  last_modified: timestamp
}

# aggregate max on columns
type field_dimension_mapping_max_fields {
  created: timestamp
  dimension_id: String
  field_id: String
  id: Int
  last_modified: timestamp
}

# order by max() on columns of table "field_dimension_mapping"
input field_dimension_mapping_max_order_by {
  created: order_by
  dimension_id: order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
}

# aggregate min on columns
type field_dimension_mapping_min_fields {
  created: timestamp
  dimension_id: String
  field_id: String
  id: Int
  last_modified: timestamp
}

# order by min() on columns of table "field_dimension_mapping"
input field_dimension_mapping_min_order_by {
  created: order_by
  dimension_id: order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
}

# response of any mutation on the table "field_dimension_mapping"
type field_dimension_mapping_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [field_dimension_mapping!]!
}

# on_conflict condition type for table "field_dimension_mapping"
input field_dimension_mapping_on_conflict {
  constraint: field_dimension_mapping_constraint!
  update_columns: [field_dimension_mapping_update_column!]! = []
  where: field_dimension_mapping_bool_exp
}

# Ordering options when selecting data from "field_dimension_mapping".
input field_dimension_mapping_order_by {
  created: order_by
  dimension: dimension_order_by
  dimension_id: order_by
  field: field_order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
}

# primary key columns input for table: field_dimension_mapping
input field_dimension_mapping_pk_columns_input {
  id: Int!
}

# select columns of table "field_dimension_mapping"
enum field_dimension_mapping_select_column {
  # column name
  created

  # column name
  dimension_id

  # column name
  field_id

  # column name
  id

  # column name
  last_modified
}

# input type for updating data in table "field_dimension_mapping"
input field_dimension_mapping_set_input {
  created: timestamp
  dimension_id: String
  field_id: String
  id: Int
  last_modified: timestamp
}

# aggregate stddev on columns
type field_dimension_mapping_stddev_fields {
  id: Float
}

# order by stddev() on columns of table "field_dimension_mapping"
input field_dimension_mapping_stddev_order_by {
  id: order_by
}

# aggregate stddev_pop on columns
type field_dimension_mapping_stddev_pop_fields {
  id: Float
}

# order by stddev_pop() on columns of table "field_dimension_mapping"
input field_dimension_mapping_stddev_pop_order_by {
  id: order_by
}

# aggregate stddev_samp on columns
type field_dimension_mapping_stddev_samp_fields {
  id: Float
}

# order by stddev_samp() on columns of table "field_dimension_mapping"
input field_dimension_mapping_stddev_samp_order_by {
  id: order_by
}

# aggregate sum on columns
type field_dimension_mapping_sum_fields {
  id: Int
}

# order by sum() on columns of table "field_dimension_mapping"
input field_dimension_mapping_sum_order_by {
  id: order_by
}

# update columns of table "field_dimension_mapping"
enum field_dimension_mapping_update_column {
  # column name
  created

  # column name
  dimension_id

  # column name
  field_id

  # column name
  id

  # column name
  last_modified
}

input field_dimension_mapping_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: field_dimension_mapping_inc_input

  # sets the columns of the filtered rows to the given values
  _set: field_dimension_mapping_set_input
  where: field_dimension_mapping_bool_exp!
}

# aggregate var_pop on columns
type field_dimension_mapping_var_pop_fields {
  id: Float
}

# order by var_pop() on columns of table "field_dimension_mapping"
input field_dimension_mapping_var_pop_order_by {
  id: order_by
}

# aggregate var_samp on columns
type field_dimension_mapping_var_samp_fields {
  id: Float
}

# order by var_samp() on columns of table "field_dimension_mapping"
input field_dimension_mapping_var_samp_order_by {
  id: order_by
}

# aggregate variance on columns
type field_dimension_mapping_variance_fields {
  id: Float
}

# order by variance() on columns of table "field_dimension_mapping"
input field_dimension_mapping_variance_order_by {
  id: order_by
}

# A Relay connection object on "field_dimension_mapping"
type field_dimension_mappingConnection {
  edges: [field_dimension_mappingEdge!]!
  pageInfo: PageInfo!
}

type field_dimension_mappingEdge {
  cursor: String!
  node: field_dimension_mapping!
}

# input type for inserting data into table "field"
input field_insert_input {
  calculation: jsonb
  copied_from_field: field_obj_rel_insert_input
  copied_from_field_id: String
  created: timestamp
  description: String
  field_category_mappings: field_category_mapping_arr_rel_insert_input
  field_copies: field_arr_rel_insert_input
  field_dimension_mappings: field_dimension_mapping_arr_rel_insert_input
  field_pipeline_datasource_mappings: field_pipeline_datasource_mapping_arr_rel_insert_input
  id: String
  last_modified: timestamp
  name: String
  short_name: String
}

# aggregate max on columns
type field_max_fields {
  copied_from_field_id: String
  created: timestamp
  description: String
  id: String
  last_modified: timestamp
  name: String
  short_name: String
}

# order by max() on columns of table "field"
input field_max_order_by {
  copied_from_field_id: order_by
  created: order_by
  description: order_by
  id: order_by
  last_modified: order_by
  name: order_by
  short_name: order_by
}

# aggregate min on columns
type field_min_fields {
  copied_from_field_id: String
  created: timestamp
  description: String
  id: String
  last_modified: timestamp
  name: String
  short_name: String
}

# order by min() on columns of table "field"
input field_min_order_by {
  copied_from_field_id: order_by
  created: order_by
  description: order_by
  id: order_by
  last_modified: order_by
  name: order_by
  short_name: order_by
}

# response of any mutation on the table "field"
type field_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [field!]!
}

# input type for inserting object relation for remote table "field"
input field_obj_rel_insert_input {
  data: field_insert_input!

  # upsert condition
  on_conflict: field_on_conflict
}

# on_conflict condition type for table "field"
input field_on_conflict {
  constraint: field_constraint!
  update_columns: [field_update_column!]! = []
  where: field_bool_exp
}

# Ordering options when selecting data from "field".
input field_order_by {
  calculation: order_by
  copied_from_field: field_order_by
  copied_from_field_id: order_by
  created: order_by
  description: order_by
  field_category_mappings_aggregate: field_category_mapping_aggregate_order_by
  field_copies_aggregate: field_aggregate_order_by
  field_dimension_mappings_aggregate: field_dimension_mapping_aggregate_order_by
  field_pipeline_datasource_mappings_aggregate: field_pipeline_datasource_mapping_aggregate_order_by
  id: order_by
  last_modified: order_by
  name: order_by
  short_name: order_by
}

# columns and relationships of "field_pipeline_datasource_mapping"
type field_pipeline_datasource_mapping implements Node {
  created: timestamp!

  # An object relationship
  field: field!
  field_id: String!
  id: ID!
  last_modified: timestamp!

  # An object relationship
  pipeline_datasource: pipeline_datasource!
  pipeline_datasource_id: String!
}

# aggregated selection of "field_pipeline_datasource_mapping"
type field_pipeline_datasource_mapping_aggregate {
  aggregate: field_pipeline_datasource_mapping_aggregate_fields
  nodes: [field_pipeline_datasource_mapping!]!
}

# aggregate fields of "field_pipeline_datasource_mapping"
type field_pipeline_datasource_mapping_aggregate_fields {
  avg: field_pipeline_datasource_mapping_avg_fields
  count(columns: [field_pipeline_datasource_mapping_select_column!], distinct: Boolean): Int!
  max: field_pipeline_datasource_mapping_max_fields
  min: field_pipeline_datasource_mapping_min_fields
  stddev: field_pipeline_datasource_mapping_stddev_fields
  stddev_pop: field_pipeline_datasource_mapping_stddev_pop_fields
  stddev_samp: field_pipeline_datasource_mapping_stddev_samp_fields
  sum: field_pipeline_datasource_mapping_sum_fields
  var_pop: field_pipeline_datasource_mapping_var_pop_fields
  var_samp: field_pipeline_datasource_mapping_var_samp_fields
  variance: field_pipeline_datasource_mapping_variance_fields
}

# order by aggregate values of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_aggregate_order_by {
  avg: field_pipeline_datasource_mapping_avg_order_by
  count: order_by
  max: field_pipeline_datasource_mapping_max_order_by
  min: field_pipeline_datasource_mapping_min_order_by
  stddev: field_pipeline_datasource_mapping_stddev_order_by
  stddev_pop: field_pipeline_datasource_mapping_stddev_pop_order_by
  stddev_samp: field_pipeline_datasource_mapping_stddev_samp_order_by
  sum: field_pipeline_datasource_mapping_sum_order_by
  var_pop: field_pipeline_datasource_mapping_var_pop_order_by
  var_samp: field_pipeline_datasource_mapping_var_samp_order_by
  variance: field_pipeline_datasource_mapping_variance_order_by
}

# input type for inserting array relation for remote table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_arr_rel_insert_input {
  data: [field_pipeline_datasource_mapping_insert_input!]!

  # upsert condition
  on_conflict: field_pipeline_datasource_mapping_on_conflict
}

# aggregate avg on columns
type field_pipeline_datasource_mapping_avg_fields {
  id: Float
}

# order by avg() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_avg_order_by {
  id: order_by
}

# Boolean expression to filter rows from the table
# "field_pipeline_datasource_mapping". All fields are combined with a logical 'AND'.
input field_pipeline_datasource_mapping_bool_exp {
  _and: [field_pipeline_datasource_mapping_bool_exp!]
  _not: field_pipeline_datasource_mapping_bool_exp
  _or: [field_pipeline_datasource_mapping_bool_exp!]
  created: timestamp_comparison_exp
  field: field_bool_exp
  field_id: String_comparison_exp
  id: Int_comparison_exp
  last_modified: timestamp_comparison_exp
  pipeline_datasource: pipeline_datasource_bool_exp
  pipeline_datasource_id: String_comparison_exp
}

# unique or primary key constraints on table "field_pipeline_datasource_mapping"
enum field_pipeline_datasource_mapping_constraint {
  # unique or primary key constraint on columns "pipeline_datasource_id", "field_id"
  field_pipeline_datasource_map_field_id_pipeline_datasource__key

  # unique or primary key constraint on columns "id"
  field_pipeline_datasource_mapping_pkey
}

# input type for incrementing numeric columns in table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_inc_input {
  id: Int
}

# input type for inserting data into table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_insert_input {
  created: timestamp
  field: field_obj_rel_insert_input
  field_id: String
  id: Int
  last_modified: timestamp
  pipeline_datasource: pipeline_datasource_obj_rel_insert_input
  pipeline_datasource_id: String
}

# aggregate max on columns
type field_pipeline_datasource_mapping_max_fields {
  created: timestamp
  field_id: String
  id: Int
  last_modified: timestamp
  pipeline_datasource_id: String
}

# order by max() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_max_order_by {
  created: order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
  pipeline_datasource_id: order_by
}

# aggregate min on columns
type field_pipeline_datasource_mapping_min_fields {
  created: timestamp
  field_id: String
  id: Int
  last_modified: timestamp
  pipeline_datasource_id: String
}

# order by min() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_min_order_by {
  created: order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
  pipeline_datasource_id: order_by
}

# response of any mutation on the table "field_pipeline_datasource_mapping"
type field_pipeline_datasource_mapping_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [field_pipeline_datasource_mapping!]!
}

# on_conflict condition type for table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_on_conflict {
  constraint: field_pipeline_datasource_mapping_constraint!
  update_columns: [field_pipeline_datasource_mapping_update_column!]! = []
  where: field_pipeline_datasource_mapping_bool_exp
}

# Ordering options when selecting data from "field_pipeline_datasource_mapping".
input field_pipeline_datasource_mapping_order_by {
  created: order_by
  field: field_order_by
  field_id: order_by
  id: order_by
  last_modified: order_by
  pipeline_datasource: pipeline_datasource_order_by
  pipeline_datasource_id: order_by
}

# primary key columns input for table: field_pipeline_datasource_mapping
input field_pipeline_datasource_mapping_pk_columns_input {
  id: Int!
}

# select columns of table "field_pipeline_datasource_mapping"
enum field_pipeline_datasource_mapping_select_column {
  # column name
  created

  # column name
  field_id

  # column name
  id

  # column name
  last_modified

  # column name
  pipeline_datasource_id
}

# input type for updating data in table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_set_input {
  created: timestamp
  field_id: String
  id: Int
  last_modified: timestamp
  pipeline_datasource_id: String
}

# aggregate stddev on columns
type field_pipeline_datasource_mapping_stddev_fields {
  id: Float
}

# order by stddev() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_stddev_order_by {
  id: order_by
}

# aggregate stddev_pop on columns
type field_pipeline_datasource_mapping_stddev_pop_fields {
  id: Float
}

# order by stddev_pop() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_stddev_pop_order_by {
  id: order_by
}

# aggregate stddev_samp on columns
type field_pipeline_datasource_mapping_stddev_samp_fields {
  id: Float
}

# order by stddev_samp() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_stddev_samp_order_by {
  id: order_by
}

# aggregate sum on columns
type field_pipeline_datasource_mapping_sum_fields {
  id: Int
}

# order by sum() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_sum_order_by {
  id: order_by
}

# update columns of table "field_pipeline_datasource_mapping"
enum field_pipeline_datasource_mapping_update_column {
  # column name
  created

  # column name
  field_id

  # column name
  id

  # column name
  last_modified

  # column name
  pipeline_datasource_id
}

input field_pipeline_datasource_mapping_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: field_pipeline_datasource_mapping_inc_input

  # sets the columns of the filtered rows to the given values
  _set: field_pipeline_datasource_mapping_set_input
  where: field_pipeline_datasource_mapping_bool_exp!
}

# aggregate var_pop on columns
type field_pipeline_datasource_mapping_var_pop_fields {
  id: Float
}

# order by var_pop() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_var_pop_order_by {
  id: order_by
}

# aggregate var_samp on columns
type field_pipeline_datasource_mapping_var_samp_fields {
  id: Float
}

# order by var_samp() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_var_samp_order_by {
  id: order_by
}

# aggregate variance on columns
type field_pipeline_datasource_mapping_variance_fields {
  id: Float
}

# order by variance() on columns of table "field_pipeline_datasource_mapping"
input field_pipeline_datasource_mapping_variance_order_by {
  id: order_by
}

# A Relay connection object on "field_pipeline_datasource_mapping"
type field_pipeline_datasource_mappingConnection {
  edges: [field_pipeline_datasource_mappingEdge!]!
  pageInfo: PageInfo!
}

type field_pipeline_datasource_mappingEdge {
  cursor: String!
  node: field_pipeline_datasource_mapping!
}

# primary key columns input for table: field
input field_pk_columns_input {
  id: String!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input field_prepend_input {
  calculation: jsonb
}

# select columns of table "field"
enum field_select_column {
  # column name
  calculation

  # column name
  copied_from_field_id

  # column name
  created

  # column name
  description

  # column name
  id

  # column name
  last_modified

  # column name
  name

  # column name
  short_name
}

# input type for updating data in table "field"
input field_set_input {
  calculation: jsonb
  copied_from_field_id: String
  created: timestamp
  description: String
  id: String
  last_modified: timestamp
  name: String
  short_name: String
}

# update columns of table "field"
enum field_update_column {
  # column name
  calculation

  # column name
  copied_from_field_id

  # column name
  created

  # column name
  description

  # column name
  id

  # column name
  last_modified

  # column name
  name

  # column name
  short_name
}

input field_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: field_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: field_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: field_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: field_delete_key_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: field_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: field_set_input
  where: field_bool_exp!
}

# A Relay connection object on "field"
type fieldConnection {
  edges: [fieldEdge!]!
  pageInfo: PageInfo!
}

type fieldEdge {
  cursor: String!
  node: field!
}

# columns and relationships of "geo_dimension_metadata"
type geo_dimension_metadata implements Node {
  # An object relationship
  dimension: dimension!

  # An object relationship
  dimensionByLatId: dimension!

  # An object relationship
  dimensionByLonId: dimension!
  id: ID!
  lat_id: String!
  lon_id: String!
}

# aggregated selection of "geo_dimension_metadata"
type geo_dimension_metadata_aggregate {
  aggregate: geo_dimension_metadata_aggregate_fields
  nodes: [geo_dimension_metadata!]!
}

# aggregate fields of "geo_dimension_metadata"
type geo_dimension_metadata_aggregate_fields {
  count(columns: [geo_dimension_metadata_select_column!], distinct: Boolean): Int!
  max: geo_dimension_metadata_max_fields
  min: geo_dimension_metadata_min_fields
}

# order by aggregate values of table "geo_dimension_metadata"
input geo_dimension_metadata_aggregate_order_by {
  count: order_by
  max: geo_dimension_metadata_max_order_by
  min: geo_dimension_metadata_min_order_by
}

# input type for inserting array relation for remote table "geo_dimension_metadata"
input geo_dimension_metadata_arr_rel_insert_input {
  data: [geo_dimension_metadata_insert_input!]!

  # upsert condition
  on_conflict: geo_dimension_metadata_on_conflict
}

# Boolean expression to filter rows from the table "geo_dimension_metadata". All fields are combined with a logical 'AND'.
input geo_dimension_metadata_bool_exp {
  _and: [geo_dimension_metadata_bool_exp!]
  _not: geo_dimension_metadata_bool_exp
  _or: [geo_dimension_metadata_bool_exp!]
  dimension: dimension_bool_exp
  dimensionByLatId: dimension_bool_exp
  dimensionByLonId: dimension_bool_exp
  id: String_comparison_exp
  lat_id: String_comparison_exp
  lon_id: String_comparison_exp
}

# unique or primary key constraints on table "geo_dimension_metadata"
enum geo_dimension_metadata_constraint {
  # unique or primary key constraint on columns "id"
  geo_dimension_metadata_pkey
}

# input type for inserting data into table "geo_dimension_metadata"
input geo_dimension_metadata_insert_input {
  dimension: dimension_obj_rel_insert_input
  dimensionByLatId: dimension_obj_rel_insert_input
  dimensionByLonId: dimension_obj_rel_insert_input
  id: String
  lat_id: String
  lon_id: String
}

# aggregate max on columns
type geo_dimension_metadata_max_fields {
  id: String
  lat_id: String
  lon_id: String
}

# order by max() on columns of table "geo_dimension_metadata"
input geo_dimension_metadata_max_order_by {
  id: order_by
  lat_id: order_by
  lon_id: order_by
}

# aggregate min on columns
type geo_dimension_metadata_min_fields {
  id: String
  lat_id: String
  lon_id: String
}

# order by min() on columns of table "geo_dimension_metadata"
input geo_dimension_metadata_min_order_by {
  id: order_by
  lat_id: order_by
  lon_id: order_by
}

# response of any mutation on the table "geo_dimension_metadata"
type geo_dimension_metadata_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [geo_dimension_metadata!]!
}

# on_conflict condition type for table "geo_dimension_metadata"
input geo_dimension_metadata_on_conflict {
  constraint: geo_dimension_metadata_constraint!
  update_columns: [geo_dimension_metadata_update_column!]! = []
  where: geo_dimension_metadata_bool_exp
}

# Ordering options when selecting data from "geo_dimension_metadata".
input geo_dimension_metadata_order_by {
  dimension: dimension_order_by
  dimensionByLatId: dimension_order_by
  dimensionByLonId: dimension_order_by
  id: order_by
  lat_id: order_by
  lon_id: order_by
}

# primary key columns input for table: geo_dimension_metadata
input geo_dimension_metadata_pk_columns_input {
  id: String!
}

# select columns of table "geo_dimension_metadata"
enum geo_dimension_metadata_select_column {
  # column name
  id

  # column name
  lat_id

  # column name
  lon_id
}

# input type for updating data in table "geo_dimension_metadata"
input geo_dimension_metadata_set_input {
  id: String
  lat_id: String
  lon_id: String
}

# update columns of table "geo_dimension_metadata"
enum geo_dimension_metadata_update_column {
  # column name
  id

  # column name
  lat_id

  # column name
  lon_id
}

input geo_dimension_metadata_updates {
  # sets the columns of the filtered rows to the given values
  _set: geo_dimension_metadata_set_input
  where: geo_dimension_metadata_bool_exp!
}

# A Relay connection object on "geo_dimension_metadata"
type geo_dimension_metadataConnection {
  edges: [geo_dimension_metadataEdge!]!
  pageInfo: PageInfo!
}

type geo_dimension_metadataEdge {
  cursor: String!
  node: geo_dimension_metadata!
}

# columns and relationships of "hierarchical_dimension_metadata"
type hierarchical_dimension_metadata implements Node {
  # An object relationship
  dimension: dimension!

  # An object relationship
  dimensionByUniqueIdentifierDimensionId: dimension!
  dimension_id: String!

  # An array relationship
  hierarchical_dimension_metadata(
    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): [hierarchical_dimension_metadata!]!

  # An aggregate relationship
  hierarchical_dimension_metadata_aggregate(
    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): hierarchical_dimension_metadata_aggregate!

  # An array relationship connection
  hierarchical_dimension_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): hierarchical_dimension_metadataConnection!

  # An object relationship
  hierarchical_dimension_metadatum: hierarchical_dimension_metadata
  id: ID!
  parent_id: Int
  unique_identifier_dimension_id: String!
}

# aggregated selection of "hierarchical_dimension_metadata"
type hierarchical_dimension_metadata_aggregate {
  aggregate: hierarchical_dimension_metadata_aggregate_fields
  nodes: [hierarchical_dimension_metadata!]!
}

# aggregate fields of "hierarchical_dimension_metadata"
type hierarchical_dimension_metadata_aggregate_fields {
  avg: hierarchical_dimension_metadata_avg_fields
  count(columns: [hierarchical_dimension_metadata_select_column!], distinct: Boolean): Int!
  max: hierarchical_dimension_metadata_max_fields
  min: hierarchical_dimension_metadata_min_fields
  stddev: hierarchical_dimension_metadata_stddev_fields
  stddev_pop: hierarchical_dimension_metadata_stddev_pop_fields
  stddev_samp: hierarchical_dimension_metadata_stddev_samp_fields
  sum: hierarchical_dimension_metadata_sum_fields
  var_pop: hierarchical_dimension_metadata_var_pop_fields
  var_samp: hierarchical_dimension_metadata_var_samp_fields
  variance: hierarchical_dimension_metadata_variance_fields
}

# order by aggregate values of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_aggregate_order_by {
  avg: hierarchical_dimension_metadata_avg_order_by
  count: order_by
  max: hierarchical_dimension_metadata_max_order_by
  min: hierarchical_dimension_metadata_min_order_by
  stddev: hierarchical_dimension_metadata_stddev_order_by
  stddev_pop: hierarchical_dimension_metadata_stddev_pop_order_by
  stddev_samp: hierarchical_dimension_metadata_stddev_samp_order_by
  sum: hierarchical_dimension_metadata_sum_order_by
  var_pop: hierarchical_dimension_metadata_var_pop_order_by
  var_samp: hierarchical_dimension_metadata_var_samp_order_by
  variance: hierarchical_dimension_metadata_variance_order_by
}

# input type for inserting array relation for remote table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_arr_rel_insert_input {
  data: [hierarchical_dimension_metadata_insert_input!]!

  # upsert condition
  on_conflict: hierarchical_dimension_metadata_on_conflict
}

# aggregate avg on columns
type hierarchical_dimension_metadata_avg_fields {
  id: Float
  parent_id: Float
}

# order by avg() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_avg_order_by {
  id: order_by
  parent_id: order_by
}

# Boolean expression to filter rows from the table
# "hierarchical_dimension_metadata". All fields are combined with a logical 'AND'.
input hierarchical_dimension_metadata_bool_exp {
  _and: [hierarchical_dimension_metadata_bool_exp!]
  _not: hierarchical_dimension_metadata_bool_exp
  _or: [hierarchical_dimension_metadata_bool_exp!]
  dimension: dimension_bool_exp
  dimensionByUniqueIdentifierDimensionId: dimension_bool_exp
  dimension_id: String_comparison_exp
  hierarchical_dimension_metadata: hierarchical_dimension_metadata_bool_exp
  hierarchical_dimension_metadatum: hierarchical_dimension_metadata_bool_exp
  id: Int_comparison_exp
  parent_id: Int_comparison_exp
  unique_identifier_dimension_id: String_comparison_exp
}

# unique or primary key constraints on table "hierarchical_dimension_metadata"
enum hierarchical_dimension_metadata_constraint {
  # unique or primary key constraint on columns "id"
  hierarchical_dimension_metadata_pkey
}

# input type for incrementing numeric columns in table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_inc_input {
  id: Int
  parent_id: Int
}

# input type for inserting data into table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_insert_input {
  dimension: dimension_obj_rel_insert_input
  dimensionByUniqueIdentifierDimensionId: dimension_obj_rel_insert_input
  dimension_id: String
  hierarchical_dimension_metadata: hierarchical_dimension_metadata_arr_rel_insert_input
  hierarchical_dimension_metadatum: hierarchical_dimension_metadata_obj_rel_insert_input
  id: Int
  parent_id: Int
  unique_identifier_dimension_id: String
}

# aggregate max on columns
type hierarchical_dimension_metadata_max_fields {
  dimension_id: String
  id: Int
  parent_id: Int
  unique_identifier_dimension_id: String
}

# order by max() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_max_order_by {
  dimension_id: order_by
  id: order_by
  parent_id: order_by
  unique_identifier_dimension_id: order_by
}

# aggregate min on columns
type hierarchical_dimension_metadata_min_fields {
  dimension_id: String
  id: Int
  parent_id: Int
  unique_identifier_dimension_id: String
}

# order by min() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_min_order_by {
  dimension_id: order_by
  id: order_by
  parent_id: order_by
  unique_identifier_dimension_id: order_by
}

# response of any mutation on the table "hierarchical_dimension_metadata"
type hierarchical_dimension_metadata_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [hierarchical_dimension_metadata!]!
}

# input type for inserting object relation for remote table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_obj_rel_insert_input {
  data: hierarchical_dimension_metadata_insert_input!

  # upsert condition
  on_conflict: hierarchical_dimension_metadata_on_conflict
}

# on_conflict condition type for table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_on_conflict {
  constraint: hierarchical_dimension_metadata_constraint!
  update_columns: [hierarchical_dimension_metadata_update_column!]! = []
  where: hierarchical_dimension_metadata_bool_exp
}

# Ordering options when selecting data from "hierarchical_dimension_metadata".
input hierarchical_dimension_metadata_order_by {
  dimension: dimension_order_by
  dimensionByUniqueIdentifierDimensionId: dimension_order_by
  dimension_id: order_by
  hierarchical_dimension_metadata_aggregate: hierarchical_dimension_metadata_aggregate_order_by
  hierarchical_dimension_metadatum: hierarchical_dimension_metadata_order_by
  id: order_by
  parent_id: order_by
  unique_identifier_dimension_id: order_by
}

# primary key columns input for table: hierarchical_dimension_metadata
input hierarchical_dimension_metadata_pk_columns_input {
  id: Int!
}

# select columns of table "hierarchical_dimension_metadata"
enum hierarchical_dimension_metadata_select_column {
  # column name
  dimension_id

  # column name
  id

  # column name
  parent_id

  # column name
  unique_identifier_dimension_id
}

# input type for updating data in table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_set_input {
  dimension_id: String
  id: Int
  parent_id: Int
  unique_identifier_dimension_id: String
}

# aggregate stddev on columns
type hierarchical_dimension_metadata_stddev_fields {
  id: Float
  parent_id: Float
}

# order by stddev() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_stddev_order_by {
  id: order_by
  parent_id: order_by
}

# aggregate stddev_pop on columns
type hierarchical_dimension_metadata_stddev_pop_fields {
  id: Float
  parent_id: Float
}

# order by stddev_pop() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_stddev_pop_order_by {
  id: order_by
  parent_id: order_by
}

# aggregate stddev_samp on columns
type hierarchical_dimension_metadata_stddev_samp_fields {
  id: Float
  parent_id: Float
}

# order by stddev_samp() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_stddev_samp_order_by {
  id: order_by
  parent_id: order_by
}

# aggregate sum on columns
type hierarchical_dimension_metadata_sum_fields {
  id: Int
  parent_id: Int
}

# order by sum() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_sum_order_by {
  id: order_by
  parent_id: order_by
}

# update columns of table "hierarchical_dimension_metadata"
enum hierarchical_dimension_metadata_update_column {
  # column name
  dimension_id

  # column name
  id

  # column name
  parent_id

  # column name
  unique_identifier_dimension_id
}

input hierarchical_dimension_metadata_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: hierarchical_dimension_metadata_inc_input

  # sets the columns of the filtered rows to the given values
  _set: hierarchical_dimension_metadata_set_input
  where: hierarchical_dimension_metadata_bool_exp!
}

# aggregate var_pop on columns
type hierarchical_dimension_metadata_var_pop_fields {
  id: Float
  parent_id: Float
}

# order by var_pop() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_var_pop_order_by {
  id: order_by
  parent_id: order_by
}

# aggregate var_samp on columns
type hierarchical_dimension_metadata_var_samp_fields {
  id: Float
  parent_id: Float
}

# order by var_samp() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_var_samp_order_by {
  id: order_by
  parent_id: order_by
}

# aggregate variance on columns
type hierarchical_dimension_metadata_variance_fields {
  id: Float
  parent_id: Float
}

# order by variance() on columns of table "hierarchical_dimension_metadata"
input hierarchical_dimension_metadata_variance_order_by {
  id: order_by
  parent_id: order_by
}

# A Relay connection object on "hierarchical_dimension_metadata"
type hierarchical_dimension_metadataConnection {
  edges: [hierarchical_dimension_metadataEdge!]!
  pageInfo: PageInfo!
}

type hierarchical_dimension_metadataEdge {
  cursor: String!
  node: hierarchical_dimension_metadata!
}

# Boolean expression to compare columns of type "Int". All fields are combined with logical 'AND'.
input Int_comparison_exp {
  _eq: Int
  _gt: Int
  _gte: Int
  _in: [Int!]
  _is_null: Boolean
  _lt: Int
  _lte: Int
  _neq: Int
  _nin: [Int!]
}

scalar jsonb

input jsonb_cast_exp {
  String: String_comparison_exp
}

# Boolean expression to compare columns of type "jsonb". All fields are combined with logical 'AND'.
input jsonb_comparison_exp {
  _cast: jsonb_cast_exp

  # is the column contained in the given json value
  _contained_in: jsonb

  # does the column contain the given json value at the top level
  _contains: jsonb
  _eq: jsonb
  _gt: jsonb
  _gte: jsonb

  # does the string exist as a top-level key in the column
  _has_key: String

  # do all of these strings exist as top-level keys in the column
  _has_keys_all: [String!]

  # do any of these strings exist as top-level keys in the column
  _has_keys_any: [String!]
  _in: [jsonb!]
  _is_null: Boolean
  _lt: jsonb
  _lte: jsonb
  _neq: jsonb
  _nin: [jsonb!]
}

scalar match_status_enum

# Boolean expression to compare columns of type "match_status_enum". All fields are combined with logical 'AND'.
input match_status_enum_comparison_exp {
  _eq: match_status_enum
  _gt: match_status_enum
  _gte: match_status_enum
  _in: [match_status_enum!]
  _is_null: Boolean
  _lt: match_status_enum
  _lte: match_status_enum
  _neq: match_status_enum
  _nin: [match_status_enum!]
}

scalar metadata_type_enum

# Boolean expression to compare columns of type "metadata_type_enum". All fields are combined with logical 'AND'.
input metadata_type_enum_comparison_exp {
  _eq: metadata_type_enum
  _gt: metadata_type_enum
  _gte: metadata_type_enum
  _in: [metadata_type_enum!]
  _is_null: Boolean
  _lt: metadata_type_enum
  _lte: metadata_type_enum
  _neq: metadata_type_enum
  _nin: [metadata_type_enum!]
}

# mutation root
type mutation_root {
  # delete data from the table: "alert_definitions"
  delete_alert_definitions(
    # filter the rows which have to be deleted
    where: alert_definitions_bool_exp!
  ): alert_definitions_mutation_response

  # delete single row from the table: "alert_definitions"
  delete_alert_definitions_by_pk(id: Int!): alert_definitions

  # delete data from the table: "alert_notifications"
  delete_alert_notifications(
    # filter the rows which have to be deleted
    where: alert_notifications_bool_exp!
  ): alert_notifications_mutation_response

  # delete single row from the table: "alert_notifications"
  delete_alert_notifications_by_pk(id: Int!): alert_notifications

  # delete data from the table: "banned_raw_pipeline_entity_match"
  delete_banned_raw_pipeline_entity_match(
    # filter the rows which have to be deleted
    where: banned_raw_pipeline_entity_match_bool_exp!
  ): banned_raw_pipeline_entity_match_mutation_response

  # delete single row from the table: "banned_raw_pipeline_entity_match"
  delete_banned_raw_pipeline_entity_match_by_pk(id: Int!): banned_raw_pipeline_entity_match

  # delete data from the table: "canonical_pipeline_entity"
  delete_canonical_pipeline_entity(
    # filter the rows which have to be deleted
    where: canonical_pipeline_entity_bool_exp!
  ): canonical_pipeline_entity_mutation_response

  # delete single row from the table: "canonical_pipeline_entity"
  delete_canonical_pipeline_entity_by_pk(id: Int!): canonical_pipeline_entity

  # delete data from the table: "case"
  delete_case(
    # filter the rows which have to be deleted
    where: case_bool_exp!
  ): case_mutation_response

  # delete single row from the table: "case"
  delete_case_by_pk(id: Int!): case

  # delete data from the table: "case_event"
  delete_case_event(
    # filter the rows which have to be deleted
    where: case_event_bool_exp!
  ): case_event_mutation_response

  # delete single row from the table: "case_event"
  delete_case_event_by_pk(id: Int!): case_event

  # delete data from the table: "case_metadata"
  delete_case_metadata(
    # filter the rows which have to be deleted
    where: case_metadata_bool_exp!
  ): case_metadata_mutation_response

  # delete single row from the table: "case_metadata"
  delete_case_metadata_by_pk(case_id: Int!, case_metadata_type_id: Int!): case_metadata

  # delete data from the table: "case_metadata_type"
  delete_case_metadata_type(
    # filter the rows which have to be deleted
    where: case_metadata_type_bool_exp!
  ): case_metadata_type_mutation_response

  # delete single row from the table: "case_metadata_type"
  delete_case_metadata_type_by_pk(id: Int!): case_metadata_type

  # delete data from the table: "case_status_type"
  delete_case_status_type(
    # filter the rows which have to be deleted
    where: case_status_type_bool_exp!
  ): case_status_type_mutation_response

  # delete single row from the table: "case_status_type"
  delete_case_status_type_by_pk(id: Int!): case_status_type

  # delete data from the table: "case_type"
  delete_case_type(
    # filter the rows which have to be deleted
    where: case_type_bool_exp!
  ): case_type_mutation_response

  # delete single row from the table: "case_type"
  delete_case_type_by_pk(id: Int!): case_type

  # delete data from the table: "case_type_default_event"
  delete_case_type_default_event(
    # filter the rows which have to be deleted
    where: case_type_default_event_bool_exp!
  ): case_type_default_event_mutation_response

  # delete single row from the table: "case_type_default_event"
  delete_case_type_default_event_by_pk(case_event_id: Int!, case_type_id: Int!): case_type_default_event

  # delete data from the table: "case_type_default_field"
  delete_case_type_default_field(
    # filter the rows which have to be deleted
    where: case_type_default_field_bool_exp!
  ): case_type_default_field_mutation_response

  # delete single row from the table: "case_type_default_field"
  delete_case_type_default_field_by_pk(case_type_id: Int!, field_id: String!): case_type_default_field

  # delete data from the table: "case_type_default_status"
  delete_case_type_default_status(
    # filter the rows which have to be deleted
    where: case_type_default_status_bool_exp!
  ): case_type_default_status_mutation_response

  # delete single row from the table: "case_type_default_status"
  delete_case_type_default_status_by_pk(case_status_type_id: Int!, case_type_id: Int!): case_type_default_status

  # delete data from the table: "case_type_metadata_from_druid_dimension"
  delete_case_type_metadata_from_druid_dimension(
    # filter the rows which have to be deleted
    where: case_type_metadata_from_druid_dimension_bool_exp!
  ): case_type_metadata_from_druid_dimension_mutation_response

  # delete single row from the table: "case_type_metadata_from_druid_dimension"
  delete_case_type_metadata_from_druid_dimension_by_pk(id: Int!): case_type_metadata_from_druid_dimension

  # delete data from the table: "case_type_metadata_from_druid_field"
  delete_case_type_metadata_from_druid_field(
    # filter the rows which have to be deleted
    where: case_type_metadata_from_druid_field_bool_exp!
  ): case_type_metadata_from_druid_field_mutation_response

  # delete single row from the table: "case_type_metadata_from_druid_field"
  delete_case_type_metadata_from_druid_field_by_pk(id: Int!): case_type_metadata_from_druid_field

  # delete data from the table: "category"
  delete_category(
    # filter the rows which have to be deleted
    where: category_bool_exp!
  ): category_mutation_response

  # delete single row from the table: "category"
  delete_category_by_pk(id: String!): category

  # delete data from the table: "dashboard"
  delete_dashboard(
    # filter the rows which have to be deleted
    where: dashboard_bool_exp!
  ): dashboard_mutation_response

  # delete single row from the table: "dashboard"
  delete_dashboard_by_pk(id: Int!): dashboard

  # delete data from the table: "dashboard_session"
  delete_dashboard_session(
    # filter the rows which have to be deleted
    where: dashboard_session_bool_exp!
  ): dashboard_session_mutation_response

  # delete single row from the table: "dashboard_session"
  delete_dashboard_session_by_pk(uuid: String!): dashboard_session

  # delete data from the table: "data_upload_file_summary"
  delete_data_upload_file_summary(
    # filter the rows which have to be deleted
    where: data_upload_file_summary_bool_exp!
  ): data_upload_file_summary_mutation_response

  # delete single row from the table: "data_upload_file_summary"
  delete_data_upload_file_summary_by_pk(id: Int!): data_upload_file_summary

  # delete data from the table: "dataprep_flow"
  delete_dataprep_flow(
    # filter the rows which have to be deleted
    where: dataprep_flow_bool_exp!
  ): dataprep_flow_mutation_response

  # delete single row from the table: "dataprep_flow"
  delete_dataprep_flow_by_pk(id: Int!): dataprep_flow

  # delete data from the table: "dataprep_job"
  delete_dataprep_job(
    # filter the rows which have to be deleted
    where: dataprep_job_bool_exp!
  ): dataprep_job_mutation_response

  # delete single row from the table: "dataprep_job"
  delete_dataprep_job_by_pk(id: Int!): dataprep_job

  # delete data from the table: "dimension"
  delete_dimension(
    # filter the rows which have to be deleted
    where: dimension_bool_exp!
  ): dimension_mutation_response

  # delete single row from the table: "dimension"
  delete_dimension_by_pk(id: String!): dimension

  # delete data from the table: "dimension_category"
  delete_dimension_category(
    # filter the rows which have to be deleted
    where: dimension_category_bool_exp!
  ): dimension_category_mutation_response

  # delete single row from the table: "dimension_category"
  delete_dimension_category_by_pk(id: String!): dimension_category

  # delete data from the table: "dimension_category_mapping"
  delete_dimension_category_mapping(
    # filter the rows which have to be deleted
    where: dimension_category_mapping_bool_exp!
  ): dimension_category_mapping_mutation_response

  # delete single row from the table: "dimension_category_mapping"
  delete_dimension_category_mapping_by_pk(id: Int!): dimension_category_mapping

  # delete data from the table: "external_alert_activity_to_ignore"
  delete_external_alert_activity_to_ignore(
    # filter the rows which have to be deleted
    where: external_alert_activity_to_ignore_bool_exp!
  ): external_alert_activity_to_ignore_mutation_response

  # delete single row from the table: "external_alert_activity_to_ignore"
  delete_external_alert_activity_to_ignore_by_pk(activity: String!, external_alert_type_id: Int!): external_alert_activity_to_ignore

  # delete data from the table: "external_alert_type"
  delete_external_alert_type(
    # filter the rows which have to be deleted
    where: external_alert_type_bool_exp!
  ): external_alert_type_mutation_response

  # delete single row from the table: "external_alert_type"
  delete_external_alert_type_by_pk(id: Int!): external_alert_type

  # delete data from the table: "feed_update"
  delete_feed_update(
    # filter the rows which have to be deleted
    where: feed_update_bool_exp!
  ): feed_update_mutation_response

  # delete single row from the table: "feed_update"
  delete_feed_update_by_pk(id: Int!): feed_update

  # delete data from the table: "feed_update_type"
  delete_feed_update_type(
    # filter the rows which have to be deleted
    where: feed_update_type_bool_exp!
  ): feed_update_type_mutation_response

  # delete single row from the table: "feed_update_type"
  delete_feed_update_type_by_pk(id: Int!): feed_update_type

  # delete data from the table: "field"
  delete_field(
    # filter the rows which have to be deleted
    where: field_bool_exp!
  ): field_mutation_response

  # delete single row from the table: "field"
  delete_field_by_pk(id: String!): field

  # delete data from the table: "field_category_mapping"
  delete_field_category_mapping(
    # filter the rows which have to be deleted
    where: field_category_mapping_bool_exp!
  ): field_category_mapping_mutation_response

  # delete single row from the table: "field_category_mapping"
  delete_field_category_mapping_by_pk(id: Int!): field_category_mapping

  # delete data from the table: "field_dimension_mapping"
  delete_field_dimension_mapping(
    # filter the rows which have to be deleted
    where: field_dimension_mapping_bool_exp!
  ): field_dimension_mapping_mutation_response

  # delete single row from the table: "field_dimension_mapping"
  delete_field_dimension_mapping_by_pk(id: Int!): field_dimension_mapping

  # delete data from the table: "field_pipeline_datasource_mapping"
  delete_field_pipeline_datasource_mapping(
    # filter the rows which have to be deleted
    where: field_pipeline_datasource_mapping_bool_exp!
  ): field_pipeline_datasource_mapping_mutation_response

  # delete single row from the table: "field_pipeline_datasource_mapping"
  delete_field_pipeline_datasource_mapping_by_pk(id: Int!): field_pipeline_datasource_mapping

  # delete data from the table: "geo_dimension_metadata"
  delete_geo_dimension_metadata(
    # filter the rows which have to be deleted
    where: geo_dimension_metadata_bool_exp!
  ): geo_dimension_metadata_mutation_response

  # delete single row from the table: "geo_dimension_metadata"
  delete_geo_dimension_metadata_by_pk(id: String!): geo_dimension_metadata

  # delete data from the table: "hierarchical_dimension_metadata"
  delete_hierarchical_dimension_metadata(
    # filter the rows which have to be deleted
    where: hierarchical_dimension_metadata_bool_exp!
  ): hierarchical_dimension_metadata_mutation_response

  # delete single row from the table: "hierarchical_dimension_metadata"
  delete_hierarchical_dimension_metadata_by_pk(id: Int!): hierarchical_dimension_metadata

  # delete data from the table: "non_hierarchical_dimension"
  delete_non_hierarchical_dimension(
    # filter the rows which have to be deleted
    where: non_hierarchical_dimension_bool_exp!
  ): non_hierarchical_dimension_mutation_response

  # delete single row from the table: "non_hierarchical_dimension"
  delete_non_hierarchical_dimension_by_pk(id: String!): non_hierarchical_dimension

  # delete data from the table: "pipeline_datasource"
  delete_pipeline_datasource(
    # filter the rows which have to be deleted
    where: pipeline_datasource_bool_exp!
  ): pipeline_datasource_mutation_response

  # delete single row from the table: "pipeline_datasource"
  delete_pipeline_datasource_by_pk(id: String!): pipeline_datasource

  # delete data from the table: "pipeline_entity_match"
  delete_pipeline_entity_match(
    # filter the rows which have to be deleted
    where: pipeline_entity_match_bool_exp!
  ): pipeline_entity_match_mutation_response

  # delete single row from the table: "pipeline_entity_match"
  delete_pipeline_entity_match_by_pk(id: Int!): pipeline_entity_match

  # delete data from the table: "pipeline_entity_type"
  delete_pipeline_entity_type(
    # filter the rows which have to be deleted
    where: pipeline_entity_type_bool_exp!
  ): pipeline_entity_type_mutation_response

  # delete single row from the table: "pipeline_entity_type"
  delete_pipeline_entity_type_by_pk(id: Int!): pipeline_entity_type

  # delete data from the table: "pipeline_run_metadata"
  delete_pipeline_run_metadata(
    # filter the rows which have to be deleted
    where: pipeline_run_metadata_bool_exp!
  ): pipeline_run_metadata_mutation_response

  # delete single row from the table: "pipeline_run_metadata"
  delete_pipeline_run_metadata_by_pk(id: Int!): pipeline_run_metadata

  # delete data from the table: "raw_pipeline_entity"
  delete_raw_pipeline_entity(
    # filter the rows which have to be deleted
    where: raw_pipeline_entity_bool_exp!
  ): raw_pipeline_entity_mutation_response

  # delete single row from the table: "raw_pipeline_entity"
  delete_raw_pipeline_entity_by_pk(id: Int!): raw_pipeline_entity

  # delete data from the table: "self_serve_source"
  delete_self_serve_source(
    # filter the rows which have to be deleted
    where: self_serve_source_bool_exp!
  ): self_serve_source_mutation_response

  # delete single row from the table: "self_serve_source"
  delete_self_serve_source_by_pk(id: Int!): self_serve_source

  # delete data from the table: "unpublished_field"
  delete_unpublished_field(
    # filter the rows which have to be deleted
    where: unpublished_field_bool_exp!
  ): unpublished_field_mutation_response

  # delete single row from the table: "unpublished_field"
  delete_unpublished_field_by_pk(id: String!): unpublished_field

  # delete data from the table: "unpublished_field_category_mapping"
  delete_unpublished_field_category_mapping(
    # filter the rows which have to be deleted
    where: unpublished_field_category_mapping_bool_exp!
  ): unpublished_field_category_mapping_mutation_response

  # delete single row from the table: "unpublished_field_category_mapping"
  delete_unpublished_field_category_mapping_by_pk(id: Int!): unpublished_field_category_mapping

  # delete data from the table: "unpublished_field_dimension_mapping"
  delete_unpublished_field_dimension_mapping(
    # filter the rows which have to be deleted
    where: unpublished_field_dimension_mapping_bool_exp!
  ): unpublished_field_dimension_mapping_mutation_response

  # delete single row from the table: "unpublished_field_dimension_mapping"
  delete_unpublished_field_dimension_mapping_by_pk(id: Int!): unpublished_field_dimension_mapping

  # delete data from the table: "unpublished_field_pipeline_datasource_mapping"
  delete_unpublished_field_pipeline_datasource_mapping(
    # filter the rows which have to be deleted
    where: unpublished_field_pipeline_datasource_mapping_bool_exp!
  ): unpublished_field_pipeline_datasource_mapping_mutation_response

  # delete single row from the table: "unpublished_field_pipeline_datasource_mapping"
  delete_unpublished_field_pipeline_datasource_mapping_by_pk(id: Int!): unpublished_field_pipeline_datasource_mapping

  # insert data into the table: "alert_definitions"
  insert_alert_definitions(
    # the rows to be inserted
    objects: [alert_definitions_insert_input!]!

    # upsert condition
    on_conflict: alert_definitions_on_conflict
  ): alert_definitions_mutation_response

  # insert a single row into the table: "alert_definitions"
  insert_alert_definitions_one(
    # the row to be inserted
    object: alert_definitions_insert_input!

    # upsert condition
    on_conflict: alert_definitions_on_conflict
  ): alert_definitions

  # insert data into the table: "alert_notifications"
  insert_alert_notifications(
    # the rows to be inserted
    objects: [alert_notifications_insert_input!]!

    # upsert condition
    on_conflict: alert_notifications_on_conflict
  ): alert_notifications_mutation_response

  # insert a single row into the table: "alert_notifications"
  insert_alert_notifications_one(
    # the row to be inserted
    object: alert_notifications_insert_input!

    # upsert condition
    on_conflict: alert_notifications_on_conflict
  ): alert_notifications

  # insert data into the table: "banned_raw_pipeline_entity_match"
  insert_banned_raw_pipeline_entity_match(
    # the rows to be inserted
    objects: [banned_raw_pipeline_entity_match_insert_input!]!

    # upsert condition
    on_conflict: banned_raw_pipeline_entity_match_on_conflict
  ): banned_raw_pipeline_entity_match_mutation_response

  # insert a single row into the table: "banned_raw_pipeline_entity_match"
  insert_banned_raw_pipeline_entity_match_one(
    # the row to be inserted
    object: banned_raw_pipeline_entity_match_insert_input!

    # upsert condition
    on_conflict: banned_raw_pipeline_entity_match_on_conflict
  ): banned_raw_pipeline_entity_match

  # insert data into the table: "canonical_pipeline_entity"
  insert_canonical_pipeline_entity(
    # the rows to be inserted
    objects: [canonical_pipeline_entity_insert_input!]!

    # upsert condition
    on_conflict: canonical_pipeline_entity_on_conflict
  ): canonical_pipeline_entity_mutation_response

  # insert a single row into the table: "canonical_pipeline_entity"
  insert_canonical_pipeline_entity_one(
    # the row to be inserted
    object: canonical_pipeline_entity_insert_input!

    # upsert condition
    on_conflict: canonical_pipeline_entity_on_conflict
  ): canonical_pipeline_entity

  # insert data into the table: "case"
  insert_case(
    # the rows to be inserted
    objects: [case_insert_input!]!

    # upsert condition
    on_conflict: case_on_conflict
  ): case_mutation_response

  # insert data into the table: "case_event"
  insert_case_event(
    # the rows to be inserted
    objects: [case_event_insert_input!]!

    # upsert condition
    on_conflict: case_event_on_conflict
  ): case_event_mutation_response

  # insert a single row into the table: "case_event"
  insert_case_event_one(
    # the row to be inserted
    object: case_event_insert_input!

    # upsert condition
    on_conflict: case_event_on_conflict
  ): case_event

  # insert data into the table: "case_metadata"
  insert_case_metadata(
    # the rows to be inserted
    objects: [case_metadata_insert_input!]!

    # upsert condition
    on_conflict: case_metadata_on_conflict
  ): case_metadata_mutation_response

  # insert a single row into the table: "case_metadata"
  insert_case_metadata_one(
    # the row to be inserted
    object: case_metadata_insert_input!

    # upsert condition
    on_conflict: case_metadata_on_conflict
  ): case_metadata

  # insert data into the table: "case_metadata_type"
  insert_case_metadata_type(
    # the rows to be inserted
    objects: [case_metadata_type_insert_input!]!

    # upsert condition
    on_conflict: case_metadata_type_on_conflict
  ): case_metadata_type_mutation_response

  # insert a single row into the table: "case_metadata_type"
  insert_case_metadata_type_one(
    # the row to be inserted
    object: case_metadata_type_insert_input!

    # upsert condition
    on_conflict: case_metadata_type_on_conflict
  ): case_metadata_type

  # insert a single row into the table: "case"
  insert_case_one(
    # the row to be inserted
    object: case_insert_input!

    # upsert condition
    on_conflict: case_on_conflict
  ): case

  # insert data into the table: "case_status_type"
  insert_case_status_type(
    # the rows to be inserted
    objects: [case_status_type_insert_input!]!

    # upsert condition
    on_conflict: case_status_type_on_conflict
  ): case_status_type_mutation_response

  # insert a single row into the table: "case_status_type"
  insert_case_status_type_one(
    # the row to be inserted
    object: case_status_type_insert_input!

    # upsert condition
    on_conflict: case_status_type_on_conflict
  ): case_status_type

  # insert data into the table: "case_type"
  insert_case_type(
    # the rows to be inserted
    objects: [case_type_insert_input!]!

    # upsert condition
    on_conflict: case_type_on_conflict
  ): case_type_mutation_response

  # insert data into the table: "case_type_default_event"
  insert_case_type_default_event(
    # the rows to be inserted
    objects: [case_type_default_event_insert_input!]!

    # upsert condition
    on_conflict: case_type_default_event_on_conflict
  ): case_type_default_event_mutation_response

  # insert a single row into the table: "case_type_default_event"
  insert_case_type_default_event_one(
    # the row to be inserted
    object: case_type_default_event_insert_input!

    # upsert condition
    on_conflict: case_type_default_event_on_conflict
  ): case_type_default_event

  # insert data into the table: "case_type_default_field"
  insert_case_type_default_field(
    # the rows to be inserted
    objects: [case_type_default_field_insert_input!]!

    # upsert condition
    on_conflict: case_type_default_field_on_conflict
  ): case_type_default_field_mutation_response

  # insert a single row into the table: "case_type_default_field"
  insert_case_type_default_field_one(
    # the row to be inserted
    object: case_type_default_field_insert_input!

    # upsert condition
    on_conflict: case_type_default_field_on_conflict
  ): case_type_default_field

  # insert data into the table: "case_type_default_status"
  insert_case_type_default_status(
    # the rows to be inserted
    objects: [case_type_default_status_insert_input!]!

    # upsert condition
    on_conflict: case_type_default_status_on_conflict
  ): case_type_default_status_mutation_response

  # insert a single row into the table: "case_type_default_status"
  insert_case_type_default_status_one(
    # the row to be inserted
    object: case_type_default_status_insert_input!

    # upsert condition
    on_conflict: case_type_default_status_on_conflict
  ): case_type_default_status

  # insert data into the table: "case_type_metadata_from_druid_dimension"
  insert_case_type_metadata_from_druid_dimension(
    # the rows to be inserted
    objects: [case_type_metadata_from_druid_dimension_insert_input!]!

    # upsert condition
    on_conflict: case_type_metadata_from_druid_dimension_on_conflict
  ): case_type_metadata_from_druid_dimension_mutation_response

  # insert a single row into the table: "case_type_metadata_from_druid_dimension"
  insert_case_type_metadata_from_druid_dimension_one(
    # the row to be inserted
    object: case_type_metadata_from_druid_dimension_insert_input!

    # upsert condition
    on_conflict: case_type_metadata_from_druid_dimension_on_conflict
  ): case_type_metadata_from_druid_dimension

  # insert data into the table: "case_type_metadata_from_druid_field"
  insert_case_type_metadata_from_druid_field(
    # the rows to be inserted
    objects: [case_type_metadata_from_druid_field_insert_input!]!

    # upsert condition
    on_conflict: case_type_metadata_from_druid_field_on_conflict
  ): case_type_metadata_from_druid_field_mutation_response

  # insert a single row into the table: "case_type_metadata_from_druid_field"
  insert_case_type_metadata_from_druid_field_one(
    # the row to be inserted
    object: case_type_metadata_from_druid_field_insert_input!

    # upsert condition
    on_conflict: case_type_metadata_from_druid_field_on_conflict
  ): case_type_metadata_from_druid_field

  # insert a single row into the table: "case_type"
  insert_case_type_one(
    # the row to be inserted
    object: case_type_insert_input!

    # upsert condition
    on_conflict: case_type_on_conflict
  ): case_type

  # insert data into the table: "category"
  insert_category(
    # the rows to be inserted
    objects: [category_insert_input!]!

    # upsert condition
    on_conflict: category_on_conflict
  ): category_mutation_response

  # insert a single row into the table: "category"
  insert_category_one(
    # the row to be inserted
    object: category_insert_input!

    # upsert condition
    on_conflict: category_on_conflict
  ): category

  # insert data into the table: "dashboard"
  insert_dashboard(
    # the rows to be inserted
    objects: [dashboard_insert_input!]!

    # upsert condition
    on_conflict: dashboard_on_conflict
  ): dashboard_mutation_response

  # insert a single row into the table: "dashboard"
  insert_dashboard_one(
    # the row to be inserted
    object: dashboard_insert_input!

    # upsert condition
    on_conflict: dashboard_on_conflict
  ): dashboard

  # insert data into the table: "dashboard_session"
  insert_dashboard_session(
    # the rows to be inserted
    objects: [dashboard_session_insert_input!]!

    # upsert condition
    on_conflict: dashboard_session_on_conflict
  ): dashboard_session_mutation_response

  # insert a single row into the table: "dashboard_session"
  insert_dashboard_session_one(
    # the row to be inserted
    object: dashboard_session_insert_input!

    # upsert condition
    on_conflict: dashboard_session_on_conflict
  ): dashboard_session

  # insert data into the table: "data_upload_file_summary"
  insert_data_upload_file_summary(
    # the rows to be inserted
    objects: [data_upload_file_summary_insert_input!]!

    # upsert condition
    on_conflict: data_upload_file_summary_on_conflict
  ): data_upload_file_summary_mutation_response

  # insert a single row into the table: "data_upload_file_summary"
  insert_data_upload_file_summary_one(
    # the row to be inserted
    object: data_upload_file_summary_insert_input!

    # upsert condition
    on_conflict: data_upload_file_summary_on_conflict
  ): data_upload_file_summary

  # insert data into the table: "dataprep_flow"
  insert_dataprep_flow(
    # the rows to be inserted
    objects: [dataprep_flow_insert_input!]!

    # upsert condition
    on_conflict: dataprep_flow_on_conflict
  ): dataprep_flow_mutation_response

  # insert a single row into the table: "dataprep_flow"
  insert_dataprep_flow_one(
    # the row to be inserted
    object: dataprep_flow_insert_input!

    # upsert condition
    on_conflict: dataprep_flow_on_conflict
  ): dataprep_flow

  # insert data into the table: "dataprep_job"
  insert_dataprep_job(
    # the rows to be inserted
    objects: [dataprep_job_insert_input!]!

    # upsert condition
    on_conflict: dataprep_job_on_conflict
  ): dataprep_job_mutation_response

  # insert a single row into the table: "dataprep_job"
  insert_dataprep_job_one(
    # the row to be inserted
    object: dataprep_job_insert_input!

    # upsert condition
    on_conflict: dataprep_job_on_conflict
  ): dataprep_job

  # insert data into the table: "dimension"
  insert_dimension(
    # the rows to be inserted
    objects: [dimension_insert_input!]!

    # upsert condition
    on_conflict: dimension_on_conflict
  ): dimension_mutation_response

  # insert data into the table: "dimension_category"
  insert_dimension_category(
    # the rows to be inserted
    objects: [dimension_category_insert_input!]!

    # upsert condition
    on_conflict: dimension_category_on_conflict
  ): dimension_category_mutation_response

  # insert data into the table: "dimension_category_mapping"
  insert_dimension_category_mapping(
    # the rows to be inserted
    objects: [dimension_category_mapping_insert_input!]!

    # upsert condition
    on_conflict: dimension_category_mapping_on_conflict
  ): dimension_category_mapping_mutation_response

  # insert a single row into the table: "dimension_category_mapping"
  insert_dimension_category_mapping_one(
    # the row to be inserted
    object: dimension_category_mapping_insert_input!

    # upsert condition
    on_conflict: dimension_category_mapping_on_conflict
  ): dimension_category_mapping

  # insert a single row into the table: "dimension_category"
  insert_dimension_category_one(
    # the row to be inserted
    object: dimension_category_insert_input!

    # upsert condition
    on_conflict: dimension_category_on_conflict
  ): dimension_category

  # insert a single row into the table: "dimension"
  insert_dimension_one(
    # the row to be inserted
    object: dimension_insert_input!

    # upsert condition
    on_conflict: dimension_on_conflict
  ): dimension

  # insert data into the table: "external_alert_activity_to_ignore"
  insert_external_alert_activity_to_ignore(
    # the rows to be inserted
    objects: [external_alert_activity_to_ignore_insert_input!]!

    # upsert condition
    on_conflict: external_alert_activity_to_ignore_on_conflict
  ): external_alert_activity_to_ignore_mutation_response

  # insert a single row into the table: "external_alert_activity_to_ignore"
  insert_external_alert_activity_to_ignore_one(
    # the row to be inserted
    object: external_alert_activity_to_ignore_insert_input!

    # upsert condition
    on_conflict: external_alert_activity_to_ignore_on_conflict
  ): external_alert_activity_to_ignore

  # insert data into the table: "external_alert_type"
  insert_external_alert_type(
    # the rows to be inserted
    objects: [external_alert_type_insert_input!]!

    # upsert condition
    on_conflict: external_alert_type_on_conflict
  ): external_alert_type_mutation_response

  # insert a single row into the table: "external_alert_type"
  insert_external_alert_type_one(
    # the row to be inserted
    object: external_alert_type_insert_input!

    # upsert condition
    on_conflict: external_alert_type_on_conflict
  ): external_alert_type

  # insert data into the table: "feed_update"
  insert_feed_update(
    # the rows to be inserted
    objects: [feed_update_insert_input!]!

    # upsert condition
    on_conflict: feed_update_on_conflict
  ): feed_update_mutation_response

  # insert a single row into the table: "feed_update"
  insert_feed_update_one(
    # the row to be inserted
    object: feed_update_insert_input!

    # upsert condition
    on_conflict: feed_update_on_conflict
  ): feed_update

  # insert data into the table: "feed_update_type"
  insert_feed_update_type(
    # the rows to be inserted
    objects: [feed_update_type_insert_input!]!

    # upsert condition
    on_conflict: feed_update_type_on_conflict
  ): feed_update_type_mutation_response

  # insert a single row into the table: "feed_update_type"
  insert_feed_update_type_one(
    # the row to be inserted
    object: feed_update_type_insert_input!

    # upsert condition
    on_conflict: feed_update_type_on_conflict
  ): feed_update_type

  # insert data into the table: "field"
  insert_field(
    # the rows to be inserted
    objects: [field_insert_input!]!

    # upsert condition
    on_conflict: field_on_conflict
  ): field_mutation_response

  # insert data into the table: "field_category_mapping"
  insert_field_category_mapping(
    # the rows to be inserted
    objects: [field_category_mapping_insert_input!]!

    # upsert condition
    on_conflict: field_category_mapping_on_conflict
  ): field_category_mapping_mutation_response

  # insert a single row into the table: "field_category_mapping"
  insert_field_category_mapping_one(
    # the row to be inserted
    object: field_category_mapping_insert_input!

    # upsert condition
    on_conflict: field_category_mapping_on_conflict
  ): field_category_mapping

  # insert data into the table: "field_dimension_mapping"
  insert_field_dimension_mapping(
    # the rows to be inserted
    objects: [field_dimension_mapping_insert_input!]!

    # upsert condition
    on_conflict: field_dimension_mapping_on_conflict
  ): field_dimension_mapping_mutation_response

  # insert a single row into the table: "field_dimension_mapping"
  insert_field_dimension_mapping_one(
    # the row to be inserted
    object: field_dimension_mapping_insert_input!

    # upsert condition
    on_conflict: field_dimension_mapping_on_conflict
  ): field_dimension_mapping

  # insert a single row into the table: "field"
  insert_field_one(
    # the row to be inserted
    object: field_insert_input!

    # upsert condition
    on_conflict: field_on_conflict
  ): field

  # insert data into the table: "field_pipeline_datasource_mapping"
  insert_field_pipeline_datasource_mapping(
    # the rows to be inserted
    objects: [field_pipeline_datasource_mapping_insert_input!]!

    # upsert condition
    on_conflict: field_pipeline_datasource_mapping_on_conflict
  ): field_pipeline_datasource_mapping_mutation_response

  # insert a single row into the table: "field_pipeline_datasource_mapping"
  insert_field_pipeline_datasource_mapping_one(
    # the row to be inserted
    object: field_pipeline_datasource_mapping_insert_input!

    # upsert condition
    on_conflict: field_pipeline_datasource_mapping_on_conflict
  ): field_pipeline_datasource_mapping

  # insert data into the table: "geo_dimension_metadata"
  insert_geo_dimension_metadata(
    # the rows to be inserted
    objects: [geo_dimension_metadata_insert_input!]!

    # upsert condition
    on_conflict: geo_dimension_metadata_on_conflict
  ): geo_dimension_metadata_mutation_response

  # insert a single row into the table: "geo_dimension_metadata"
  insert_geo_dimension_metadata_one(
    # the row to be inserted
    object: geo_dimension_metadata_insert_input!

    # upsert condition
    on_conflict: geo_dimension_metadata_on_conflict
  ): geo_dimension_metadata

  # insert data into the table: "hierarchical_dimension_metadata"
  insert_hierarchical_dimension_metadata(
    # the rows to be inserted
    objects: [hierarchical_dimension_metadata_insert_input!]!

    # upsert condition
    on_conflict: hierarchical_dimension_metadata_on_conflict
  ): hierarchical_dimension_metadata_mutation_response

  # insert a single row into the table: "hierarchical_dimension_metadata"
  insert_hierarchical_dimension_metadata_one(
    # the row to be inserted
    object: hierarchical_dimension_metadata_insert_input!

    # upsert condition
    on_conflict: hierarchical_dimension_metadata_on_conflict
  ): hierarchical_dimension_metadata

  # insert data into the table: "non_hierarchical_dimension"
  insert_non_hierarchical_dimension(
    # the rows to be inserted
    objects: [non_hierarchical_dimension_insert_input!]!

    # upsert condition
    on_conflict: non_hierarchical_dimension_on_conflict
  ): non_hierarchical_dimension_mutation_response

  # insert a single row into the table: "non_hierarchical_dimension"
  insert_non_hierarchical_dimension_one(
    # the row to be inserted
    object: non_hierarchical_dimension_insert_input!

    # upsert condition
    on_conflict: non_hierarchical_dimension_on_conflict
  ): non_hierarchical_dimension

  # insert data into the table: "pipeline_datasource"
  insert_pipeline_datasource(
    # the rows to be inserted
    objects: [pipeline_datasource_insert_input!]!

    # upsert condition
    on_conflict: pipeline_datasource_on_conflict
  ): pipeline_datasource_mutation_response

  # insert a single row into the table: "pipeline_datasource"
  insert_pipeline_datasource_one(
    # the row to be inserted
    object: pipeline_datasource_insert_input!

    # upsert condition
    on_conflict: pipeline_datasource_on_conflict
  ): pipeline_datasource

  # insert data into the table: "pipeline_entity_match"
  insert_pipeline_entity_match(
    # the rows to be inserted
    objects: [pipeline_entity_match_insert_input!]!

    # upsert condition
    on_conflict: pipeline_entity_match_on_conflict
  ): pipeline_entity_match_mutation_response

  # insert a single row into the table: "pipeline_entity_match"
  insert_pipeline_entity_match_one(
    # the row to be inserted
    object: pipeline_entity_match_insert_input!

    # upsert condition
    on_conflict: pipeline_entity_match_on_conflict
  ): pipeline_entity_match

  # insert data into the table: "pipeline_entity_type"
  insert_pipeline_entity_type(
    # the rows to be inserted
    objects: [pipeline_entity_type_insert_input!]!

    # upsert condition
    on_conflict: pipeline_entity_type_on_conflict
  ): pipeline_entity_type_mutation_response

  # insert a single row into the table: "pipeline_entity_type"
  insert_pipeline_entity_type_one(
    # the row to be inserted
    object: pipeline_entity_type_insert_input!

    # upsert condition
    on_conflict: pipeline_entity_type_on_conflict
  ): pipeline_entity_type

  # insert data into the table: "pipeline_run_metadata"
  insert_pipeline_run_metadata(
    # the rows to be inserted
    objects: [pipeline_run_metadata_insert_input!]!

    # upsert condition
    on_conflict: pipeline_run_metadata_on_conflict
  ): pipeline_run_metadata_mutation_response

  # insert a single row into the table: "pipeline_run_metadata"
  insert_pipeline_run_metadata_one(
    # the row to be inserted
    object: pipeline_run_metadata_insert_input!

    # upsert condition
    on_conflict: pipeline_run_metadata_on_conflict
  ): pipeline_run_metadata

  # insert data into the table: "raw_pipeline_entity"
  insert_raw_pipeline_entity(
    # the rows to be inserted
    objects: [raw_pipeline_entity_insert_input!]!

    # upsert condition
    on_conflict: raw_pipeline_entity_on_conflict
  ): raw_pipeline_entity_mutation_response

  # insert a single row into the table: "raw_pipeline_entity"
  insert_raw_pipeline_entity_one(
    # the row to be inserted
    object: raw_pipeline_entity_insert_input!

    # upsert condition
    on_conflict: raw_pipeline_entity_on_conflict
  ): raw_pipeline_entity

  # insert data into the table: "self_serve_source"
  insert_self_serve_source(
    # the rows to be inserted
    objects: [self_serve_source_insert_input!]!

    # upsert condition
    on_conflict: self_serve_source_on_conflict
  ): self_serve_source_mutation_response

  # insert a single row into the table: "self_serve_source"
  insert_self_serve_source_one(
    # the row to be inserted
    object: self_serve_source_insert_input!

    # upsert condition
    on_conflict: self_serve_source_on_conflict
  ): self_serve_source

  # insert data into the table: "unpublished_field"
  insert_unpublished_field(
    # the rows to be inserted
    objects: [unpublished_field_insert_input!]!

    # upsert condition
    on_conflict: unpublished_field_on_conflict
  ): unpublished_field_mutation_response

  # insert data into the table: "unpublished_field_category_mapping"
  insert_unpublished_field_category_mapping(
    # the rows to be inserted
    objects: [unpublished_field_category_mapping_insert_input!]!

    # upsert condition
    on_conflict: unpublished_field_category_mapping_on_conflict
  ): unpublished_field_category_mapping_mutation_response

  # insert a single row into the table: "unpublished_field_category_mapping"
  insert_unpublished_field_category_mapping_one(
    # the row to be inserted
    object: unpublished_field_category_mapping_insert_input!

    # upsert condition
    on_conflict: unpublished_field_category_mapping_on_conflict
  ): unpublished_field_category_mapping

  # insert data into the table: "unpublished_field_dimension_mapping"
  insert_unpublished_field_dimension_mapping(
    # the rows to be inserted
    objects: [unpublished_field_dimension_mapping_insert_input!]!

    # upsert condition
    on_conflict: unpublished_field_dimension_mapping_on_conflict
  ): unpublished_field_dimension_mapping_mutation_response

  # insert a single row into the table: "unpublished_field_dimension_mapping"
  insert_unpublished_field_dimension_mapping_one(
    # the row to be inserted
    object: unpublished_field_dimension_mapping_insert_input!

    # upsert condition
    on_conflict: unpublished_field_dimension_mapping_on_conflict
  ): unpublished_field_dimension_mapping

  # insert a single row into the table: "unpublished_field"
  insert_unpublished_field_one(
    # the row to be inserted
    object: unpublished_field_insert_input!

    # upsert condition
    on_conflict: unpublished_field_on_conflict
  ): unpublished_field

  # insert data into the table: "unpublished_field_pipeline_datasource_mapping"
  insert_unpublished_field_pipeline_datasource_mapping(
    # the rows to be inserted
    objects: [unpublished_field_pipeline_datasource_mapping_insert_input!]!

    # upsert condition
    on_conflict: unpublished_field_pipeline_datasource_mapping_on_conflict
  ): unpublished_field_pipeline_datasource_mapping_mutation_response

  # insert a single row into the table: "unpublished_field_pipeline_datasource_mapping"
  insert_unpublished_field_pipeline_datasource_mapping_one(
    # the row to be inserted
    object: unpublished_field_pipeline_datasource_mapping_insert_input!

    # upsert condition
    on_conflict: unpublished_field_pipeline_datasource_mapping_on_conflict
  ): unpublished_field_pipeline_datasource_mapping

  # update data of the table: "alert_definitions"
  update_alert_definitions(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: alert_definitions_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: alert_definitions_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: alert_definitions_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: alert_definitions_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: alert_definitions_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: alert_definitions_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: alert_definitions_set_input

    # filter the rows which have to be updated
    where: alert_definitions_bool_exp!
  ): alert_definitions_mutation_response

  # update single row of the table: "alert_definitions"
  update_alert_definitions_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: alert_definitions_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: alert_definitions_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: alert_definitions_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: alert_definitions_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: alert_definitions_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: alert_definitions_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: alert_definitions_set_input
    pk_columns: alert_definitions_pk_columns_input!
  ): alert_definitions

  # update multiples rows of table: "alert_definitions"
  update_alert_definitions_many(
    # updates to execute, in order
    updates: [alert_definitions_updates!]!
  ): [alert_definitions_mutation_response]

  # update data of the table: "alert_notifications"
  update_alert_notifications(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: alert_notifications_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: alert_notifications_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: alert_notifications_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: alert_notifications_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: alert_notifications_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: alert_notifications_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: alert_notifications_set_input

    # filter the rows which have to be updated
    where: alert_notifications_bool_exp!
  ): alert_notifications_mutation_response

  # update single row of the table: "alert_notifications"
  update_alert_notifications_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: alert_notifications_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: alert_notifications_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: alert_notifications_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: alert_notifications_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: alert_notifications_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: alert_notifications_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: alert_notifications_set_input
    pk_columns: alert_notifications_pk_columns_input!
  ): alert_notifications

  # update multiples rows of table: "alert_notifications"
  update_alert_notifications_many(
    # updates to execute, in order
    updates: [alert_notifications_updates!]!
  ): [alert_notifications_mutation_response]

  # update data of the table: "banned_raw_pipeline_entity_match"
  update_banned_raw_pipeline_entity_match(
    # increments the numeric columns with given value of the filtered values
    _inc: banned_raw_pipeline_entity_match_inc_input

    # sets the columns of the filtered rows to the given values
    _set: banned_raw_pipeline_entity_match_set_input

    # filter the rows which have to be updated
    where: banned_raw_pipeline_entity_match_bool_exp!
  ): banned_raw_pipeline_entity_match_mutation_response

  # update single row of the table: "banned_raw_pipeline_entity_match"
  update_banned_raw_pipeline_entity_match_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: banned_raw_pipeline_entity_match_inc_input

    # sets the columns of the filtered rows to the given values
    _set: banned_raw_pipeline_entity_match_set_input
    pk_columns: banned_raw_pipeline_entity_match_pk_columns_input!
  ): banned_raw_pipeline_entity_match

  # update multiples rows of table: "banned_raw_pipeline_entity_match"
  update_banned_raw_pipeline_entity_match_many(
    # updates to execute, in order
    updates: [banned_raw_pipeline_entity_match_updates!]!
  ): [banned_raw_pipeline_entity_match_mutation_response]

  # update data of the table: "canonical_pipeline_entity"
  update_canonical_pipeline_entity(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: canonical_pipeline_entity_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: canonical_pipeline_entity_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: canonical_pipeline_entity_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: canonical_pipeline_entity_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: canonical_pipeline_entity_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: canonical_pipeline_entity_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: canonical_pipeline_entity_set_input

    # filter the rows which have to be updated
    where: canonical_pipeline_entity_bool_exp!
  ): canonical_pipeline_entity_mutation_response

  # update single row of the table: "canonical_pipeline_entity"
  update_canonical_pipeline_entity_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: canonical_pipeline_entity_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: canonical_pipeline_entity_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: canonical_pipeline_entity_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: canonical_pipeline_entity_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: canonical_pipeline_entity_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: canonical_pipeline_entity_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: canonical_pipeline_entity_set_input
    pk_columns: canonical_pipeline_entity_pk_columns_input!
  ): canonical_pipeline_entity

  # update multiples rows of table: "canonical_pipeline_entity"
  update_canonical_pipeline_entity_many(
    # updates to execute, in order
    updates: [canonical_pipeline_entity_updates!]!
  ): [canonical_pipeline_entity_mutation_response]

  # update data of the table: "case"
  update_case(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: case_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: case_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: case_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: case_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: case_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: case_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: case_set_input

    # filter the rows which have to be updated
    where: case_bool_exp!
  ): case_mutation_response

  # update single row of the table: "case"
  update_case_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: case_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: case_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: case_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: case_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: case_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: case_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: case_set_input
    pk_columns: case_pk_columns_input!
  ): case

  # update data of the table: "case_event"
  update_case_event(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: case_event_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: case_event_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: case_event_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: case_event_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: case_event_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: case_event_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: case_event_set_input

    # filter the rows which have to be updated
    where: case_event_bool_exp!
  ): case_event_mutation_response

  # update single row of the table: "case_event"
  update_case_event_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: case_event_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: case_event_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: case_event_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: case_event_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: case_event_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: case_event_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: case_event_set_input
    pk_columns: case_event_pk_columns_input!
  ): case_event

  # update multiples rows of table: "case_event"
  update_case_event_many(
    # updates to execute, in order
    updates: [case_event_updates!]!
  ): [case_event_mutation_response]

  # update multiples rows of table: "case"
  update_case_many(
    # updates to execute, in order
    updates: [case_updates!]!
  ): [case_mutation_response]

  # update data of the table: "case_metadata"
  update_case_metadata(
    # increments the numeric columns with given value of the filtered values
    _inc: case_metadata_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_metadata_set_input

    # filter the rows which have to be updated
    where: case_metadata_bool_exp!
  ): case_metadata_mutation_response

  # update single row of the table: "case_metadata"
  update_case_metadata_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: case_metadata_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_metadata_set_input
    pk_columns: case_metadata_pk_columns_input!
  ): case_metadata

  # update multiples rows of table: "case_metadata"
  update_case_metadata_many(
    # updates to execute, in order
    updates: [case_metadata_updates!]!
  ): [case_metadata_mutation_response]

  # update data of the table: "case_metadata_type"
  update_case_metadata_type(
    # increments the numeric columns with given value of the filtered values
    _inc: case_metadata_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_metadata_type_set_input

    # filter the rows which have to be updated
    where: case_metadata_type_bool_exp!
  ): case_metadata_type_mutation_response

  # update single row of the table: "case_metadata_type"
  update_case_metadata_type_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: case_metadata_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_metadata_type_set_input
    pk_columns: case_metadata_type_pk_columns_input!
  ): case_metadata_type

  # update multiples rows of table: "case_metadata_type"
  update_case_metadata_type_many(
    # updates to execute, in order
    updates: [case_metadata_type_updates!]!
  ): [case_metadata_type_mutation_response]

  # update data of the table: "case_status_type"
  update_case_status_type(
    # increments the numeric columns with given value of the filtered values
    _inc: case_status_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_status_type_set_input

    # filter the rows which have to be updated
    where: case_status_type_bool_exp!
  ): case_status_type_mutation_response

  # update single row of the table: "case_status_type"
  update_case_status_type_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: case_status_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_status_type_set_input
    pk_columns: case_status_type_pk_columns_input!
  ): case_status_type

  # update multiples rows of table: "case_status_type"
  update_case_status_type_many(
    # updates to execute, in order
    updates: [case_status_type_updates!]!
  ): [case_status_type_mutation_response]

  # update data of the table: "case_type"
  update_case_type(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: case_type_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: case_type_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: case_type_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: case_type_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: case_type_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: case_type_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_set_input

    # filter the rows which have to be updated
    where: case_type_bool_exp!
  ): case_type_mutation_response

  # update single row of the table: "case_type"
  update_case_type_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: case_type_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: case_type_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: case_type_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: case_type_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: case_type_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: case_type_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_set_input
    pk_columns: case_type_pk_columns_input!
  ): case_type

  # update data of the table: "case_type_default_event"
  update_case_type_default_event(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_default_event_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_default_event_set_input

    # filter the rows which have to be updated
    where: case_type_default_event_bool_exp!
  ): case_type_default_event_mutation_response

  # update single row of the table: "case_type_default_event"
  update_case_type_default_event_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_default_event_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_default_event_set_input
    pk_columns: case_type_default_event_pk_columns_input!
  ): case_type_default_event

  # update multiples rows of table: "case_type_default_event"
  update_case_type_default_event_many(
    # updates to execute, in order
    updates: [case_type_default_event_updates!]!
  ): [case_type_default_event_mutation_response]

  # update data of the table: "case_type_default_field"
  update_case_type_default_field(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_default_field_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_default_field_set_input

    # filter the rows which have to be updated
    where: case_type_default_field_bool_exp!
  ): case_type_default_field_mutation_response

  # update single row of the table: "case_type_default_field"
  update_case_type_default_field_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_default_field_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_default_field_set_input
    pk_columns: case_type_default_field_pk_columns_input!
  ): case_type_default_field

  # update multiples rows of table: "case_type_default_field"
  update_case_type_default_field_many(
    # updates to execute, in order
    updates: [case_type_default_field_updates!]!
  ): [case_type_default_field_mutation_response]

  # update data of the table: "case_type_default_status"
  update_case_type_default_status(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_default_status_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_default_status_set_input

    # filter the rows which have to be updated
    where: case_type_default_status_bool_exp!
  ): case_type_default_status_mutation_response

  # update single row of the table: "case_type_default_status"
  update_case_type_default_status_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_default_status_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_default_status_set_input
    pk_columns: case_type_default_status_pk_columns_input!
  ): case_type_default_status

  # update multiples rows of table: "case_type_default_status"
  update_case_type_default_status_many(
    # updates to execute, in order
    updates: [case_type_default_status_updates!]!
  ): [case_type_default_status_mutation_response]

  # update multiples rows of table: "case_type"
  update_case_type_many(
    # updates to execute, in order
    updates: [case_type_updates!]!
  ): [case_type_mutation_response]

  # update data of the table: "case_type_metadata_from_druid_dimension"
  update_case_type_metadata_from_druid_dimension(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_metadata_from_druid_dimension_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_metadata_from_druid_dimension_set_input

    # filter the rows which have to be updated
    where: case_type_metadata_from_druid_dimension_bool_exp!
  ): case_type_metadata_from_druid_dimension_mutation_response

  # update single row of the table: "case_type_metadata_from_druid_dimension"
  update_case_type_metadata_from_druid_dimension_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_metadata_from_druid_dimension_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_metadata_from_druid_dimension_set_input
    pk_columns: case_type_metadata_from_druid_dimension_pk_columns_input!
  ): case_type_metadata_from_druid_dimension

  # update multiples rows of table: "case_type_metadata_from_druid_dimension"
  update_case_type_metadata_from_druid_dimension_many(
    # updates to execute, in order
    updates: [case_type_metadata_from_druid_dimension_updates!]!
  ): [case_type_metadata_from_druid_dimension_mutation_response]

  # update data of the table: "case_type_metadata_from_druid_field"
  update_case_type_metadata_from_druid_field(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_metadata_from_druid_field_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_metadata_from_druid_field_set_input

    # filter the rows which have to be updated
    where: case_type_metadata_from_druid_field_bool_exp!
  ): case_type_metadata_from_druid_field_mutation_response

  # update single row of the table: "case_type_metadata_from_druid_field"
  update_case_type_metadata_from_druid_field_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: case_type_metadata_from_druid_field_inc_input

    # sets the columns of the filtered rows to the given values
    _set: case_type_metadata_from_druid_field_set_input
    pk_columns: case_type_metadata_from_druid_field_pk_columns_input!
  ): case_type_metadata_from_druid_field

  # update multiples rows of table: "case_type_metadata_from_druid_field"
  update_case_type_metadata_from_druid_field_many(
    # updates to execute, in order
    updates: [case_type_metadata_from_druid_field_updates!]!
  ): [case_type_metadata_from_druid_field_mutation_response]

  # update data of the table: "category"
  update_category(
    # sets the columns of the filtered rows to the given values
    _set: category_set_input

    # filter the rows which have to be updated
    where: category_bool_exp!
  ): category_mutation_response

  # update single row of the table: "category"
  update_category_by_pk(
    # sets the columns of the filtered rows to the given values
    _set: category_set_input
    pk_columns: category_pk_columns_input!
  ): category

  # update multiples rows of table: "category"
  update_category_many(
    # updates to execute, in order
    updates: [category_updates!]!
  ): [category_mutation_response]

  # update data of the table: "dashboard"
  update_dashboard(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: dashboard_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: dashboard_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: dashboard_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: dashboard_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: dashboard_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: dashboard_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: dashboard_set_input

    # filter the rows which have to be updated
    where: dashboard_bool_exp!
  ): dashboard_mutation_response

  # update single row of the table: "dashboard"
  update_dashboard_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: dashboard_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: dashboard_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: dashboard_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: dashboard_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: dashboard_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: dashboard_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: dashboard_set_input
    pk_columns: dashboard_pk_columns_input!
  ): dashboard

  # update multiples rows of table: "dashboard"
  update_dashboard_many(
    # updates to execute, in order
    updates: [dashboard_updates!]!
  ): [dashboard_mutation_response]

  # update data of the table: "dashboard_session"
  update_dashboard_session(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: dashboard_session_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: dashboard_session_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: dashboard_session_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: dashboard_session_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: dashboard_session_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: dashboard_session_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: dashboard_session_set_input

    # filter the rows which have to be updated
    where: dashboard_session_bool_exp!
  ): dashboard_session_mutation_response

  # update single row of the table: "dashboard_session"
  update_dashboard_session_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: dashboard_session_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: dashboard_session_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: dashboard_session_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: dashboard_session_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: dashboard_session_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: dashboard_session_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: dashboard_session_set_input
    pk_columns: dashboard_session_pk_columns_input!
  ): dashboard_session

  # update multiples rows of table: "dashboard_session"
  update_dashboard_session_many(
    # updates to execute, in order
    updates: [dashboard_session_updates!]!
  ): [dashboard_session_mutation_response]

  # update data of the table: "data_upload_file_summary"
  update_data_upload_file_summary(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: data_upload_file_summary_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: data_upload_file_summary_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: data_upload_file_summary_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: data_upload_file_summary_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: data_upload_file_summary_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: data_upload_file_summary_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: data_upload_file_summary_set_input

    # filter the rows which have to be updated
    where: data_upload_file_summary_bool_exp!
  ): data_upload_file_summary_mutation_response

  # update single row of the table: "data_upload_file_summary"
  update_data_upload_file_summary_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: data_upload_file_summary_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: data_upload_file_summary_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: data_upload_file_summary_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: data_upload_file_summary_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: data_upload_file_summary_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: data_upload_file_summary_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: data_upload_file_summary_set_input
    pk_columns: data_upload_file_summary_pk_columns_input!
  ): data_upload_file_summary

  # update multiples rows of table: "data_upload_file_summary"
  update_data_upload_file_summary_many(
    # updates to execute, in order
    updates: [data_upload_file_summary_updates!]!
  ): [data_upload_file_summary_mutation_response]

  # update data of the table: "dataprep_flow"
  update_dataprep_flow(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: dataprep_flow_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: dataprep_flow_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: dataprep_flow_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: dataprep_flow_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: dataprep_flow_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: dataprep_flow_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: dataprep_flow_set_input

    # filter the rows which have to be updated
    where: dataprep_flow_bool_exp!
  ): dataprep_flow_mutation_response

  # update single row of the table: "dataprep_flow"
  update_dataprep_flow_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: dataprep_flow_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: dataprep_flow_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: dataprep_flow_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: dataprep_flow_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: dataprep_flow_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: dataprep_flow_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: dataprep_flow_set_input
    pk_columns: dataprep_flow_pk_columns_input!
  ): dataprep_flow

  # update multiples rows of table: "dataprep_flow"
  update_dataprep_flow_many(
    # updates to execute, in order
    updates: [dataprep_flow_updates!]!
  ): [dataprep_flow_mutation_response]

  # update data of the table: "dataprep_job"
  update_dataprep_job(
    # increments the numeric columns with given value of the filtered values
    _inc: dataprep_job_inc_input

    # sets the columns of the filtered rows to the given values
    _set: dataprep_job_set_input

    # filter the rows which have to be updated
    where: dataprep_job_bool_exp!
  ): dataprep_job_mutation_response

  # update single row of the table: "dataprep_job"
  update_dataprep_job_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: dataprep_job_inc_input

    # sets the columns of the filtered rows to the given values
    _set: dataprep_job_set_input
    pk_columns: dataprep_job_pk_columns_input!
  ): dataprep_job

  # update multiples rows of table: "dataprep_job"
  update_dataprep_job_many(
    # updates to execute, in order
    updates: [dataprep_job_updates!]!
  ): [dataprep_job_mutation_response]

  # update data of the table: "dimension"
  update_dimension(
    # sets the columns of the filtered rows to the given values
    _set: dimension_set_input

    # filter the rows which have to be updated
    where: dimension_bool_exp!
  ): dimension_mutation_response

  # update single row of the table: "dimension"
  update_dimension_by_pk(
    # sets the columns of the filtered rows to the given values
    _set: dimension_set_input
    pk_columns: dimension_pk_columns_input!
  ): dimension

  # update data of the table: "dimension_category"
  update_dimension_category(
    # sets the columns of the filtered rows to the given values
    _set: dimension_category_set_input

    # filter the rows which have to be updated
    where: dimension_category_bool_exp!
  ): dimension_category_mutation_response

  # update single row of the table: "dimension_category"
  update_dimension_category_by_pk(
    # sets the columns of the filtered rows to the given values
    _set: dimension_category_set_input
    pk_columns: dimension_category_pk_columns_input!
  ): dimension_category

  # update multiples rows of table: "dimension_category"
  update_dimension_category_many(
    # updates to execute, in order
    updates: [dimension_category_updates!]!
  ): [dimension_category_mutation_response]

  # update data of the table: "dimension_category_mapping"
  update_dimension_category_mapping(
    # increments the numeric columns with given value of the filtered values
    _inc: dimension_category_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: dimension_category_mapping_set_input

    # filter the rows which have to be updated
    where: dimension_category_mapping_bool_exp!
  ): dimension_category_mapping_mutation_response

  # update single row of the table: "dimension_category_mapping"
  update_dimension_category_mapping_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: dimension_category_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: dimension_category_mapping_set_input
    pk_columns: dimension_category_mapping_pk_columns_input!
  ): dimension_category_mapping

  # update multiples rows of table: "dimension_category_mapping"
  update_dimension_category_mapping_many(
    # updates to execute, in order
    updates: [dimension_category_mapping_updates!]!
  ): [dimension_category_mapping_mutation_response]

  # update multiples rows of table: "dimension"
  update_dimension_many(
    # updates to execute, in order
    updates: [dimension_updates!]!
  ): [dimension_mutation_response]

  # update data of the table: "external_alert_activity_to_ignore"
  update_external_alert_activity_to_ignore(
    # increments the numeric columns with given value of the filtered values
    _inc: external_alert_activity_to_ignore_inc_input

    # sets the columns of the filtered rows to the given values
    _set: external_alert_activity_to_ignore_set_input

    # filter the rows which have to be updated
    where: external_alert_activity_to_ignore_bool_exp!
  ): external_alert_activity_to_ignore_mutation_response

  # update single row of the table: "external_alert_activity_to_ignore"
  update_external_alert_activity_to_ignore_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: external_alert_activity_to_ignore_inc_input

    # sets the columns of the filtered rows to the given values
    _set: external_alert_activity_to_ignore_set_input
    pk_columns: external_alert_activity_to_ignore_pk_columns_input!
  ): external_alert_activity_to_ignore

  # update multiples rows of table: "external_alert_activity_to_ignore"
  update_external_alert_activity_to_ignore_many(
    # updates to execute, in order
    updates: [external_alert_activity_to_ignore_updates!]!
  ): [external_alert_activity_to_ignore_mutation_response]

  # update data of the table: "external_alert_type"
  update_external_alert_type(
    # increments the numeric columns with given value of the filtered values
    _inc: external_alert_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: external_alert_type_set_input

    # filter the rows which have to be updated
    where: external_alert_type_bool_exp!
  ): external_alert_type_mutation_response

  # update single row of the table: "external_alert_type"
  update_external_alert_type_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: external_alert_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: external_alert_type_set_input
    pk_columns: external_alert_type_pk_columns_input!
  ): external_alert_type

  # update multiples rows of table: "external_alert_type"
  update_external_alert_type_many(
    # updates to execute, in order
    updates: [external_alert_type_updates!]!
  ): [external_alert_type_mutation_response]

  # update data of the table: "feed_update"
  update_feed_update(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: feed_update_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: feed_update_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: feed_update_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: feed_update_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: feed_update_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: feed_update_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: feed_update_set_input

    # filter the rows which have to be updated
    where: feed_update_bool_exp!
  ): feed_update_mutation_response

  # update single row of the table: "feed_update"
  update_feed_update_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: feed_update_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: feed_update_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: feed_update_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: feed_update_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: feed_update_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: feed_update_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: feed_update_set_input
    pk_columns: feed_update_pk_columns_input!
  ): feed_update

  # update multiples rows of table: "feed_update"
  update_feed_update_many(
    # updates to execute, in order
    updates: [feed_update_updates!]!
  ): [feed_update_mutation_response]

  # update data of the table: "feed_update_type"
  update_feed_update_type(
    # increments the numeric columns with given value of the filtered values
    _inc: feed_update_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: feed_update_type_set_input

    # filter the rows which have to be updated
    where: feed_update_type_bool_exp!
  ): feed_update_type_mutation_response

  # update single row of the table: "feed_update_type"
  update_feed_update_type_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: feed_update_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: feed_update_type_set_input
    pk_columns: feed_update_type_pk_columns_input!
  ): feed_update_type

  # update multiples rows of table: "feed_update_type"
  update_feed_update_type_many(
    # updates to execute, in order
    updates: [feed_update_type_updates!]!
  ): [feed_update_type_mutation_response]

  # update data of the table: "field"
  update_field(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: field_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: field_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: field_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: field_delete_key_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: field_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: field_set_input

    # filter the rows which have to be updated
    where: field_bool_exp!
  ): field_mutation_response

  # update single row of the table: "field"
  update_field_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: field_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: field_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: field_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: field_delete_key_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: field_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: field_set_input
    pk_columns: field_pk_columns_input!
  ): field

  # update data of the table: "field_category_mapping"
  update_field_category_mapping(
    # increments the numeric columns with given value of the filtered values
    _inc: field_category_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: field_category_mapping_set_input

    # filter the rows which have to be updated
    where: field_category_mapping_bool_exp!
  ): field_category_mapping_mutation_response

  # update single row of the table: "field_category_mapping"
  update_field_category_mapping_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: field_category_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: field_category_mapping_set_input
    pk_columns: field_category_mapping_pk_columns_input!
  ): field_category_mapping

  # update multiples rows of table: "field_category_mapping"
  update_field_category_mapping_many(
    # updates to execute, in order
    updates: [field_category_mapping_updates!]!
  ): [field_category_mapping_mutation_response]

  # update data of the table: "field_dimension_mapping"
  update_field_dimension_mapping(
    # increments the numeric columns with given value of the filtered values
    _inc: field_dimension_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: field_dimension_mapping_set_input

    # filter the rows which have to be updated
    where: field_dimension_mapping_bool_exp!
  ): field_dimension_mapping_mutation_response

  # update single row of the table: "field_dimension_mapping"
  update_field_dimension_mapping_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: field_dimension_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: field_dimension_mapping_set_input
    pk_columns: field_dimension_mapping_pk_columns_input!
  ): field_dimension_mapping

  # update multiples rows of table: "field_dimension_mapping"
  update_field_dimension_mapping_many(
    # updates to execute, in order
    updates: [field_dimension_mapping_updates!]!
  ): [field_dimension_mapping_mutation_response]

  # update multiples rows of table: "field"
  update_field_many(
    # updates to execute, in order
    updates: [field_updates!]!
  ): [field_mutation_response]

  # update data of the table: "field_pipeline_datasource_mapping"
  update_field_pipeline_datasource_mapping(
    # increments the numeric columns with given value of the filtered values
    _inc: field_pipeline_datasource_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: field_pipeline_datasource_mapping_set_input

    # filter the rows which have to be updated
    where: field_pipeline_datasource_mapping_bool_exp!
  ): field_pipeline_datasource_mapping_mutation_response

  # update single row of the table: "field_pipeline_datasource_mapping"
  update_field_pipeline_datasource_mapping_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: field_pipeline_datasource_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: field_pipeline_datasource_mapping_set_input
    pk_columns: field_pipeline_datasource_mapping_pk_columns_input!
  ): field_pipeline_datasource_mapping

  # update multiples rows of table: "field_pipeline_datasource_mapping"
  update_field_pipeline_datasource_mapping_many(
    # updates to execute, in order
    updates: [field_pipeline_datasource_mapping_updates!]!
  ): [field_pipeline_datasource_mapping_mutation_response]

  # update data of the table: "geo_dimension_metadata"
  update_geo_dimension_metadata(
    # sets the columns of the filtered rows to the given values
    _set: geo_dimension_metadata_set_input

    # filter the rows which have to be updated
    where: geo_dimension_metadata_bool_exp!
  ): geo_dimension_metadata_mutation_response

  # update single row of the table: "geo_dimension_metadata"
  update_geo_dimension_metadata_by_pk(
    # sets the columns of the filtered rows to the given values
    _set: geo_dimension_metadata_set_input
    pk_columns: geo_dimension_metadata_pk_columns_input!
  ): geo_dimension_metadata

  # update multiples rows of table: "geo_dimension_metadata"
  update_geo_dimension_metadata_many(
    # updates to execute, in order
    updates: [geo_dimension_metadata_updates!]!
  ): [geo_dimension_metadata_mutation_response]

  # update data of the table: "hierarchical_dimension_metadata"
  update_hierarchical_dimension_metadata(
    # increments the numeric columns with given value of the filtered values
    _inc: hierarchical_dimension_metadata_inc_input

    # sets the columns of the filtered rows to the given values
    _set: hierarchical_dimension_metadata_set_input

    # filter the rows which have to be updated
    where: hierarchical_dimension_metadata_bool_exp!
  ): hierarchical_dimension_metadata_mutation_response

  # update single row of the table: "hierarchical_dimension_metadata"
  update_hierarchical_dimension_metadata_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: hierarchical_dimension_metadata_inc_input

    # sets the columns of the filtered rows to the given values
    _set: hierarchical_dimension_metadata_set_input
    pk_columns: hierarchical_dimension_metadata_pk_columns_input!
  ): hierarchical_dimension_metadata

  # update multiples rows of table: "hierarchical_dimension_metadata"
  update_hierarchical_dimension_metadata_many(
    # updates to execute, in order
    updates: [hierarchical_dimension_metadata_updates!]!
  ): [hierarchical_dimension_metadata_mutation_response]

  # update data of the table: "non_hierarchical_dimension"
  update_non_hierarchical_dimension(
    # sets the columns of the filtered rows to the given values
    _set: non_hierarchical_dimension_set_input

    # filter the rows which have to be updated
    where: non_hierarchical_dimension_bool_exp!
  ): non_hierarchical_dimension_mutation_response

  # update single row of the table: "non_hierarchical_dimension"
  update_non_hierarchical_dimension_by_pk(
    # sets the columns of the filtered rows to the given values
    _set: non_hierarchical_dimension_set_input
    pk_columns: non_hierarchical_dimension_pk_columns_input!
  ): non_hierarchical_dimension

  # update multiples rows of table: "non_hierarchical_dimension"
  update_non_hierarchical_dimension_many(
    # updates to execute, in order
    updates: [non_hierarchical_dimension_updates!]!
  ): [non_hierarchical_dimension_mutation_response]

  # update data of the table: "pipeline_datasource"
  update_pipeline_datasource(
    # sets the columns of the filtered rows to the given values
    _set: pipeline_datasource_set_input

    # filter the rows which have to be updated
    where: pipeline_datasource_bool_exp!
  ): pipeline_datasource_mutation_response

  # update single row of the table: "pipeline_datasource"
  update_pipeline_datasource_by_pk(
    # sets the columns of the filtered rows to the given values
    _set: pipeline_datasource_set_input
    pk_columns: pipeline_datasource_pk_columns_input!
  ): pipeline_datasource

  # update multiples rows of table: "pipeline_datasource"
  update_pipeline_datasource_many(
    # updates to execute, in order
    updates: [pipeline_datasource_updates!]!
  ): [pipeline_datasource_mutation_response]

  # update data of the table: "pipeline_entity_match"
  update_pipeline_entity_match(
    # increments the numeric columns with given value of the filtered values
    _inc: pipeline_entity_match_inc_input

    # sets the columns of the filtered rows to the given values
    _set: pipeline_entity_match_set_input

    # filter the rows which have to be updated
    where: pipeline_entity_match_bool_exp!
  ): pipeline_entity_match_mutation_response

  # update single row of the table: "pipeline_entity_match"
  update_pipeline_entity_match_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: pipeline_entity_match_inc_input

    # sets the columns of the filtered rows to the given values
    _set: pipeline_entity_match_set_input
    pk_columns: pipeline_entity_match_pk_columns_input!
  ): pipeline_entity_match

  # update multiples rows of table: "pipeline_entity_match"
  update_pipeline_entity_match_many(
    # updates to execute, in order
    updates: [pipeline_entity_match_updates!]!
  ): [pipeline_entity_match_mutation_response]

  # update data of the table: "pipeline_entity_type"
  update_pipeline_entity_type(
    # increments the numeric columns with given value of the filtered values
    _inc: pipeline_entity_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: pipeline_entity_type_set_input

    # filter the rows which have to be updated
    where: pipeline_entity_type_bool_exp!
  ): pipeline_entity_type_mutation_response

  # update single row of the table: "pipeline_entity_type"
  update_pipeline_entity_type_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: pipeline_entity_type_inc_input

    # sets the columns of the filtered rows to the given values
    _set: pipeline_entity_type_set_input
    pk_columns: pipeline_entity_type_pk_columns_input!
  ): pipeline_entity_type

  # update multiples rows of table: "pipeline_entity_type"
  update_pipeline_entity_type_many(
    # updates to execute, in order
    updates: [pipeline_entity_type_updates!]!
  ): [pipeline_entity_type_mutation_response]

  # update data of the table: "pipeline_run_metadata"
  update_pipeline_run_metadata(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: pipeline_run_metadata_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: pipeline_run_metadata_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: pipeline_run_metadata_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: pipeline_run_metadata_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: pipeline_run_metadata_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: pipeline_run_metadata_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: pipeline_run_metadata_set_input

    # filter the rows which have to be updated
    where: pipeline_run_metadata_bool_exp!
  ): pipeline_run_metadata_mutation_response

  # update single row of the table: "pipeline_run_metadata"
  update_pipeline_run_metadata_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: pipeline_run_metadata_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: pipeline_run_metadata_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: pipeline_run_metadata_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: pipeline_run_metadata_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: pipeline_run_metadata_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: pipeline_run_metadata_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: pipeline_run_metadata_set_input
    pk_columns: pipeline_run_metadata_pk_columns_input!
  ): pipeline_run_metadata

  # update multiples rows of table: "pipeline_run_metadata"
  update_pipeline_run_metadata_many(
    # updates to execute, in order
    updates: [pipeline_run_metadata_updates!]!
  ): [pipeline_run_metadata_mutation_response]

  # update data of the table: "raw_pipeline_entity"
  update_raw_pipeline_entity(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: raw_pipeline_entity_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: raw_pipeline_entity_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: raw_pipeline_entity_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: raw_pipeline_entity_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: raw_pipeline_entity_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: raw_pipeline_entity_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: raw_pipeline_entity_set_input

    # filter the rows which have to be updated
    where: raw_pipeline_entity_bool_exp!
  ): raw_pipeline_entity_mutation_response

  # update single row of the table: "raw_pipeline_entity"
  update_raw_pipeline_entity_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: raw_pipeline_entity_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: raw_pipeline_entity_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: raw_pipeline_entity_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: raw_pipeline_entity_delete_key_input

    # increments the numeric columns with given value of the filtered values
    _inc: raw_pipeline_entity_inc_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: raw_pipeline_entity_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: raw_pipeline_entity_set_input
    pk_columns: raw_pipeline_entity_pk_columns_input!
  ): raw_pipeline_entity

  # update multiples rows of table: "raw_pipeline_entity"
  update_raw_pipeline_entity_many(
    # updates to execute, in order
    updates: [raw_pipeline_entity_updates!]!
  ): [raw_pipeline_entity_mutation_response]

  # update data of the table: "self_serve_source"
  update_self_serve_source(
    # increments the numeric columns with given value of the filtered values
    _inc: self_serve_source_inc_input

    # sets the columns of the filtered rows to the given values
    _set: self_serve_source_set_input

    # filter the rows which have to be updated
    where: self_serve_source_bool_exp!
  ): self_serve_source_mutation_response

  # update single row of the table: "self_serve_source"
  update_self_serve_source_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: self_serve_source_inc_input

    # sets the columns of the filtered rows to the given values
    _set: self_serve_source_set_input
    pk_columns: self_serve_source_pk_columns_input!
  ): self_serve_source

  # update multiples rows of table: "self_serve_source"
  update_self_serve_source_many(
    # updates to execute, in order
    updates: [self_serve_source_updates!]!
  ): [self_serve_source_mutation_response]

  # update data of the table: "unpublished_field"
  update_unpublished_field(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: unpublished_field_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: unpublished_field_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: unpublished_field_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: unpublished_field_delete_key_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: unpublished_field_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: unpublished_field_set_input

    # filter the rows which have to be updated
    where: unpublished_field_bool_exp!
  ): unpublished_field_mutation_response

  # update single row of the table: "unpublished_field"
  update_unpublished_field_by_pk(
    # append existing jsonb value of filtered columns with new jsonb value
    _append: unpublished_field_append_input

    # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    _delete_at_path: unpublished_field_delete_at_path_input

    # delete the array element with specified index (negative integers count from
    # the end). throws an error if top level container is not an array
    _delete_elem: unpublished_field_delete_elem_input

    # delete key/value pair or string element. key/value pairs are matched based on their key value
    _delete_key: unpublished_field_delete_key_input

    # prepend existing jsonb value of filtered columns with new jsonb value
    _prepend: unpublished_field_prepend_input

    # sets the columns of the filtered rows to the given values
    _set: unpublished_field_set_input
    pk_columns: unpublished_field_pk_columns_input!
  ): unpublished_field

  # update data of the table: "unpublished_field_category_mapping"
  update_unpublished_field_category_mapping(
    # increments the numeric columns with given value of the filtered values
    _inc: unpublished_field_category_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: unpublished_field_category_mapping_set_input

    # filter the rows which have to be updated
    where: unpublished_field_category_mapping_bool_exp!
  ): unpublished_field_category_mapping_mutation_response

  # update single row of the table: "unpublished_field_category_mapping"
  update_unpublished_field_category_mapping_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: unpublished_field_category_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: unpublished_field_category_mapping_set_input
    pk_columns: unpublished_field_category_mapping_pk_columns_input!
  ): unpublished_field_category_mapping

  # update multiples rows of table: "unpublished_field_category_mapping"
  update_unpublished_field_category_mapping_many(
    # updates to execute, in order
    updates: [unpublished_field_category_mapping_updates!]!
  ): [unpublished_field_category_mapping_mutation_response]

  # update data of the table: "unpublished_field_dimension_mapping"
  update_unpublished_field_dimension_mapping(
    # increments the numeric columns with given value of the filtered values
    _inc: unpublished_field_dimension_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: unpublished_field_dimension_mapping_set_input

    # filter the rows which have to be updated
    where: unpublished_field_dimension_mapping_bool_exp!
  ): unpublished_field_dimension_mapping_mutation_response

  # update single row of the table: "unpublished_field_dimension_mapping"
  update_unpublished_field_dimension_mapping_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: unpublished_field_dimension_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: unpublished_field_dimension_mapping_set_input
    pk_columns: unpublished_field_dimension_mapping_pk_columns_input!
  ): unpublished_field_dimension_mapping

  # update multiples rows of table: "unpublished_field_dimension_mapping"
  update_unpublished_field_dimension_mapping_many(
    # updates to execute, in order
    updates: [unpublished_field_dimension_mapping_updates!]!
  ): [unpublished_field_dimension_mapping_mutation_response]

  # update multiples rows of table: "unpublished_field"
  update_unpublished_field_many(
    # updates to execute, in order
    updates: [unpublished_field_updates!]!
  ): [unpublished_field_mutation_response]

  # update data of the table: "unpublished_field_pipeline_datasource_mapping"
  update_unpublished_field_pipeline_datasource_mapping(
    # increments the numeric columns with given value of the filtered values
    _inc: unpublished_field_pipeline_datasource_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: unpublished_field_pipeline_datasource_mapping_set_input

    # filter the rows which have to be updated
    where: unpublished_field_pipeline_datasource_mapping_bool_exp!
  ): unpublished_field_pipeline_datasource_mapping_mutation_response

  # update single row of the table: "unpublished_field_pipeline_datasource_mapping"
  update_unpublished_field_pipeline_datasource_mapping_by_pk(
    # increments the numeric columns with given value of the filtered values
    _inc: unpublished_field_pipeline_datasource_mapping_inc_input

    # sets the columns of the filtered rows to the given values
    _set: unpublished_field_pipeline_datasource_mapping_set_input
    pk_columns: unpublished_field_pipeline_datasource_mapping_pk_columns_input!
  ): unpublished_field_pipeline_datasource_mapping

  # update multiples rows of table: "unpublished_field_pipeline_datasource_mapping"
  update_unpublished_field_pipeline_datasource_mapping_many(
    # updates to execute, in order
    updates: [unpublished_field_pipeline_datasource_mapping_updates!]!
  ): [unpublished_field_pipeline_datasource_mapping_mutation_response]
}

# An object with globally unique ID
interface Node {
  # A globally unique identifier
  id: ID!
}

# columns and relationships of "non_hierarchical_dimension"
type non_hierarchical_dimension implements Node {
  # An object relationship
  dimension: dimension!
  id: ID!
}

# aggregated selection of "non_hierarchical_dimension"
type non_hierarchical_dimension_aggregate {
  aggregate: non_hierarchical_dimension_aggregate_fields
  nodes: [non_hierarchical_dimension!]!
}

# aggregate fields of "non_hierarchical_dimension"
type non_hierarchical_dimension_aggregate_fields {
  count(columns: [non_hierarchical_dimension_select_column!], distinct: Boolean): Int!
  max: non_hierarchical_dimension_max_fields
  min: non_hierarchical_dimension_min_fields
}

# order by aggregate values of table "non_hierarchical_dimension"
input non_hierarchical_dimension_aggregate_order_by {
  count: order_by
  max: non_hierarchical_dimension_max_order_by
  min: non_hierarchical_dimension_min_order_by
}

# input type for inserting array relation for remote table "non_hierarchical_dimension"
input non_hierarchical_dimension_arr_rel_insert_input {
  data: [non_hierarchical_dimension_insert_input!]!

  # upsert condition
  on_conflict: non_hierarchical_dimension_on_conflict
}

# Boolean expression to filter rows from the table "non_hierarchical_dimension". All fields are combined with a logical 'AND'.
input non_hierarchical_dimension_bool_exp {
  _and: [non_hierarchical_dimension_bool_exp!]
  _not: non_hierarchical_dimension_bool_exp
  _or: [non_hierarchical_dimension_bool_exp!]
  dimension: dimension_bool_exp
  id: String_comparison_exp
}

# unique or primary key constraints on table "non_hierarchical_dimension"
enum non_hierarchical_dimension_constraint {
  # unique or primary key constraint on columns "id"
  non_hierarchical_dimension_pkey
}

# input type for inserting data into table "non_hierarchical_dimension"
input non_hierarchical_dimension_insert_input {
  dimension: dimension_obj_rel_insert_input
  id: String
}

# aggregate max on columns
type non_hierarchical_dimension_max_fields {
  id: String
}

# order by max() on columns of table "non_hierarchical_dimension"
input non_hierarchical_dimension_max_order_by {
  id: order_by
}

# aggregate min on columns
type non_hierarchical_dimension_min_fields {
  id: String
}

# order by min() on columns of table "non_hierarchical_dimension"
input non_hierarchical_dimension_min_order_by {
  id: order_by
}

# response of any mutation on the table "non_hierarchical_dimension"
type non_hierarchical_dimension_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [non_hierarchical_dimension!]!
}

# on_conflict condition type for table "non_hierarchical_dimension"
input non_hierarchical_dimension_on_conflict {
  constraint: non_hierarchical_dimension_constraint!
  update_columns: [non_hierarchical_dimension_update_column!]! = []
  where: non_hierarchical_dimension_bool_exp
}

# Ordering options when selecting data from "non_hierarchical_dimension".
input non_hierarchical_dimension_order_by {
  dimension: dimension_order_by
  id: order_by
}

# primary key columns input for table: non_hierarchical_dimension
input non_hierarchical_dimension_pk_columns_input {
  id: String!
}

# select columns of table "non_hierarchical_dimension"
enum non_hierarchical_dimension_select_column {
  # column name
  id
}

# input type for updating data in table "non_hierarchical_dimension"
input non_hierarchical_dimension_set_input {
  id: String
}

# update columns of table "non_hierarchical_dimension"
enum non_hierarchical_dimension_update_column {
  # column name
  id
}

input non_hierarchical_dimension_updates {
  # sets the columns of the filtered rows to the given values
  _set: non_hierarchical_dimension_set_input
  where: non_hierarchical_dimension_bool_exp!
}

# A Relay connection object on "non_hierarchical_dimension"
type non_hierarchical_dimensionConnection {
  edges: [non_hierarchical_dimensionEdge!]!
  pageInfo: PageInfo!
}

type non_hierarchical_dimensionEdge {
  cursor: String!
  node: non_hierarchical_dimension!
}

# column ordering options
enum order_by {
  # in ascending order, nulls last
  asc

  # in ascending order, nulls first
  asc_nulls_first

  # in ascending order, nulls last
  asc_nulls_last

  # in descending order, nulls first
  desc

  # in descending order, nulls first
  desc_nulls_first

  # in descending order, nulls last
  desc_nulls_last
}

type PageInfo {
  endCursor: String!
  hasNextPage: Boolean!
  hasPreviousPage: Boolean!
  startCursor: String!
}

# columns and relationships of "pipeline_datasource"
type pipeline_datasource implements Node {
  created: timestamp!

  # An array relationship
  field_pipeline_datasource_mappings(
    # distinct select on columns
    distinct_on: [field_pipeline_datasource_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: field_pipeline_datasource_mapping_bool_exp
  ): [field_pipeline_datasource_mapping!]!

  # An aggregate relationship
  field_pipeline_datasource_mappings_aggregate(
    # distinct select on columns
    distinct_on: [field_pipeline_datasource_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: field_pipeline_datasource_mapping_bool_exp
  ): field_pipeline_datasource_mapping_aggregate!

  # An array relationship connection
  field_pipeline_datasource_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_pipeline_datasource_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: field_pipeline_datasource_mapping_bool_exp
  ): field_pipeline_datasource_mappingConnection!
  id: ID!
  last_modified: timestamp!
  name: String!

  # An array relationship
  self_serve_sources(
    # distinct select on columns
    distinct_on: [self_serve_source_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [self_serve_source_order_by!]

    # filter the rows returned
    where: self_serve_source_bool_exp
  ): [self_serve_source!]!

  # An aggregate relationship
  self_serve_sources_aggregate(
    # distinct select on columns
    distinct_on: [self_serve_source_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [self_serve_source_order_by!]

    # filter the rows returned
    where: self_serve_source_bool_exp
  ): self_serve_source_aggregate!

  # An array relationship connection
  self_serve_sources_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [self_serve_source_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [self_serve_source_order_by!]

    # filter the rows returned
    where: self_serve_source_bool_exp
  ): self_serve_sourceConnection!

  # An array relationship
  unpublished_field_pipeline_datasource_mappings(
    # distinct select on columns
    distinct_on: [unpublished_field_pipeline_datasource_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_pipeline_datasource_mapping_bool_exp
  ): [unpublished_field_pipeline_datasource_mapping!]!

  # An aggregate relationship
  unpublished_field_pipeline_datasource_mappings_aggregate(
    # distinct select on columns
    distinct_on: [unpublished_field_pipeline_datasource_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_pipeline_datasource_mapping_bool_exp
  ): unpublished_field_pipeline_datasource_mapping_aggregate!

  # An array relationship connection
  unpublished_field_pipeline_datasource_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_pipeline_datasource_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_pipeline_datasource_mapping_bool_exp
  ): unpublished_field_pipeline_datasource_mappingConnection!
}

# Boolean expression to filter rows from the table "pipeline_datasource". All fields are combined with a logical 'AND'.
input pipeline_datasource_bool_exp {
  _and: [pipeline_datasource_bool_exp!]
  _not: pipeline_datasource_bool_exp
  _or: [pipeline_datasource_bool_exp!]
  created: timestamp_comparison_exp
  field_pipeline_datasource_mappings: field_pipeline_datasource_mapping_bool_exp
  id: String_comparison_exp
  last_modified: timestamp_comparison_exp
  name: String_comparison_exp
  self_serve_sources: self_serve_source_bool_exp
  unpublished_field_pipeline_datasource_mappings: unpublished_field_pipeline_datasource_mapping_bool_exp
}

# unique or primary key constraints on table "pipeline_datasource"
enum pipeline_datasource_constraint {
  # unique or primary key constraint on columns "id"
  pipeline_datasource_pkey
}

# input type for inserting data into table "pipeline_datasource"
input pipeline_datasource_insert_input {
  created: timestamp
  field_pipeline_datasource_mappings: field_pipeline_datasource_mapping_arr_rel_insert_input
  id: String
  last_modified: timestamp
  name: String
  self_serve_sources: self_serve_source_arr_rel_insert_input
  unpublished_field_pipeline_datasource_mappings: unpublished_field_pipeline_datasource_mapping_arr_rel_insert_input
}

# response of any mutation on the table "pipeline_datasource"
type pipeline_datasource_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [pipeline_datasource!]!
}

# input type for inserting object relation for remote table "pipeline_datasource"
input pipeline_datasource_obj_rel_insert_input {
  data: pipeline_datasource_insert_input!

  # upsert condition
  on_conflict: pipeline_datasource_on_conflict
}

# on_conflict condition type for table "pipeline_datasource"
input pipeline_datasource_on_conflict {
  constraint: pipeline_datasource_constraint!
  update_columns: [pipeline_datasource_update_column!]! = []
  where: pipeline_datasource_bool_exp
}

# Ordering options when selecting data from "pipeline_datasource".
input pipeline_datasource_order_by {
  created: order_by
  field_pipeline_datasource_mappings_aggregate: field_pipeline_datasource_mapping_aggregate_order_by
  id: order_by
  last_modified: order_by
  name: order_by
  self_serve_sources_aggregate: self_serve_source_aggregate_order_by
  unpublished_field_pipeline_datasource_mappings_aggregate: unpublished_field_pipeline_datasource_mapping_aggregate_order_by
}

# primary key columns input for table: pipeline_datasource
input pipeline_datasource_pk_columns_input {
  id: String!
}

# select columns of table "pipeline_datasource"
enum pipeline_datasource_select_column {
  # column name
  created

  # column name
  id

  # column name
  last_modified

  # column name
  name
}

# input type for updating data in table "pipeline_datasource"
input pipeline_datasource_set_input {
  created: timestamp
  id: String
  last_modified: timestamp
  name: String
}

# update columns of table "pipeline_datasource"
enum pipeline_datasource_update_column {
  # column name
  created

  # column name
  id

  # column name
  last_modified

  # column name
  name
}

input pipeline_datasource_updates {
  # sets the columns of the filtered rows to the given values
  _set: pipeline_datasource_set_input
  where: pipeline_datasource_bool_exp!
}

# A Relay connection object on "pipeline_datasource"
type pipeline_datasourceConnection {
  edges: [pipeline_datasourceEdge!]!
  pageInfo: PageInfo!
}

type pipeline_datasourceEdge {
  cursor: String!
  node: pipeline_datasource!
}

# columns and relationships of "pipeline_entity_match"
type pipeline_entity_match implements Node {
  canonical_entity_id: Int!

  # An object relationship
  canonical_pipeline_entity: canonical_pipeline_entity!
  date_changed: timestamp!
  id: ID!
  raw_entity_id: Int!

  # An object relationship
  raw_pipeline_entity: raw_pipeline_entity!
  self_match: Boolean!
  user_id: Int
  validated_status: match_status_enum!
}

# aggregated selection of "pipeline_entity_match"
type pipeline_entity_match_aggregate {
  aggregate: pipeline_entity_match_aggregate_fields
  nodes: [pipeline_entity_match!]!
}

# aggregate fields of "pipeline_entity_match"
type pipeline_entity_match_aggregate_fields {
  avg: pipeline_entity_match_avg_fields
  count(columns: [pipeline_entity_match_select_column!], distinct: Boolean): Int!
  max: pipeline_entity_match_max_fields
  min: pipeline_entity_match_min_fields
  stddev: pipeline_entity_match_stddev_fields
  stddev_pop: pipeline_entity_match_stddev_pop_fields
  stddev_samp: pipeline_entity_match_stddev_samp_fields
  sum: pipeline_entity_match_sum_fields
  var_pop: pipeline_entity_match_var_pop_fields
  var_samp: pipeline_entity_match_var_samp_fields
  variance: pipeline_entity_match_variance_fields
}

# order by aggregate values of table "pipeline_entity_match"
input pipeline_entity_match_aggregate_order_by {
  avg: pipeline_entity_match_avg_order_by
  count: order_by
  max: pipeline_entity_match_max_order_by
  min: pipeline_entity_match_min_order_by
  stddev: pipeline_entity_match_stddev_order_by
  stddev_pop: pipeline_entity_match_stddev_pop_order_by
  stddev_samp: pipeline_entity_match_stddev_samp_order_by
  sum: pipeline_entity_match_sum_order_by
  var_pop: pipeline_entity_match_var_pop_order_by
  var_samp: pipeline_entity_match_var_samp_order_by
  variance: pipeline_entity_match_variance_order_by
}

# input type for inserting array relation for remote table "pipeline_entity_match"
input pipeline_entity_match_arr_rel_insert_input {
  data: [pipeline_entity_match_insert_input!]!

  # upsert condition
  on_conflict: pipeline_entity_match_on_conflict
}

# aggregate avg on columns
type pipeline_entity_match_avg_fields {
  canonical_entity_id: Float
  id: Float
  raw_entity_id: Float
  user_id: Float
}

# order by avg() on columns of table "pipeline_entity_match"
input pipeline_entity_match_avg_order_by {
  canonical_entity_id: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
}

# Boolean expression to filter rows from the table "pipeline_entity_match". All fields are combined with a logical 'AND'.
input pipeline_entity_match_bool_exp {
  _and: [pipeline_entity_match_bool_exp!]
  _not: pipeline_entity_match_bool_exp
  _or: [pipeline_entity_match_bool_exp!]
  canonical_entity_id: Int_comparison_exp
  canonical_pipeline_entity: canonical_pipeline_entity_bool_exp
  date_changed: timestamp_comparison_exp
  id: Int_comparison_exp
  raw_entity_id: Int_comparison_exp
  raw_pipeline_entity: raw_pipeline_entity_bool_exp
  self_match: Boolean_comparison_exp
  user_id: Int_comparison_exp
  validated_status: match_status_enum_comparison_exp
}

# unique or primary key constraints on table "pipeline_entity_match"
enum pipeline_entity_match_constraint {
  # unique or primary key constraint on columns "id"
  pipeline_entity_match_pkey
}

# input type for incrementing numeric columns in table "pipeline_entity_match"
input pipeline_entity_match_inc_input {
  canonical_entity_id: Int
  id: Int
  raw_entity_id: Int
  user_id: Int
}

# input type for inserting data into table "pipeline_entity_match"
input pipeline_entity_match_insert_input {
  canonical_entity_id: Int
  canonical_pipeline_entity: canonical_pipeline_entity_obj_rel_insert_input
  date_changed: timestamp
  id: Int
  raw_entity_id: Int
  raw_pipeline_entity: raw_pipeline_entity_obj_rel_insert_input
  self_match: Boolean
  user_id: Int
  validated_status: match_status_enum
}

# aggregate max on columns
type pipeline_entity_match_max_fields {
  canonical_entity_id: Int
  date_changed: timestamp
  id: Int
  raw_entity_id: Int
  user_id: Int
  validated_status: match_status_enum
}

# order by max() on columns of table "pipeline_entity_match"
input pipeline_entity_match_max_order_by {
  canonical_entity_id: order_by
  date_changed: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
  validated_status: order_by
}

# aggregate min on columns
type pipeline_entity_match_min_fields {
  canonical_entity_id: Int
  date_changed: timestamp
  id: Int
  raw_entity_id: Int
  user_id: Int
  validated_status: match_status_enum
}

# order by min() on columns of table "pipeline_entity_match"
input pipeline_entity_match_min_order_by {
  canonical_entity_id: order_by
  date_changed: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
  validated_status: order_by
}

# response of any mutation on the table "pipeline_entity_match"
type pipeline_entity_match_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [pipeline_entity_match!]!
}

# input type for inserting object relation for remote table "pipeline_entity_match"
input pipeline_entity_match_obj_rel_insert_input {
  data: pipeline_entity_match_insert_input!

  # upsert condition
  on_conflict: pipeline_entity_match_on_conflict
}

# on_conflict condition type for table "pipeline_entity_match"
input pipeline_entity_match_on_conflict {
  constraint: pipeline_entity_match_constraint!
  update_columns: [pipeline_entity_match_update_column!]! = []
  where: pipeline_entity_match_bool_exp
}

# Ordering options when selecting data from "pipeline_entity_match".
input pipeline_entity_match_order_by {
  canonical_entity_id: order_by
  canonical_pipeline_entity: canonical_pipeline_entity_order_by
  date_changed: order_by
  id: order_by
  raw_entity_id: order_by
  raw_pipeline_entity: raw_pipeline_entity_order_by
  self_match: order_by
  user_id: order_by
  validated_status: order_by
}

# primary key columns input for table: pipeline_entity_match
input pipeline_entity_match_pk_columns_input {
  id: Int!
}

# select columns of table "pipeline_entity_match"
enum pipeline_entity_match_select_column {
  # column name
  canonical_entity_id

  # column name
  date_changed

  # column name
  id

  # column name
  raw_entity_id

  # column name
  self_match

  # column name
  user_id

  # column name
  validated_status
}

# input type for updating data in table "pipeline_entity_match"
input pipeline_entity_match_set_input {
  canonical_entity_id: Int
  date_changed: timestamp
  id: Int
  raw_entity_id: Int
  self_match: Boolean
  user_id: Int
  validated_status: match_status_enum
}

# aggregate stddev on columns
type pipeline_entity_match_stddev_fields {
  canonical_entity_id: Float
  id: Float
  raw_entity_id: Float
  user_id: Float
}

# order by stddev() on columns of table "pipeline_entity_match"
input pipeline_entity_match_stddev_order_by {
  canonical_entity_id: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
}

# aggregate stddev_pop on columns
type pipeline_entity_match_stddev_pop_fields {
  canonical_entity_id: Float
  id: Float
  raw_entity_id: Float
  user_id: Float
}

# order by stddev_pop() on columns of table "pipeline_entity_match"
input pipeline_entity_match_stddev_pop_order_by {
  canonical_entity_id: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
}

# aggregate stddev_samp on columns
type pipeline_entity_match_stddev_samp_fields {
  canonical_entity_id: Float
  id: Float
  raw_entity_id: Float
  user_id: Float
}

# order by stddev_samp() on columns of table "pipeline_entity_match"
input pipeline_entity_match_stddev_samp_order_by {
  canonical_entity_id: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
}

# aggregate sum on columns
type pipeline_entity_match_sum_fields {
  canonical_entity_id: Int
  id: Int
  raw_entity_id: Int
  user_id: Int
}

# order by sum() on columns of table "pipeline_entity_match"
input pipeline_entity_match_sum_order_by {
  canonical_entity_id: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
}

# update columns of table "pipeline_entity_match"
enum pipeline_entity_match_update_column {
  # column name
  canonical_entity_id

  # column name
  date_changed

  # column name
  id

  # column name
  raw_entity_id

  # column name
  self_match

  # column name
  user_id

  # column name
  validated_status
}

input pipeline_entity_match_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: pipeline_entity_match_inc_input

  # sets the columns of the filtered rows to the given values
  _set: pipeline_entity_match_set_input
  where: pipeline_entity_match_bool_exp!
}

# aggregate var_pop on columns
type pipeline_entity_match_var_pop_fields {
  canonical_entity_id: Float
  id: Float
  raw_entity_id: Float
  user_id: Float
}

# order by var_pop() on columns of table "pipeline_entity_match"
input pipeline_entity_match_var_pop_order_by {
  canonical_entity_id: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
}

# aggregate var_samp on columns
type pipeline_entity_match_var_samp_fields {
  canonical_entity_id: Float
  id: Float
  raw_entity_id: Float
  user_id: Float
}

# order by var_samp() on columns of table "pipeline_entity_match"
input pipeline_entity_match_var_samp_order_by {
  canonical_entity_id: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
}

# aggregate variance on columns
type pipeline_entity_match_variance_fields {
  canonical_entity_id: Float
  id: Float
  raw_entity_id: Float
  user_id: Float
}

# order by variance() on columns of table "pipeline_entity_match"
input pipeline_entity_match_variance_order_by {
  canonical_entity_id: order_by
  id: order_by
  raw_entity_id: order_by
  user_id: order_by
}

# A Relay connection object on "pipeline_entity_match"
type pipeline_entity_matchConnection {
  edges: [pipeline_entity_matchEdge!]!
  pageInfo: PageInfo!
}

type pipeline_entity_matchEdge {
  cursor: String!
  node: pipeline_entity_match!
}

# columns and relationships of "pipeline_entity_type"
type pipeline_entity_type implements Node {
  # An array relationship
  canonical_pipeline_entities(
    # distinct select on columns
    distinct_on: [canonical_pipeline_entity_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [canonical_pipeline_entity_order_by!]

    # filter the rows returned
    where: canonical_pipeline_entity_bool_exp
  ): [canonical_pipeline_entity!]!

  # An aggregate relationship
  canonical_pipeline_entities_aggregate(
    # distinct select on columns
    distinct_on: [canonical_pipeline_entity_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [canonical_pipeline_entity_order_by!]

    # filter the rows returned
    where: canonical_pipeline_entity_bool_exp
  ): canonical_pipeline_entity_aggregate!

  # An array relationship connection
  canonical_pipeline_entities_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [canonical_pipeline_entity_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [canonical_pipeline_entity_order_by!]

    # filter the rows returned
    where: canonical_pipeline_entity_bool_exp
  ): canonical_pipeline_entityConnection!
  description: String
  id: ID!
  name: String!

  # An array relationship
  raw_pipeline_entities(
    # distinct select on columns
    distinct_on: [raw_pipeline_entity_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [raw_pipeline_entity_order_by!]

    # filter the rows returned
    where: raw_pipeline_entity_bool_exp
  ): [raw_pipeline_entity!]!

  # An aggregate relationship
  raw_pipeline_entities_aggregate(
    # distinct select on columns
    distinct_on: [raw_pipeline_entity_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [raw_pipeline_entity_order_by!]

    # filter the rows returned
    where: raw_pipeline_entity_bool_exp
  ): raw_pipeline_entity_aggregate!

  # An array relationship connection
  raw_pipeline_entities_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [raw_pipeline_entity_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [raw_pipeline_entity_order_by!]

    # filter the rows returned
    where: raw_pipeline_entity_bool_exp
  ): raw_pipeline_entityConnection!
}

# Boolean expression to filter rows from the table "pipeline_entity_type". All fields are combined with a logical 'AND'.
input pipeline_entity_type_bool_exp {
  _and: [pipeline_entity_type_bool_exp!]
  _not: pipeline_entity_type_bool_exp
  _or: [pipeline_entity_type_bool_exp!]
  canonical_pipeline_entities: canonical_pipeline_entity_bool_exp
  description: String_comparison_exp
  id: Int_comparison_exp
  name: String_comparison_exp
  raw_pipeline_entities: raw_pipeline_entity_bool_exp
}

# unique or primary key constraints on table "pipeline_entity_type"
enum pipeline_entity_type_constraint {
  # unique or primary key constraint on columns "name"
  pipeline_entity_type_name_key

  # unique or primary key constraint on columns "id"
  pipeline_entity_type_pkey
}

# input type for incrementing numeric columns in table "pipeline_entity_type"
input pipeline_entity_type_inc_input {
  id: Int
}

# input type for inserting data into table "pipeline_entity_type"
input pipeline_entity_type_insert_input {
  canonical_pipeline_entities: canonical_pipeline_entity_arr_rel_insert_input
  description: String
  id: Int
  name: String
  raw_pipeline_entities: raw_pipeline_entity_arr_rel_insert_input
}

# response of any mutation on the table "pipeline_entity_type"
type pipeline_entity_type_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [pipeline_entity_type!]!
}

# input type for inserting object relation for remote table "pipeline_entity_type"
input pipeline_entity_type_obj_rel_insert_input {
  data: pipeline_entity_type_insert_input!

  # upsert condition
  on_conflict: pipeline_entity_type_on_conflict
}

# on_conflict condition type for table "pipeline_entity_type"
input pipeline_entity_type_on_conflict {
  constraint: pipeline_entity_type_constraint!
  update_columns: [pipeline_entity_type_update_column!]! = []
  where: pipeline_entity_type_bool_exp
}

# Ordering options when selecting data from "pipeline_entity_type".
input pipeline_entity_type_order_by {
  canonical_pipeline_entities_aggregate: canonical_pipeline_entity_aggregate_order_by
  description: order_by
  id: order_by
  name: order_by
  raw_pipeline_entities_aggregate: raw_pipeline_entity_aggregate_order_by
}

# primary key columns input for table: pipeline_entity_type
input pipeline_entity_type_pk_columns_input {
  id: Int!
}

# select columns of table "pipeline_entity_type"
enum pipeline_entity_type_select_column {
  # column name
  description

  # column name
  id

  # column name
  name
}

# input type for updating data in table "pipeline_entity_type"
input pipeline_entity_type_set_input {
  description: String
  id: Int
  name: String
}

# update columns of table "pipeline_entity_type"
enum pipeline_entity_type_update_column {
  # column name
  description

  # column name
  id

  # column name
  name
}

input pipeline_entity_type_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: pipeline_entity_type_inc_input

  # sets the columns of the filtered rows to the given values
  _set: pipeline_entity_type_set_input
  where: pipeline_entity_type_bool_exp!
}

# A Relay connection object on "pipeline_entity_type"
type pipeline_entity_typeConnection {
  edges: [pipeline_entity_typeEdge!]!
  pageInfo: PageInfo!
}

type pipeline_entity_typeEdge {
  cursor: String!
  node: pipeline_entity_type!
}

# columns and relationships of "pipeline_run_metadata"
type pipeline_run_metadata implements Node {
  digest_metadata(
    # JSON select path
    path: String
  ): jsonb
  generation_datetime: timestamp
  id: ID!
  source: String
}

# append existing jsonb value of filtered columns with new jsonb value
input pipeline_run_metadata_append_input {
  digest_metadata: jsonb
}

# Boolean expression to filter rows from the table "pipeline_run_metadata". All fields are combined with a logical 'AND'.
input pipeline_run_metadata_bool_exp {
  _and: [pipeline_run_metadata_bool_exp!]
  _not: pipeline_run_metadata_bool_exp
  _or: [pipeline_run_metadata_bool_exp!]
  digest_metadata: jsonb_comparison_exp
  generation_datetime: timestamp_comparison_exp
  id: Int_comparison_exp
  source: String_comparison_exp
}

# unique or primary key constraints on table "pipeline_run_metadata"
enum pipeline_run_metadata_constraint {
  # unique or primary key constraint on columns "id"
  data_digest_overview_pkey
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input pipeline_run_metadata_delete_at_path_input {
  digest_metadata: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input pipeline_run_metadata_delete_elem_input {
  digest_metadata: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input pipeline_run_metadata_delete_key_input {
  digest_metadata: String
}

# input type for incrementing numeric columns in table "pipeline_run_metadata"
input pipeline_run_metadata_inc_input {
  id: Int
}

# input type for inserting data into table "pipeline_run_metadata"
input pipeline_run_metadata_insert_input {
  digest_metadata: jsonb
  generation_datetime: timestamp
  id: Int
  source: String
}

# response of any mutation on the table "pipeline_run_metadata"
type pipeline_run_metadata_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [pipeline_run_metadata!]!
}

# on_conflict condition type for table "pipeline_run_metadata"
input pipeline_run_metadata_on_conflict {
  constraint: pipeline_run_metadata_constraint!
  update_columns: [pipeline_run_metadata_update_column!]! = []
  where: pipeline_run_metadata_bool_exp
}

# Ordering options when selecting data from "pipeline_run_metadata".
input pipeline_run_metadata_order_by {
  digest_metadata: order_by
  generation_datetime: order_by
  id: order_by
  source: order_by
}

# primary key columns input for table: pipeline_run_metadata
input pipeline_run_metadata_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input pipeline_run_metadata_prepend_input {
  digest_metadata: jsonb
}

# select columns of table "pipeline_run_metadata"
enum pipeline_run_metadata_select_column {
  # column name
  digest_metadata

  # column name
  generation_datetime

  # column name
  id

  # column name
  source
}

# input type for updating data in table "pipeline_run_metadata"
input pipeline_run_metadata_set_input {
  digest_metadata: jsonb
  generation_datetime: timestamp
  id: Int
  source: String
}

# update columns of table "pipeline_run_metadata"
enum pipeline_run_metadata_update_column {
  # column name
  digest_metadata

  # column name
  generation_datetime

  # column name
  id

  # column name
  source
}

input pipeline_run_metadata_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: pipeline_run_metadata_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: pipeline_run_metadata_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: pipeline_run_metadata_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: pipeline_run_metadata_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: pipeline_run_metadata_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: pipeline_run_metadata_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: pipeline_run_metadata_set_input
  where: pipeline_run_metadata_bool_exp!
}

# A Relay connection object on "pipeline_run_metadata"
type pipeline_run_metadataConnection {
  edges: [pipeline_run_metadataEdge!]!
  pageInfo: PageInfo!
}

type pipeline_run_metadataEdge {
  cursor: String!
  node: pipeline_run_metadata!
}

type query_root {
  # fetch data from the table: "alert_definitions"
  alert_definitions_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [alert_definitions_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [alert_definitions_order_by!]

    # filter the rows returned
    where: alert_definitions_bool_exp
  ): alert_definitionsConnection!

  # An array relationship connection
  alert_notifications_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [alert_notifications_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [alert_notifications_order_by!]

    # filter the rows returned
    where: alert_notifications_bool_exp
  ): alert_notificationsConnection!

  # fetch data from the table: "banned_raw_pipeline_entity_match"
  banned_raw_pipeline_entity_match_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [banned_raw_pipeline_entity_match_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [banned_raw_pipeline_entity_match_order_by!]

    # filter the rows returned
    where: banned_raw_pipeline_entity_match_bool_exp
  ): banned_raw_pipeline_entity_matchConnection!

  # fetch data from the table: "canonical_pipeline_entity"
  canonical_pipeline_entity_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [canonical_pipeline_entity_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [canonical_pipeline_entity_order_by!]

    # filter the rows returned
    where: canonical_pipeline_entity_bool_exp
  ): canonical_pipeline_entityConnection!

  # fetch data from the table: "case"
  case_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_order_by!]

    # filter the rows returned
    where: case_bool_exp
  ): caseConnection!

  # fetch data from the table: "case_event"
  case_event_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_event_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_event_order_by!]

    # filter the rows returned
    where: case_event_bool_exp
  ): case_eventConnection!

  # An array relationship connection
  case_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_order_by!]

    # filter the rows returned
    where: case_metadata_bool_exp
  ): case_metadataConnection!

  # fetch data from the table: "case_metadata_type"
  case_metadata_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_metadata_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_type_order_by!]

    # filter the rows returned
    where: case_metadata_type_bool_exp
  ): case_metadata_typeConnection!

  # fetch data from the table: "case_status_type"
  case_status_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_status_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_status_type_order_by!]

    # filter the rows returned
    where: case_status_type_bool_exp
  ): case_status_typeConnection!

  # fetch data from the table: "case_type"
  case_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_order_by!]

    # filter the rows returned
    where: case_type_bool_exp
  ): case_typeConnection!

  # fetch data from the table: "case_type_default_event"
  case_type_default_event_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_event_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_event_order_by!]

    # filter the rows returned
    where: case_type_default_event_bool_exp
  ): case_type_default_eventConnection!

  # fetch data from the table: "case_type_default_field"
  case_type_default_field_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_field_order_by!]

    # filter the rows returned
    where: case_type_default_field_bool_exp
  ): case_type_default_fieldConnection!

  # fetch data from the table: "case_type_default_status"
  case_type_default_status_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_status_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_status_order_by!]

    # filter the rows returned
    where: case_type_default_status_bool_exp
  ): case_type_default_statusConnection!

  # fetch data from the table: "case_type_metadata_from_druid_dimension"
  case_type_metadata_from_druid_dimension_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_dimension_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_dimension_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_dimension_bool_exp
  ): case_type_metadata_from_druid_dimensionConnection!

  # fetch data from the table: "case_type_metadata_from_druid_field"
  case_type_metadata_from_druid_field_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_field_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_field_bool_exp
  ): case_type_metadata_from_druid_fieldConnection!

  # fetch data from the table: "category"
  category_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [category_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [category_order_by!]

    # filter the rows returned
    where: category_bool_exp
  ): categoryConnection!

  # fetch data from the table: "dashboard"
  dashboard_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dashboard_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dashboard_order_by!]

    # filter the rows returned
    where: dashboard_bool_exp
  ): dashboardConnection!

  # fetch data from the table: "dashboard_session"
  dashboard_session_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dashboard_session_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dashboard_session_order_by!]

    # filter the rows returned
    where: dashboard_session_bool_exp
  ): dashboard_sessionConnection!

  # fetch data from the table: "data_upload_file_summary"
  data_upload_file_summary_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [data_upload_file_summary_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [data_upload_file_summary_order_by!]

    # filter the rows returned
    where: data_upload_file_summary_bool_exp
  ): data_upload_file_summaryConnection!

  # fetch data from the table: "dataprep_flow"
  dataprep_flow_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dataprep_flow_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dataprep_flow_order_by!]

    # filter the rows returned
    where: dataprep_flow_bool_exp
  ): dataprep_flowConnection!

  # fetch data from the table: "dataprep_job"
  dataprep_job_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dataprep_job_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dataprep_job_order_by!]

    # filter the rows returned
    where: dataprep_job_bool_exp
  ): dataprep_jobConnection!

  # fetch data from the table: "dimension_category"
  dimension_category_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_category_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_order_by!]

    # filter the rows returned
    where: dimension_category_bool_exp
  ): dimension_categoryConnection!

  # fetch data from the table: "dimension_category_mapping"
  dimension_category_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_mapping_order_by!]

    # filter the rows returned
    where: dimension_category_mapping_bool_exp
  ): dimension_category_mappingConnection!

  # fetch data from the table: "dimension"
  dimension_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_order_by!]

    # filter the rows returned
    where: dimension_bool_exp
  ): dimensionConnection!

  # fetch data from the table: "external_alert_activity_to_ignore"
  external_alert_activity_to_ignore_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [external_alert_activity_to_ignore_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [external_alert_activity_to_ignore_order_by!]

    # filter the rows returned
    where: external_alert_activity_to_ignore_bool_exp
  ): external_alert_activity_to_ignoreConnection!

  # fetch data from the table: "external_alert_type"
  external_alert_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [external_alert_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [external_alert_type_order_by!]

    # filter the rows returned
    where: external_alert_type_bool_exp
  ): external_alert_typeConnection!

  # fetch data from the table: "feed_update"
  feed_update_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [feed_update_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [feed_update_order_by!]

    # filter the rows returned
    where: feed_update_bool_exp
  ): feed_updateConnection!

  # fetch data from the table: "feed_update_type"
  feed_update_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [feed_update_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [feed_update_type_order_by!]

    # filter the rows returned
    where: feed_update_type_bool_exp
  ): feed_update_typeConnection!

  # fetch data from the table: "field_category_mapping"
  field_category_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_category_mapping_order_by!]

    # filter the rows returned
    where: field_category_mapping_bool_exp
  ): field_category_mappingConnection!

  # fetch data from the table: "field"
  field_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_order_by!]

    # filter the rows returned
    where: field_bool_exp
  ): fieldConnection!

  # fetch data from the table: "field_dimension_mapping"
  field_dimension_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_dimension_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_dimension_mapping_order_by!]

    # filter the rows returned
    where: field_dimension_mapping_bool_exp
  ): field_dimension_mappingConnection!

  # fetch data from the table: "field_pipeline_datasource_mapping"
  field_pipeline_datasource_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_pipeline_datasource_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: field_pipeline_datasource_mapping_bool_exp
  ): field_pipeline_datasource_mappingConnection!

  # An array relationship connection
  geo_dimension_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): geo_dimension_metadataConnection!

  # An array relationship connection
  hierarchical_dimension_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): hierarchical_dimension_metadataConnection!
  node(
    # A globally unique id
    id: ID!
  ): Node

  # fetch data from the table: "non_hierarchical_dimension"
  non_hierarchical_dimension_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [non_hierarchical_dimension_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [non_hierarchical_dimension_order_by!]

    # filter the rows returned
    where: non_hierarchical_dimension_bool_exp
  ): non_hierarchical_dimensionConnection!

  # fetch data from the table: "pipeline_datasource"
  pipeline_datasource_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_datasource_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_datasource_order_by!]

    # filter the rows returned
    where: pipeline_datasource_bool_exp
  ): pipeline_datasourceConnection!

  # fetch data from the table: "pipeline_entity_match"
  pipeline_entity_match_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_entity_match_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_match_order_by!]

    # filter the rows returned
    where: pipeline_entity_match_bool_exp
  ): pipeline_entity_matchConnection!

  # fetch data from the table: "pipeline_entity_type"
  pipeline_entity_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_entity_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_type_order_by!]

    # filter the rows returned
    where: pipeline_entity_type_bool_exp
  ): pipeline_entity_typeConnection!

  # fetch data from the table: "pipeline_run_metadata"
  pipeline_run_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_run_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_run_metadata_order_by!]

    # filter the rows returned
    where: pipeline_run_metadata_bool_exp
  ): pipeline_run_metadataConnection!

  # fetch data from the table: "raw_pipeline_entity"
  raw_pipeline_entity_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [raw_pipeline_entity_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [raw_pipeline_entity_order_by!]

    # filter the rows returned
    where: raw_pipeline_entity_bool_exp
  ): raw_pipeline_entityConnection!

  # fetch data from the table: "self_serve_source"
  self_serve_source_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [self_serve_source_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [self_serve_source_order_by!]

    # filter the rows returned
    where: self_serve_source_bool_exp
  ): self_serve_sourceConnection!

  # fetch data from the table: "unpublished_field_category_mapping"
  unpublished_field_category_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_category_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_category_mapping_bool_exp
  ): unpublished_field_category_mappingConnection!

  # fetch data from the table: "unpublished_field"
  unpublished_field_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_order_by!]

    # filter the rows returned
    where: unpublished_field_bool_exp
  ): unpublished_fieldConnection!

  # fetch data from the table: "unpublished_field_dimension_mapping"
  unpublished_field_dimension_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_dimension_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_dimension_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_dimension_mapping_bool_exp
  ): unpublished_field_dimension_mappingConnection!

  # fetch data from the table: "unpublished_field_pipeline_datasource_mapping"
  unpublished_field_pipeline_datasource_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_pipeline_datasource_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_pipeline_datasource_mapping_bool_exp
  ): unpublished_field_pipeline_datasource_mappingConnection!
}

# columns and relationships of "raw_pipeline_entity"
type raw_pipeline_entity implements Node {
  # An array relationship
  bannedRawPipelineEntityMatchesByRawEntityIdB(
    # distinct select on columns
    distinct_on: [banned_raw_pipeline_entity_match_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [banned_raw_pipeline_entity_match_order_by!]

    # filter the rows returned
    where: banned_raw_pipeline_entity_match_bool_exp
  ): [banned_raw_pipeline_entity_match!]!

  # An aggregate relationship
  bannedRawPipelineEntityMatchesByRawEntityIdB_aggregate(
    # distinct select on columns
    distinct_on: [banned_raw_pipeline_entity_match_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [banned_raw_pipeline_entity_match_order_by!]

    # filter the rows returned
    where: banned_raw_pipeline_entity_match_bool_exp
  ): banned_raw_pipeline_entity_match_aggregate!

  # An array relationship connection
  bannedRawPipelineEntityMatchesByRawEntityIdB_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [banned_raw_pipeline_entity_match_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [banned_raw_pipeline_entity_match_order_by!]

    # filter the rows returned
    where: banned_raw_pipeline_entity_match_bool_exp
  ): banned_raw_pipeline_entity_matchConnection!

  # An array relationship
  banned_raw_pipeline_entity_matches(
    # distinct select on columns
    distinct_on: [banned_raw_pipeline_entity_match_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [banned_raw_pipeline_entity_match_order_by!]

    # filter the rows returned
    where: banned_raw_pipeline_entity_match_bool_exp
  ): [banned_raw_pipeline_entity_match!]!

  # An aggregate relationship
  banned_raw_pipeline_entity_matches_aggregate(
    # distinct select on columns
    distinct_on: [banned_raw_pipeline_entity_match_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [banned_raw_pipeline_entity_match_order_by!]

    # filter the rows returned
    where: banned_raw_pipeline_entity_match_bool_exp
  ): banned_raw_pipeline_entity_match_aggregate!

  # An array relationship connection
  banned_raw_pipeline_entity_matches_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [banned_raw_pipeline_entity_match_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [banned_raw_pipeline_entity_match_order_by!]

    # filter the rows returned
    where: banned_raw_pipeline_entity_match_bool_exp
  ): banned_raw_pipeline_entity_matchConnection!
  entity_metadata(
    # JSON select path
    path: String
  ): jsonb!
  entity_type_id: Int!
  id: ID!
  in_latest_datasource: Boolean!

  # An object relationship
  pipeline_entity_match: pipeline_entity_match

  # An array relationship
  pipeline_entity_matches(
    # distinct select on columns
    distinct_on: [pipeline_entity_match_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_match_order_by!]

    # filter the rows returned
    where: pipeline_entity_match_bool_exp
  ): [pipeline_entity_match!]!

  # An aggregate relationship
  pipeline_entity_matches_aggregate(
    # distinct select on columns
    distinct_on: [pipeline_entity_match_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_match_order_by!]

    # filter the rows returned
    where: pipeline_entity_match_bool_exp
  ): pipeline_entity_match_aggregate!

  # An array relationship connection
  pipeline_entity_matches_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_entity_match_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_match_order_by!]

    # filter the rows returned
    where: pipeline_entity_match_bool_exp
  ): pipeline_entity_matchConnection!

  # An object relationship
  pipeline_entity_type: pipeline_entity_type!
  search_term: String
  source: String!
  source_entity_id: String!
}

# aggregated selection of "raw_pipeline_entity"
type raw_pipeline_entity_aggregate {
  aggregate: raw_pipeline_entity_aggregate_fields
  nodes: [raw_pipeline_entity!]!
}

# aggregate fields of "raw_pipeline_entity"
type raw_pipeline_entity_aggregate_fields {
  avg: raw_pipeline_entity_avg_fields
  count(columns: [raw_pipeline_entity_select_column!], distinct: Boolean): Int!
  max: raw_pipeline_entity_max_fields
  min: raw_pipeline_entity_min_fields
  stddev: raw_pipeline_entity_stddev_fields
  stddev_pop: raw_pipeline_entity_stddev_pop_fields
  stddev_samp: raw_pipeline_entity_stddev_samp_fields
  sum: raw_pipeline_entity_sum_fields
  var_pop: raw_pipeline_entity_var_pop_fields
  var_samp: raw_pipeline_entity_var_samp_fields
  variance: raw_pipeline_entity_variance_fields
}

# order by aggregate values of table "raw_pipeline_entity"
input raw_pipeline_entity_aggregate_order_by {
  avg: raw_pipeline_entity_avg_order_by
  count: order_by
  max: raw_pipeline_entity_max_order_by
  min: raw_pipeline_entity_min_order_by
  stddev: raw_pipeline_entity_stddev_order_by
  stddev_pop: raw_pipeline_entity_stddev_pop_order_by
  stddev_samp: raw_pipeline_entity_stddev_samp_order_by
  sum: raw_pipeline_entity_sum_order_by
  var_pop: raw_pipeline_entity_var_pop_order_by
  var_samp: raw_pipeline_entity_var_samp_order_by
  variance: raw_pipeline_entity_variance_order_by
}

# append existing jsonb value of filtered columns with new jsonb value
input raw_pipeline_entity_append_input {
  entity_metadata: jsonb
}

# input type for inserting array relation for remote table "raw_pipeline_entity"
input raw_pipeline_entity_arr_rel_insert_input {
  data: [raw_pipeline_entity_insert_input!]!

  # upsert condition
  on_conflict: raw_pipeline_entity_on_conflict
}

# aggregate avg on columns
type raw_pipeline_entity_avg_fields {
  entity_type_id: Float
  id: Float
}

# order by avg() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_avg_order_by {
  entity_type_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table "raw_pipeline_entity". All fields are combined with a logical 'AND'.
input raw_pipeline_entity_bool_exp {
  _and: [raw_pipeline_entity_bool_exp!]
  _not: raw_pipeline_entity_bool_exp
  _or: [raw_pipeline_entity_bool_exp!]
  bannedRawPipelineEntityMatchesByRawEntityIdB: banned_raw_pipeline_entity_match_bool_exp
  banned_raw_pipeline_entity_matches: banned_raw_pipeline_entity_match_bool_exp
  entity_metadata: jsonb_comparison_exp
  entity_type_id: Int_comparison_exp
  id: Int_comparison_exp
  in_latest_datasource: Boolean_comparison_exp
  pipeline_entity_match: pipeline_entity_match_bool_exp
  pipeline_entity_matches: pipeline_entity_match_bool_exp
  pipeline_entity_type: pipeline_entity_type_bool_exp
  search_term: String_comparison_exp
  source: String_comparison_exp
  source_entity_id: String_comparison_exp
}

# unique or primary key constraints on table "raw_pipeline_entity"
enum raw_pipeline_entity_constraint {
  # unique or primary key constraint on columns "id"
  raw_pipeline_entity_pkey

  # unique or primary key constraint on columns "entity_type_id", "source", "source_entity_id"
  unique_raw_pipeline_entity
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input raw_pipeline_entity_delete_at_path_input {
  entity_metadata: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input raw_pipeline_entity_delete_elem_input {
  entity_metadata: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input raw_pipeline_entity_delete_key_input {
  entity_metadata: String
}

# input type for incrementing numeric columns in table "raw_pipeline_entity"
input raw_pipeline_entity_inc_input {
  entity_type_id: Int
  id: Int
}

# input type for inserting data into table "raw_pipeline_entity"
input raw_pipeline_entity_insert_input {
  bannedRawPipelineEntityMatchesByRawEntityIdB: banned_raw_pipeline_entity_match_arr_rel_insert_input
  banned_raw_pipeline_entity_matches: banned_raw_pipeline_entity_match_arr_rel_insert_input
  entity_metadata: jsonb
  entity_type_id: Int
  id: Int
  in_latest_datasource: Boolean
  pipeline_entity_match: pipeline_entity_match_obj_rel_insert_input
  pipeline_entity_matches: pipeline_entity_match_arr_rel_insert_input
  pipeline_entity_type: pipeline_entity_type_obj_rel_insert_input
  search_term: String
  source: String
  source_entity_id: String
}

# aggregate max on columns
type raw_pipeline_entity_max_fields {
  entity_type_id: Int
  id: Int
  search_term: String
  source: String
  source_entity_id: String
}

# order by max() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_max_order_by {
  entity_type_id: order_by
  id: order_by
  search_term: order_by
  source: order_by
  source_entity_id: order_by
}

# aggregate min on columns
type raw_pipeline_entity_min_fields {
  entity_type_id: Int
  id: Int
  search_term: String
  source: String
  source_entity_id: String
}

# order by min() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_min_order_by {
  entity_type_id: order_by
  id: order_by
  search_term: order_by
  source: order_by
  source_entity_id: order_by
}

# response of any mutation on the table "raw_pipeline_entity"
type raw_pipeline_entity_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [raw_pipeline_entity!]!
}

# input type for inserting object relation for remote table "raw_pipeline_entity"
input raw_pipeline_entity_obj_rel_insert_input {
  data: raw_pipeline_entity_insert_input!

  # upsert condition
  on_conflict: raw_pipeline_entity_on_conflict
}

# on_conflict condition type for table "raw_pipeline_entity"
input raw_pipeline_entity_on_conflict {
  constraint: raw_pipeline_entity_constraint!
  update_columns: [raw_pipeline_entity_update_column!]! = []
  where: raw_pipeline_entity_bool_exp
}

# Ordering options when selecting data from "raw_pipeline_entity".
input raw_pipeline_entity_order_by {
  bannedRawPipelineEntityMatchesByRawEntityIdB_aggregate: banned_raw_pipeline_entity_match_aggregate_order_by
  banned_raw_pipeline_entity_matches_aggregate: banned_raw_pipeline_entity_match_aggregate_order_by
  entity_metadata: order_by
  entity_type_id: order_by
  id: order_by
  in_latest_datasource: order_by
  pipeline_entity_match: pipeline_entity_match_order_by
  pipeline_entity_matches_aggregate: pipeline_entity_match_aggregate_order_by
  pipeline_entity_type: pipeline_entity_type_order_by
  search_term: order_by
  source: order_by
  source_entity_id: order_by
}

# primary key columns input for table: raw_pipeline_entity
input raw_pipeline_entity_pk_columns_input {
  id: Int!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input raw_pipeline_entity_prepend_input {
  entity_metadata: jsonb
}

# select columns of table "raw_pipeline_entity"
enum raw_pipeline_entity_select_column {
  # column name
  entity_metadata

  # column name
  entity_type_id

  # column name
  id

  # column name
  in_latest_datasource

  # column name
  search_term

  # column name
  source

  # column name
  source_entity_id
}

# input type for updating data in table "raw_pipeline_entity"
input raw_pipeline_entity_set_input {
  entity_metadata: jsonb
  entity_type_id: Int
  id: Int
  in_latest_datasource: Boolean
  search_term: String
  source: String
  source_entity_id: String
}

# aggregate stddev on columns
type raw_pipeline_entity_stddev_fields {
  entity_type_id: Float
  id: Float
}

# order by stddev() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_stddev_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type raw_pipeline_entity_stddev_pop_fields {
  entity_type_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_stddev_pop_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type raw_pipeline_entity_stddev_samp_fields {
  entity_type_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_stddev_samp_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate sum on columns
type raw_pipeline_entity_sum_fields {
  entity_type_id: Int
  id: Int
}

# order by sum() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_sum_order_by {
  entity_type_id: order_by
  id: order_by
}

# update columns of table "raw_pipeline_entity"
enum raw_pipeline_entity_update_column {
  # column name
  entity_metadata

  # column name
  entity_type_id

  # column name
  id

  # column name
  in_latest_datasource

  # column name
  search_term

  # column name
  source

  # column name
  source_entity_id
}

input raw_pipeline_entity_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: raw_pipeline_entity_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: raw_pipeline_entity_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: raw_pipeline_entity_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: raw_pipeline_entity_delete_key_input

  # increments the numeric columns with given value of the filtered values
  _inc: raw_pipeline_entity_inc_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: raw_pipeline_entity_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: raw_pipeline_entity_set_input
  where: raw_pipeline_entity_bool_exp!
}

# aggregate var_pop on columns
type raw_pipeline_entity_var_pop_fields {
  entity_type_id: Float
  id: Float
}

# order by var_pop() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_var_pop_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type raw_pipeline_entity_var_samp_fields {
  entity_type_id: Float
  id: Float
}

# order by var_samp() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_var_samp_order_by {
  entity_type_id: order_by
  id: order_by
}

# aggregate variance on columns
type raw_pipeline_entity_variance_fields {
  entity_type_id: Float
  id: Float
}

# order by variance() on columns of table "raw_pipeline_entity"
input raw_pipeline_entity_variance_order_by {
  entity_type_id: order_by
  id: order_by
}

# A Relay connection object on "raw_pipeline_entity"
type raw_pipeline_entityConnection {
  edges: [raw_pipeline_entityEdge!]!
  pageInfo: PageInfo!
}

type raw_pipeline_entityEdge {
  cursor: String!
  node: raw_pipeline_entity!
}

# columns and relationships of "self_serve_source"
type self_serve_source implements Node {
  created: timestamp!

  # An array relationship
  data_upload_file_summaries(
    # distinct select on columns
    distinct_on: [data_upload_file_summary_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [data_upload_file_summary_order_by!]

    # filter the rows returned
    where: data_upload_file_summary_bool_exp
  ): [data_upload_file_summary!]!

  # An aggregate relationship
  data_upload_file_summaries_aggregate(
    # distinct select on columns
    distinct_on: [data_upload_file_summary_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [data_upload_file_summary_order_by!]

    # filter the rows returned
    where: data_upload_file_summary_bool_exp
  ): data_upload_file_summary_aggregate!

  # An array relationship connection
  data_upload_file_summaries_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [data_upload_file_summary_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [data_upload_file_summary_order_by!]

    # filter the rows returned
    where: data_upload_file_summary_bool_exp
  ): data_upload_file_summaryConnection!

  # An object relationship
  dataprep_flow: dataprep_flow
  dataprep_flow_id: Int
  id: ID!
  last_modified: timestamp!

  # An object relationship
  pipeline_datasource: pipeline_datasource!
  source_id: String!
}

# aggregated selection of "self_serve_source"
type self_serve_source_aggregate {
  aggregate: self_serve_source_aggregate_fields
  nodes: [self_serve_source!]!
}

# aggregate fields of "self_serve_source"
type self_serve_source_aggregate_fields {
  avg: self_serve_source_avg_fields
  count(columns: [self_serve_source_select_column!], distinct: Boolean): Int!
  max: self_serve_source_max_fields
  min: self_serve_source_min_fields
  stddev: self_serve_source_stddev_fields
  stddev_pop: self_serve_source_stddev_pop_fields
  stddev_samp: self_serve_source_stddev_samp_fields
  sum: self_serve_source_sum_fields
  var_pop: self_serve_source_var_pop_fields
  var_samp: self_serve_source_var_samp_fields
  variance: self_serve_source_variance_fields
}

# order by aggregate values of table "self_serve_source"
input self_serve_source_aggregate_order_by {
  avg: self_serve_source_avg_order_by
  count: order_by
  max: self_serve_source_max_order_by
  min: self_serve_source_min_order_by
  stddev: self_serve_source_stddev_order_by
  stddev_pop: self_serve_source_stddev_pop_order_by
  stddev_samp: self_serve_source_stddev_samp_order_by
  sum: self_serve_source_sum_order_by
  var_pop: self_serve_source_var_pop_order_by
  var_samp: self_serve_source_var_samp_order_by
  variance: self_serve_source_variance_order_by
}

# input type for inserting array relation for remote table "self_serve_source"
input self_serve_source_arr_rel_insert_input {
  data: [self_serve_source_insert_input!]!

  # upsert condition
  on_conflict: self_serve_source_on_conflict
}

# aggregate avg on columns
type self_serve_source_avg_fields {
  dataprep_flow_id: Float
  id: Float
}

# order by avg() on columns of table "self_serve_source"
input self_serve_source_avg_order_by {
  dataprep_flow_id: order_by
  id: order_by
}

# Boolean expression to filter rows from the table "self_serve_source". All fields are combined with a logical 'AND'.
input self_serve_source_bool_exp {
  _and: [self_serve_source_bool_exp!]
  _not: self_serve_source_bool_exp
  _or: [self_serve_source_bool_exp!]
  created: timestamp_comparison_exp
  data_upload_file_summaries: data_upload_file_summary_bool_exp
  dataprep_flow: dataprep_flow_bool_exp
  dataprep_flow_id: Int_comparison_exp
  id: Int_comparison_exp
  last_modified: timestamp_comparison_exp
  pipeline_datasource: pipeline_datasource_bool_exp
  source_id: String_comparison_exp
}

# unique or primary key constraints on table "self_serve_source"
enum self_serve_source_constraint {
  # unique or primary key constraint on columns "id"
  self_serve_source_pkey
}

# input type for incrementing numeric columns in table "self_serve_source"
input self_serve_source_inc_input {
  dataprep_flow_id: Int
  id: Int
}

# input type for inserting data into table "self_serve_source"
input self_serve_source_insert_input {
  created: timestamp
  data_upload_file_summaries: data_upload_file_summary_arr_rel_insert_input
  dataprep_flow: dataprep_flow_obj_rel_insert_input
  dataprep_flow_id: Int
  id: Int
  last_modified: timestamp
  pipeline_datasource: pipeline_datasource_obj_rel_insert_input
  source_id: String
}

# aggregate max on columns
type self_serve_source_max_fields {
  created: timestamp
  dataprep_flow_id: Int
  id: Int
  last_modified: timestamp
  source_id: String
}

# order by max() on columns of table "self_serve_source"
input self_serve_source_max_order_by {
  created: order_by
  dataprep_flow_id: order_by
  id: order_by
  last_modified: order_by
  source_id: order_by
}

# aggregate min on columns
type self_serve_source_min_fields {
  created: timestamp
  dataprep_flow_id: Int
  id: Int
  last_modified: timestamp
  source_id: String
}

# order by min() on columns of table "self_serve_source"
input self_serve_source_min_order_by {
  created: order_by
  dataprep_flow_id: order_by
  id: order_by
  last_modified: order_by
  source_id: order_by
}

# response of any mutation on the table "self_serve_source"
type self_serve_source_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [self_serve_source!]!
}

# input type for inserting object relation for remote table "self_serve_source"
input self_serve_source_obj_rel_insert_input {
  data: self_serve_source_insert_input!

  # upsert condition
  on_conflict: self_serve_source_on_conflict
}

# on_conflict condition type for table "self_serve_source"
input self_serve_source_on_conflict {
  constraint: self_serve_source_constraint!
  update_columns: [self_serve_source_update_column!]! = []
  where: self_serve_source_bool_exp
}

# Ordering options when selecting data from "self_serve_source".
input self_serve_source_order_by {
  created: order_by
  data_upload_file_summaries_aggregate: data_upload_file_summary_aggregate_order_by
  dataprep_flow: dataprep_flow_order_by
  dataprep_flow_id: order_by
  id: order_by
  last_modified: order_by
  pipeline_datasource: pipeline_datasource_order_by
  source_id: order_by
}

# primary key columns input for table: self_serve_source
input self_serve_source_pk_columns_input {
  id: Int!
}

# select columns of table "self_serve_source"
enum self_serve_source_select_column {
  # column name
  created

  # column name
  dataprep_flow_id

  # column name
  id

  # column name
  last_modified

  # column name
  source_id
}

# input type for updating data in table "self_serve_source"
input self_serve_source_set_input {
  created: timestamp
  dataprep_flow_id: Int
  id: Int
  last_modified: timestamp
  source_id: String
}

# aggregate stddev on columns
type self_serve_source_stddev_fields {
  dataprep_flow_id: Float
  id: Float
}

# order by stddev() on columns of table "self_serve_source"
input self_serve_source_stddev_order_by {
  dataprep_flow_id: order_by
  id: order_by
}

# aggregate stddev_pop on columns
type self_serve_source_stddev_pop_fields {
  dataprep_flow_id: Float
  id: Float
}

# order by stddev_pop() on columns of table "self_serve_source"
input self_serve_source_stddev_pop_order_by {
  dataprep_flow_id: order_by
  id: order_by
}

# aggregate stddev_samp on columns
type self_serve_source_stddev_samp_fields {
  dataprep_flow_id: Float
  id: Float
}

# order by stddev_samp() on columns of table "self_serve_source"
input self_serve_source_stddev_samp_order_by {
  dataprep_flow_id: order_by
  id: order_by
}

# aggregate sum on columns
type self_serve_source_sum_fields {
  dataprep_flow_id: Int
  id: Int
}

# order by sum() on columns of table "self_serve_source"
input self_serve_source_sum_order_by {
  dataprep_flow_id: order_by
  id: order_by
}

# update columns of table "self_serve_source"
enum self_serve_source_update_column {
  # column name
  created

  # column name
  dataprep_flow_id

  # column name
  id

  # column name
  last_modified

  # column name
  source_id
}

input self_serve_source_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: self_serve_source_inc_input

  # sets the columns of the filtered rows to the given values
  _set: self_serve_source_set_input
  where: self_serve_source_bool_exp!
}

# aggregate var_pop on columns
type self_serve_source_var_pop_fields {
  dataprep_flow_id: Float
  id: Float
}

# order by var_pop() on columns of table "self_serve_source"
input self_serve_source_var_pop_order_by {
  dataprep_flow_id: order_by
  id: order_by
}

# aggregate var_samp on columns
type self_serve_source_var_samp_fields {
  dataprep_flow_id: Float
  id: Float
}

# order by var_samp() on columns of table "self_serve_source"
input self_serve_source_var_samp_order_by {
  dataprep_flow_id: order_by
  id: order_by
}

# aggregate variance on columns
type self_serve_source_variance_fields {
  dataprep_flow_id: Float
  id: Float
}

# order by variance() on columns of table "self_serve_source"
input self_serve_source_variance_order_by {
  dataprep_flow_id: order_by
  id: order_by
}

# A Relay connection object on "self_serve_source"
type self_serve_sourceConnection {
  edges: [self_serve_sourceEdge!]!
  pageInfo: PageInfo!
}

type self_serve_sourceEdge {
  cursor: String!
  node: self_serve_source!
}

# Boolean expression to compare columns of type "String". All fields are combined with logical 'AND'.
input String_comparison_exp {
  _eq: String
  _gt: String
  _gte: String

  # does the column match the given case-insensitive pattern
  _ilike: String
  _in: [String!]

  # does the column match the given POSIX regular expression, case insensitive
  _iregex: String
  _is_null: Boolean

  # does the column match the given pattern
  _like: String
  _lt: String
  _lte: String
  _neq: String

  # does the column NOT match the given case-insensitive pattern
  _nilike: String
  _nin: [String!]

  # does the column NOT match the given POSIX regular expression, case insensitive
  _niregex: String

  # does the column NOT match the given pattern
  _nlike: String

  # does the column NOT match the given POSIX regular expression, case sensitive
  _nregex: String

  # does the column NOT match the given SQL regular expression
  _nsimilar: String

  # does the column match the given POSIX regular expression, case sensitive
  _regex: String

  # does the column match the given SQL regular expression
  _similar: String
}

type subscription_root {
  # fetch data from the table: "alert_definitions"
  alert_definitions_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [alert_definitions_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [alert_definitions_order_by!]

    # filter the rows returned
    where: alert_definitions_bool_exp
  ): alert_definitionsConnection!

  # An array relationship connection
  alert_notifications_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [alert_notifications_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [alert_notifications_order_by!]

    # filter the rows returned
    where: alert_notifications_bool_exp
  ): alert_notificationsConnection!

  # fetch data from the table: "banned_raw_pipeline_entity_match"
  banned_raw_pipeline_entity_match_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [banned_raw_pipeline_entity_match_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [banned_raw_pipeline_entity_match_order_by!]

    # filter the rows returned
    where: banned_raw_pipeline_entity_match_bool_exp
  ): banned_raw_pipeline_entity_matchConnection!

  # fetch data from the table: "canonical_pipeline_entity"
  canonical_pipeline_entity_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [canonical_pipeline_entity_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [canonical_pipeline_entity_order_by!]

    # filter the rows returned
    where: canonical_pipeline_entity_bool_exp
  ): canonical_pipeline_entityConnection!

  # fetch data from the table: "case"
  case_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_order_by!]

    # filter the rows returned
    where: case_bool_exp
  ): caseConnection!

  # fetch data from the table: "case_event"
  case_event_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_event_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_event_order_by!]

    # filter the rows returned
    where: case_event_bool_exp
  ): case_eventConnection!

  # An array relationship connection
  case_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_order_by!]

    # filter the rows returned
    where: case_metadata_bool_exp
  ): case_metadataConnection!

  # fetch data from the table: "case_metadata_type"
  case_metadata_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_metadata_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_metadata_type_order_by!]

    # filter the rows returned
    where: case_metadata_type_bool_exp
  ): case_metadata_typeConnection!

  # fetch data from the table: "case_status_type"
  case_status_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_status_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_status_type_order_by!]

    # filter the rows returned
    where: case_status_type_bool_exp
  ): case_status_typeConnection!

  # fetch data from the table: "case_type"
  case_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_order_by!]

    # filter the rows returned
    where: case_type_bool_exp
  ): case_typeConnection!

  # fetch data from the table: "case_type_default_event"
  case_type_default_event_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_event_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_event_order_by!]

    # filter the rows returned
    where: case_type_default_event_bool_exp
  ): case_type_default_eventConnection!

  # fetch data from the table: "case_type_default_field"
  case_type_default_field_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_field_order_by!]

    # filter the rows returned
    where: case_type_default_field_bool_exp
  ): case_type_default_fieldConnection!

  # fetch data from the table: "case_type_default_status"
  case_type_default_status_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_default_status_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_default_status_order_by!]

    # filter the rows returned
    where: case_type_default_status_bool_exp
  ): case_type_default_statusConnection!

  # fetch data from the table: "case_type_metadata_from_druid_dimension"
  case_type_metadata_from_druid_dimension_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_dimension_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_dimension_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_dimension_bool_exp
  ): case_type_metadata_from_druid_dimensionConnection!

  # fetch data from the table: "case_type_metadata_from_druid_field"
  case_type_metadata_from_druid_field_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [case_type_metadata_from_druid_field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [case_type_metadata_from_druid_field_order_by!]

    # filter the rows returned
    where: case_type_metadata_from_druid_field_bool_exp
  ): case_type_metadata_from_druid_fieldConnection!

  # fetch data from the table: "category"
  category_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [category_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [category_order_by!]

    # filter the rows returned
    where: category_bool_exp
  ): categoryConnection!

  # fetch data from the table: "dashboard"
  dashboard_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dashboard_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dashboard_order_by!]

    # filter the rows returned
    where: dashboard_bool_exp
  ): dashboardConnection!

  # fetch data from the table: "dashboard_session"
  dashboard_session_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dashboard_session_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dashboard_session_order_by!]

    # filter the rows returned
    where: dashboard_session_bool_exp
  ): dashboard_sessionConnection!

  # fetch data from the table: "data_upload_file_summary"
  data_upload_file_summary_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [data_upload_file_summary_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [data_upload_file_summary_order_by!]

    # filter the rows returned
    where: data_upload_file_summary_bool_exp
  ): data_upload_file_summaryConnection!

  # fetch data from the table: "dataprep_flow"
  dataprep_flow_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dataprep_flow_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dataprep_flow_order_by!]

    # filter the rows returned
    where: dataprep_flow_bool_exp
  ): dataprep_flowConnection!

  # fetch data from the table: "dataprep_job"
  dataprep_job_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dataprep_job_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dataprep_job_order_by!]

    # filter the rows returned
    where: dataprep_job_bool_exp
  ): dataprep_jobConnection!

  # fetch data from the table: "dimension_category"
  dimension_category_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_category_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_order_by!]

    # filter the rows returned
    where: dimension_category_bool_exp
  ): dimension_categoryConnection!

  # fetch data from the table: "dimension_category_mapping"
  dimension_category_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_category_mapping_order_by!]

    # filter the rows returned
    where: dimension_category_mapping_bool_exp
  ): dimension_category_mappingConnection!

  # fetch data from the table: "dimension"
  dimension_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [dimension_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [dimension_order_by!]

    # filter the rows returned
    where: dimension_bool_exp
  ): dimensionConnection!

  # fetch data from the table: "external_alert_activity_to_ignore"
  external_alert_activity_to_ignore_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [external_alert_activity_to_ignore_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [external_alert_activity_to_ignore_order_by!]

    # filter the rows returned
    where: external_alert_activity_to_ignore_bool_exp
  ): external_alert_activity_to_ignoreConnection!

  # fetch data from the table: "external_alert_type"
  external_alert_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [external_alert_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [external_alert_type_order_by!]

    # filter the rows returned
    where: external_alert_type_bool_exp
  ): external_alert_typeConnection!

  # fetch data from the table: "feed_update"
  feed_update_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [feed_update_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [feed_update_order_by!]

    # filter the rows returned
    where: feed_update_bool_exp
  ): feed_updateConnection!

  # fetch data from the table: "feed_update_type"
  feed_update_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [feed_update_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [feed_update_type_order_by!]

    # filter the rows returned
    where: feed_update_type_bool_exp
  ): feed_update_typeConnection!

  # fetch data from the table: "field_category_mapping"
  field_category_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_category_mapping_order_by!]

    # filter the rows returned
    where: field_category_mapping_bool_exp
  ): field_category_mappingConnection!

  # fetch data from the table: "field"
  field_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_order_by!]

    # filter the rows returned
    where: field_bool_exp
  ): fieldConnection!

  # fetch data from the table: "field_dimension_mapping"
  field_dimension_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_dimension_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_dimension_mapping_order_by!]

    # filter the rows returned
    where: field_dimension_mapping_bool_exp
  ): field_dimension_mappingConnection!

  # fetch data from the table: "field_pipeline_datasource_mapping"
  field_pipeline_datasource_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [field_pipeline_datasource_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: field_pipeline_datasource_mapping_bool_exp
  ): field_pipeline_datasource_mappingConnection!

  # An array relationship connection
  geo_dimension_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [geo_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [geo_dimension_metadata_order_by!]

    # filter the rows returned
    where: geo_dimension_metadata_bool_exp
  ): geo_dimension_metadataConnection!

  # An array relationship connection
  hierarchical_dimension_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [hierarchical_dimension_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [hierarchical_dimension_metadata_order_by!]

    # filter the rows returned
    where: hierarchical_dimension_metadata_bool_exp
  ): hierarchical_dimension_metadataConnection!
  node(
    # A globally unique id
    id: ID!
  ): Node

  # fetch data from the table: "non_hierarchical_dimension"
  non_hierarchical_dimension_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [non_hierarchical_dimension_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [non_hierarchical_dimension_order_by!]

    # filter the rows returned
    where: non_hierarchical_dimension_bool_exp
  ): non_hierarchical_dimensionConnection!

  # fetch data from the table: "pipeline_datasource"
  pipeline_datasource_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_datasource_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_datasource_order_by!]

    # filter the rows returned
    where: pipeline_datasource_bool_exp
  ): pipeline_datasourceConnection!

  # fetch data from the table: "pipeline_entity_match"
  pipeline_entity_match_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_entity_match_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_match_order_by!]

    # filter the rows returned
    where: pipeline_entity_match_bool_exp
  ): pipeline_entity_matchConnection!

  # fetch data from the table: "pipeline_entity_type"
  pipeline_entity_type_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_entity_type_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_entity_type_order_by!]

    # filter the rows returned
    where: pipeline_entity_type_bool_exp
  ): pipeline_entity_typeConnection!

  # fetch data from the table: "pipeline_run_metadata"
  pipeline_run_metadata_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [pipeline_run_metadata_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [pipeline_run_metadata_order_by!]

    # filter the rows returned
    where: pipeline_run_metadata_bool_exp
  ): pipeline_run_metadataConnection!

  # fetch data from the table: "raw_pipeline_entity"
  raw_pipeline_entity_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [raw_pipeline_entity_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [raw_pipeline_entity_order_by!]

    # filter the rows returned
    where: raw_pipeline_entity_bool_exp
  ): raw_pipeline_entityConnection!

  # fetch data from the table: "self_serve_source"
  self_serve_source_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [self_serve_source_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [self_serve_source_order_by!]

    # filter the rows returned
    where: self_serve_source_bool_exp
  ): self_serve_sourceConnection!

  # fetch data from the table: "unpublished_field_category_mapping"
  unpublished_field_category_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_category_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_category_mapping_bool_exp
  ): unpublished_field_category_mappingConnection!

  # fetch data from the table: "unpublished_field"
  unpublished_field_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_order_by!]

    # filter the rows returned
    where: unpublished_field_bool_exp
  ): unpublished_fieldConnection!

  # fetch data from the table: "unpublished_field_dimension_mapping"
  unpublished_field_dimension_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_dimension_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_dimension_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_dimension_mapping_bool_exp
  ): unpublished_field_dimension_mappingConnection!

  # fetch data from the table: "unpublished_field_pipeline_datasource_mapping"
  unpublished_field_pipeline_datasource_mapping_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_pipeline_datasource_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_pipeline_datasource_mapping_bool_exp
  ): unpublished_field_pipeline_datasource_mappingConnection!
}

scalar timestamp

# Boolean expression to compare columns of type "timestamp". All fields are combined with logical 'AND'.
input timestamp_comparison_exp {
  _eq: timestamp
  _gt: timestamp
  _gte: timestamp
  _in: [timestamp!]
  _is_null: Boolean
  _lt: timestamp
  _lte: timestamp
  _neq: timestamp
  _nin: [timestamp!]
}

# columns and relationships of "unpublished_field"
type unpublished_field implements Node {
  calculation(
    # JSON select path
    path: String
  ): jsonb
  description: String
  id: ID!
  name: String
  short_name: String

  # An array relationship
  unpublished_field_category_mappings(
    # distinct select on columns
    distinct_on: [unpublished_field_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_category_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_category_mapping_bool_exp
  ): [unpublished_field_category_mapping!]!

  # An aggregate relationship
  unpublished_field_category_mappings_aggregate(
    # distinct select on columns
    distinct_on: [unpublished_field_category_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_category_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_category_mapping_bool_exp
  ): unpublished_field_category_mapping_aggregate!

  # An array relationship connection
  unpublished_field_category_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_category_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_category_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_category_mapping_bool_exp
  ): unpublished_field_category_mappingConnection!

  # An array relationship
  unpublished_field_dimension_mappings(
    # distinct select on columns
    distinct_on: [unpublished_field_dimension_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_dimension_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_dimension_mapping_bool_exp
  ): [unpublished_field_dimension_mapping!]!

  # An aggregate relationship
  unpublished_field_dimension_mappings_aggregate(
    # distinct select on columns
    distinct_on: [unpublished_field_dimension_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_dimension_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_dimension_mapping_bool_exp
  ): unpublished_field_dimension_mapping_aggregate!

  # An array relationship connection
  unpublished_field_dimension_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_dimension_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_dimension_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_dimension_mapping_bool_exp
  ): unpublished_field_dimension_mappingConnection!

  # An array relationship
  unpublished_field_pipeline_datasource_mappings(
    # distinct select on columns
    distinct_on: [unpublished_field_pipeline_datasource_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_pipeline_datasource_mapping_bool_exp
  ): [unpublished_field_pipeline_datasource_mapping!]!

  # An aggregate relationship
  unpublished_field_pipeline_datasource_mappings_aggregate(
    # distinct select on columns
    distinct_on: [unpublished_field_pipeline_datasource_mapping_select_column!]

    # limit the number of rows returned
    limit: Int

    # skip the first n rows. Use only with order_by
    offset: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_pipeline_datasource_mapping_bool_exp
  ): unpublished_field_pipeline_datasource_mapping_aggregate!

  # An array relationship connection
  unpublished_field_pipeline_datasource_mappings_connection(
    after: String
    before: String

    # distinct select on columns
    distinct_on: [unpublished_field_pipeline_datasource_mapping_select_column!]
    first: Int
    last: Int

    # sort the rows by one or more columns
    order_by: [unpublished_field_pipeline_datasource_mapping_order_by!]

    # filter the rows returned
    where: unpublished_field_pipeline_datasource_mapping_bool_exp
  ): unpublished_field_pipeline_datasource_mappingConnection!
}

# append existing jsonb value of filtered columns with new jsonb value
input unpublished_field_append_input {
  calculation: jsonb
}

# Boolean expression to filter rows from the table "unpublished_field". All fields are combined with a logical 'AND'.
input unpublished_field_bool_exp {
  _and: [unpublished_field_bool_exp!]
  _not: unpublished_field_bool_exp
  _or: [unpublished_field_bool_exp!]
  calculation: jsonb_comparison_exp
  description: String_comparison_exp
  id: String_comparison_exp
  name: String_comparison_exp
  short_name: String_comparison_exp
  unpublished_field_category_mappings: unpublished_field_category_mapping_bool_exp
  unpublished_field_dimension_mappings: unpublished_field_dimension_mapping_bool_exp
  unpublished_field_pipeline_datasource_mappings: unpublished_field_pipeline_datasource_mapping_bool_exp
}

# columns and relationships of "unpublished_field_category_mapping"
type unpublished_field_category_mapping implements Node {
  # An object relationship
  category: category!
  category_id: String!
  id: ID!

  # An object relationship
  unpublished_field: unpublished_field!
  unpublished_field_id: String!
}

# aggregated selection of "unpublished_field_category_mapping"
type unpublished_field_category_mapping_aggregate {
  aggregate: unpublished_field_category_mapping_aggregate_fields
  nodes: [unpublished_field_category_mapping!]!
}

# aggregate fields of "unpublished_field_category_mapping"
type unpublished_field_category_mapping_aggregate_fields {
  avg: unpublished_field_category_mapping_avg_fields
  count(columns: [unpublished_field_category_mapping_select_column!], distinct: Boolean): Int!
  max: unpublished_field_category_mapping_max_fields
  min: unpublished_field_category_mapping_min_fields
  stddev: unpublished_field_category_mapping_stddev_fields
  stddev_pop: unpublished_field_category_mapping_stddev_pop_fields
  stddev_samp: unpublished_field_category_mapping_stddev_samp_fields
  sum: unpublished_field_category_mapping_sum_fields
  var_pop: unpublished_field_category_mapping_var_pop_fields
  var_samp: unpublished_field_category_mapping_var_samp_fields
  variance: unpublished_field_category_mapping_variance_fields
}

# order by aggregate values of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_aggregate_order_by {
  avg: unpublished_field_category_mapping_avg_order_by
  count: order_by
  max: unpublished_field_category_mapping_max_order_by
  min: unpublished_field_category_mapping_min_order_by
  stddev: unpublished_field_category_mapping_stddev_order_by
  stddev_pop: unpublished_field_category_mapping_stddev_pop_order_by
  stddev_samp: unpublished_field_category_mapping_stddev_samp_order_by
  sum: unpublished_field_category_mapping_sum_order_by
  var_pop: unpublished_field_category_mapping_var_pop_order_by
  var_samp: unpublished_field_category_mapping_var_samp_order_by
  variance: unpublished_field_category_mapping_variance_order_by
}

# input type for inserting array relation for remote table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_arr_rel_insert_input {
  data: [unpublished_field_category_mapping_insert_input!]!

  # upsert condition
  on_conflict: unpublished_field_category_mapping_on_conflict
}

# aggregate avg on columns
type unpublished_field_category_mapping_avg_fields {
  id: Float
}

# order by avg() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_avg_order_by {
  id: order_by
}

# Boolean expression to filter rows from the table
# "unpublished_field_category_mapping". All fields are combined with a logical 'AND'.
input unpublished_field_category_mapping_bool_exp {
  _and: [unpublished_field_category_mapping_bool_exp!]
  _not: unpublished_field_category_mapping_bool_exp
  _or: [unpublished_field_category_mapping_bool_exp!]
  category: category_bool_exp
  category_id: String_comparison_exp
  id: Int_comparison_exp
  unpublished_field: unpublished_field_bool_exp
  unpublished_field_id: String_comparison_exp
}

# unique or primary key constraints on table "unpublished_field_category_mapping"
enum unpublished_field_category_mapping_constraint {
  # unique or primary key constraint on columns "category_id", "unpublished_field_id"
  unpublished_field_category_ma_unpublished_field_id_category_key

  # unique or primary key constraint on columns "id"
  unpublished_field_category_mapping_pkey
}

# input type for incrementing numeric columns in table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_inc_input {
  id: Int
}

# input type for inserting data into table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_insert_input {
  category: category_obj_rel_insert_input
  category_id: String
  id: Int
  unpublished_field: unpublished_field_obj_rel_insert_input
  unpublished_field_id: String
}

# aggregate max on columns
type unpublished_field_category_mapping_max_fields {
  category_id: String
  id: Int
  unpublished_field_id: String
}

# order by max() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_max_order_by {
  category_id: order_by
  id: order_by
  unpublished_field_id: order_by
}

# aggregate min on columns
type unpublished_field_category_mapping_min_fields {
  category_id: String
  id: Int
  unpublished_field_id: String
}

# order by min() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_min_order_by {
  category_id: order_by
  id: order_by
  unpublished_field_id: order_by
}

# response of any mutation on the table "unpublished_field_category_mapping"
type unpublished_field_category_mapping_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [unpublished_field_category_mapping!]!
}

# on_conflict condition type for table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_on_conflict {
  constraint: unpublished_field_category_mapping_constraint!
  update_columns: [unpublished_field_category_mapping_update_column!]! = []
  where: unpublished_field_category_mapping_bool_exp
}

# Ordering options when selecting data from "unpublished_field_category_mapping".
input unpublished_field_category_mapping_order_by {
  category: category_order_by
  category_id: order_by
  id: order_by
  unpublished_field: unpublished_field_order_by
  unpublished_field_id: order_by
}

# primary key columns input for table: unpublished_field_category_mapping
input unpublished_field_category_mapping_pk_columns_input {
  id: Int!
}

# select columns of table "unpublished_field_category_mapping"
enum unpublished_field_category_mapping_select_column {
  # column name
  category_id

  # column name
  id

  # column name
  unpublished_field_id
}

# input type for updating data in table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_set_input {
  category_id: String
  id: Int
  unpublished_field_id: String
}

# aggregate stddev on columns
type unpublished_field_category_mapping_stddev_fields {
  id: Float
}

# order by stddev() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_stddev_order_by {
  id: order_by
}

# aggregate stddev_pop on columns
type unpublished_field_category_mapping_stddev_pop_fields {
  id: Float
}

# order by stddev_pop() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_stddev_pop_order_by {
  id: order_by
}

# aggregate stddev_samp on columns
type unpublished_field_category_mapping_stddev_samp_fields {
  id: Float
}

# order by stddev_samp() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_stddev_samp_order_by {
  id: order_by
}

# aggregate sum on columns
type unpublished_field_category_mapping_sum_fields {
  id: Int
}

# order by sum() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_sum_order_by {
  id: order_by
}

# update columns of table "unpublished_field_category_mapping"
enum unpublished_field_category_mapping_update_column {
  # column name
  category_id

  # column name
  id

  # column name
  unpublished_field_id
}

input unpublished_field_category_mapping_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: unpublished_field_category_mapping_inc_input

  # sets the columns of the filtered rows to the given values
  _set: unpublished_field_category_mapping_set_input
  where: unpublished_field_category_mapping_bool_exp!
}

# aggregate var_pop on columns
type unpublished_field_category_mapping_var_pop_fields {
  id: Float
}

# order by var_pop() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_var_pop_order_by {
  id: order_by
}

# aggregate var_samp on columns
type unpublished_field_category_mapping_var_samp_fields {
  id: Float
}

# order by var_samp() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_var_samp_order_by {
  id: order_by
}

# aggregate variance on columns
type unpublished_field_category_mapping_variance_fields {
  id: Float
}

# order by variance() on columns of table "unpublished_field_category_mapping"
input unpublished_field_category_mapping_variance_order_by {
  id: order_by
}

# A Relay connection object on "unpublished_field_category_mapping"
type unpublished_field_category_mappingConnection {
  edges: [unpublished_field_category_mappingEdge!]!
  pageInfo: PageInfo!
}

type unpublished_field_category_mappingEdge {
  cursor: String!
  node: unpublished_field_category_mapping!
}

# unique or primary key constraints on table "unpublished_field"
enum unpublished_field_constraint {
  # unique or primary key constraint on columns "id"
  unpublished_field_pkey
}

# delete the field or element with specified path (for JSON arrays, negative integers count from the end)
input unpublished_field_delete_at_path_input {
  calculation: [String!]
}

# delete the array element with specified index (negative integers count from the
# end). throws an error if top level container is not an array
input unpublished_field_delete_elem_input {
  calculation: Int
}

# delete key/value pair or string element. key/value pairs are matched based on their key value
input unpublished_field_delete_key_input {
  calculation: String
}

# columns and relationships of "unpublished_field_dimension_mapping"
type unpublished_field_dimension_mapping implements Node {
  # An object relationship
  dimension: dimension!
  dimension_id: String!
  id: ID!

  # An object relationship
  unpublished_field: unpublished_field!
  unpublished_field_id: String!
}

# aggregated selection of "unpublished_field_dimension_mapping"
type unpublished_field_dimension_mapping_aggregate {
  aggregate: unpublished_field_dimension_mapping_aggregate_fields
  nodes: [unpublished_field_dimension_mapping!]!
}

# aggregate fields of "unpublished_field_dimension_mapping"
type unpublished_field_dimension_mapping_aggregate_fields {
  avg: unpublished_field_dimension_mapping_avg_fields
  count(columns: [unpublished_field_dimension_mapping_select_column!], distinct: Boolean): Int!
  max: unpublished_field_dimension_mapping_max_fields
  min: unpublished_field_dimension_mapping_min_fields
  stddev: unpublished_field_dimension_mapping_stddev_fields
  stddev_pop: unpublished_field_dimension_mapping_stddev_pop_fields
  stddev_samp: unpublished_field_dimension_mapping_stddev_samp_fields
  sum: unpublished_field_dimension_mapping_sum_fields
  var_pop: unpublished_field_dimension_mapping_var_pop_fields
  var_samp: unpublished_field_dimension_mapping_var_samp_fields
  variance: unpublished_field_dimension_mapping_variance_fields
}

# order by aggregate values of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_aggregate_order_by {
  avg: unpublished_field_dimension_mapping_avg_order_by
  count: order_by
  max: unpublished_field_dimension_mapping_max_order_by
  min: unpublished_field_dimension_mapping_min_order_by
  stddev: unpublished_field_dimension_mapping_stddev_order_by
  stddev_pop: unpublished_field_dimension_mapping_stddev_pop_order_by
  stddev_samp: unpublished_field_dimension_mapping_stddev_samp_order_by
  sum: unpublished_field_dimension_mapping_sum_order_by
  var_pop: unpublished_field_dimension_mapping_var_pop_order_by
  var_samp: unpublished_field_dimension_mapping_var_samp_order_by
  variance: unpublished_field_dimension_mapping_variance_order_by
}

# input type for inserting array relation for remote table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_arr_rel_insert_input {
  data: [unpublished_field_dimension_mapping_insert_input!]!

  # upsert condition
  on_conflict: unpublished_field_dimension_mapping_on_conflict
}

# aggregate avg on columns
type unpublished_field_dimension_mapping_avg_fields {
  id: Float
}

# order by avg() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_avg_order_by {
  id: order_by
}

# Boolean expression to filter rows from the table
# "unpublished_field_dimension_mapping". All fields are combined with a logical 'AND'.
input unpublished_field_dimension_mapping_bool_exp {
  _and: [unpublished_field_dimension_mapping_bool_exp!]
  _not: unpublished_field_dimension_mapping_bool_exp
  _or: [unpublished_field_dimension_mapping_bool_exp!]
  dimension: dimension_bool_exp
  dimension_id: String_comparison_exp
  id: Int_comparison_exp
  unpublished_field: unpublished_field_bool_exp
  unpublished_field_id: String_comparison_exp
}

# unique or primary key constraints on table "unpublished_field_dimension_mapping"
enum unpublished_field_dimension_mapping_constraint {
  # unique or primary key constraint on columns "dimension_id", "unpublished_field_id"
  unpublished_field_dimension_m_unpublished_field_id_dimensio_key

  # unique or primary key constraint on columns "id"
  unpublished_field_dimension_mapping_pkey
}

# input type for incrementing numeric columns in table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_inc_input {
  id: Int
}

# input type for inserting data into table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_insert_input {
  dimension: dimension_obj_rel_insert_input
  dimension_id: String
  id: Int
  unpublished_field: unpublished_field_obj_rel_insert_input
  unpublished_field_id: String
}

# aggregate max on columns
type unpublished_field_dimension_mapping_max_fields {
  dimension_id: String
  id: Int
  unpublished_field_id: String
}

# order by max() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_max_order_by {
  dimension_id: order_by
  id: order_by
  unpublished_field_id: order_by
}

# aggregate min on columns
type unpublished_field_dimension_mapping_min_fields {
  dimension_id: String
  id: Int
  unpublished_field_id: String
}

# order by min() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_min_order_by {
  dimension_id: order_by
  id: order_by
  unpublished_field_id: order_by
}

# response of any mutation on the table "unpublished_field_dimension_mapping"
type unpublished_field_dimension_mapping_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [unpublished_field_dimension_mapping!]!
}

# on_conflict condition type for table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_on_conflict {
  constraint: unpublished_field_dimension_mapping_constraint!
  update_columns: [unpublished_field_dimension_mapping_update_column!]! = []
  where: unpublished_field_dimension_mapping_bool_exp
}

# Ordering options when selecting data from "unpublished_field_dimension_mapping".
input unpublished_field_dimension_mapping_order_by {
  dimension: dimension_order_by
  dimension_id: order_by
  id: order_by
  unpublished_field: unpublished_field_order_by
  unpublished_field_id: order_by
}

# primary key columns input for table: unpublished_field_dimension_mapping
input unpublished_field_dimension_mapping_pk_columns_input {
  id: Int!
}

# select columns of table "unpublished_field_dimension_mapping"
enum unpublished_field_dimension_mapping_select_column {
  # column name
  dimension_id

  # column name
  id

  # column name
  unpublished_field_id
}

# input type for updating data in table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_set_input {
  dimension_id: String
  id: Int
  unpublished_field_id: String
}

# aggregate stddev on columns
type unpublished_field_dimension_mapping_stddev_fields {
  id: Float
}

# order by stddev() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_stddev_order_by {
  id: order_by
}

# aggregate stddev_pop on columns
type unpublished_field_dimension_mapping_stddev_pop_fields {
  id: Float
}

# order by stddev_pop() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_stddev_pop_order_by {
  id: order_by
}

# aggregate stddev_samp on columns
type unpublished_field_dimension_mapping_stddev_samp_fields {
  id: Float
}

# order by stddev_samp() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_stddev_samp_order_by {
  id: order_by
}

# aggregate sum on columns
type unpublished_field_dimension_mapping_sum_fields {
  id: Int
}

# order by sum() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_sum_order_by {
  id: order_by
}

# update columns of table "unpublished_field_dimension_mapping"
enum unpublished_field_dimension_mapping_update_column {
  # column name
  dimension_id

  # column name
  id

  # column name
  unpublished_field_id
}

input unpublished_field_dimension_mapping_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: unpublished_field_dimension_mapping_inc_input

  # sets the columns of the filtered rows to the given values
  _set: unpublished_field_dimension_mapping_set_input
  where: unpublished_field_dimension_mapping_bool_exp!
}

# aggregate var_pop on columns
type unpublished_field_dimension_mapping_var_pop_fields {
  id: Float
}

# order by var_pop() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_var_pop_order_by {
  id: order_by
}

# aggregate var_samp on columns
type unpublished_field_dimension_mapping_var_samp_fields {
  id: Float
}

# order by var_samp() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_var_samp_order_by {
  id: order_by
}

# aggregate variance on columns
type unpublished_field_dimension_mapping_variance_fields {
  id: Float
}

# order by variance() on columns of table "unpublished_field_dimension_mapping"
input unpublished_field_dimension_mapping_variance_order_by {
  id: order_by
}

# A Relay connection object on "unpublished_field_dimension_mapping"
type unpublished_field_dimension_mappingConnection {
  edges: [unpublished_field_dimension_mappingEdge!]!
  pageInfo: PageInfo!
}

type unpublished_field_dimension_mappingEdge {
  cursor: String!
  node: unpublished_field_dimension_mapping!
}

# input type for inserting data into table "unpublished_field"
input unpublished_field_insert_input {
  calculation: jsonb
  description: String
  id: String
  name: String
  short_name: String
  unpublished_field_category_mappings: unpublished_field_category_mapping_arr_rel_insert_input
  unpublished_field_dimension_mappings: unpublished_field_dimension_mapping_arr_rel_insert_input
  unpublished_field_pipeline_datasource_mappings: unpublished_field_pipeline_datasource_mapping_arr_rel_insert_input
}

# response of any mutation on the table "unpublished_field"
type unpublished_field_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [unpublished_field!]!
}

# input type for inserting object relation for remote table "unpublished_field"
input unpublished_field_obj_rel_insert_input {
  data: unpublished_field_insert_input!

  # upsert condition
  on_conflict: unpublished_field_on_conflict
}

# on_conflict condition type for table "unpublished_field"
input unpublished_field_on_conflict {
  constraint: unpublished_field_constraint!
  update_columns: [unpublished_field_update_column!]! = []
  where: unpublished_field_bool_exp
}

# Ordering options when selecting data from "unpublished_field".
input unpublished_field_order_by {
  calculation: order_by
  description: order_by
  id: order_by
  name: order_by
  short_name: order_by
  unpublished_field_category_mappings_aggregate: unpublished_field_category_mapping_aggregate_order_by
  unpublished_field_dimension_mappings_aggregate: unpublished_field_dimension_mapping_aggregate_order_by
  unpublished_field_pipeline_datasource_mappings_aggregate: unpublished_field_pipeline_datasource_mapping_aggregate_order_by
}

# columns and relationships of "unpublished_field_pipeline_datasource_mapping"
type unpublished_field_pipeline_datasource_mapping implements Node {
  id: ID!

  # An object relationship
  pipeline_datasource: pipeline_datasource!
  pipeline_datasource_id: String!

  # An object relationship
  unpublished_field: unpublished_field!
  unpublished_field_id: String!
}

# aggregated selection of "unpublished_field_pipeline_datasource_mapping"
type unpublished_field_pipeline_datasource_mapping_aggregate {
  aggregate: unpublished_field_pipeline_datasource_mapping_aggregate_fields
  nodes: [unpublished_field_pipeline_datasource_mapping!]!
}

# aggregate fields of "unpublished_field_pipeline_datasource_mapping"
type unpublished_field_pipeline_datasource_mapping_aggregate_fields {
  avg: unpublished_field_pipeline_datasource_mapping_avg_fields
  count(columns: [unpublished_field_pipeline_datasource_mapping_select_column!], distinct: Boolean): Int!
  max: unpublished_field_pipeline_datasource_mapping_max_fields
  min: unpublished_field_pipeline_datasource_mapping_min_fields
  stddev: unpublished_field_pipeline_datasource_mapping_stddev_fields
  stddev_pop: unpublished_field_pipeline_datasource_mapping_stddev_pop_fields
  stddev_samp: unpublished_field_pipeline_datasource_mapping_stddev_samp_fields
  sum: unpublished_field_pipeline_datasource_mapping_sum_fields
  var_pop: unpublished_field_pipeline_datasource_mapping_var_pop_fields
  var_samp: unpublished_field_pipeline_datasource_mapping_var_samp_fields
  variance: unpublished_field_pipeline_datasource_mapping_variance_fields
}

# order by aggregate values of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_aggregate_order_by {
  avg: unpublished_field_pipeline_datasource_mapping_avg_order_by
  count: order_by
  max: unpublished_field_pipeline_datasource_mapping_max_order_by
  min: unpublished_field_pipeline_datasource_mapping_min_order_by
  stddev: unpublished_field_pipeline_datasource_mapping_stddev_order_by
  stddev_pop: unpublished_field_pipeline_datasource_mapping_stddev_pop_order_by
  stddev_samp: unpublished_field_pipeline_datasource_mapping_stddev_samp_order_by
  sum: unpublished_field_pipeline_datasource_mapping_sum_order_by
  var_pop: unpublished_field_pipeline_datasource_mapping_var_pop_order_by
  var_samp: unpublished_field_pipeline_datasource_mapping_var_samp_order_by
  variance: unpublished_field_pipeline_datasource_mapping_variance_order_by
}

# input type for inserting array relation for remote table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_arr_rel_insert_input {
  data: [unpublished_field_pipeline_datasource_mapping_insert_input!]!

  # upsert condition
  on_conflict: unpublished_field_pipeline_datasource_mapping_on_conflict
}

# aggregate avg on columns
type unpublished_field_pipeline_datasource_mapping_avg_fields {
  id: Float
}

# order by avg() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_avg_order_by {
  id: order_by
}

# Boolean expression to filter rows from the table
# "unpublished_field_pipeline_datasource_mapping". All fields are combined with a logical 'AND'.
input unpublished_field_pipeline_datasource_mapping_bool_exp {
  _and: [unpublished_field_pipeline_datasource_mapping_bool_exp!]
  _not: unpublished_field_pipeline_datasource_mapping_bool_exp
  _or: [unpublished_field_pipeline_datasource_mapping_bool_exp!]
  id: Int_comparison_exp
  pipeline_datasource: pipeline_datasource_bool_exp
  pipeline_datasource_id: String_comparison_exp
  unpublished_field: unpublished_field_bool_exp
  unpublished_field_id: String_comparison_exp
}

# unique or primary key constraints on table "unpublished_field_pipeline_datasource_mapping"
enum unpublished_field_pipeline_datasource_mapping_constraint {
  # unique or primary key constraint on columns "pipeline_datasource_id", "unpublished_field_id"
  unpublished_field_pipeline_da_unpublished_field_id_pipeline_key

  # unique or primary key constraint on columns "id"
  unpublished_field_pipeline_datasource_mapping_pkey
}

# input type for incrementing numeric columns in table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_inc_input {
  id: Int
}

# input type for inserting data into table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_insert_input {
  id: Int
  pipeline_datasource: pipeline_datasource_obj_rel_insert_input
  pipeline_datasource_id: String
  unpublished_field: unpublished_field_obj_rel_insert_input
  unpublished_field_id: String
}

# aggregate max on columns
type unpublished_field_pipeline_datasource_mapping_max_fields {
  id: Int
  pipeline_datasource_id: String
  unpublished_field_id: String
}

# order by max() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_max_order_by {
  id: order_by
  pipeline_datasource_id: order_by
  unpublished_field_id: order_by
}

# aggregate min on columns
type unpublished_field_pipeline_datasource_mapping_min_fields {
  id: Int
  pipeline_datasource_id: String
  unpublished_field_id: String
}

# order by min() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_min_order_by {
  id: order_by
  pipeline_datasource_id: order_by
  unpublished_field_id: order_by
}

# response of any mutation on the table "unpublished_field_pipeline_datasource_mapping"
type unpublished_field_pipeline_datasource_mapping_mutation_response {
  # number of rows affected by the mutation
  affected_rows: Int!

  # data from the rows affected by the mutation
  returning: [unpublished_field_pipeline_datasource_mapping!]!
}

# on_conflict condition type for table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_on_conflict {
  constraint: unpublished_field_pipeline_datasource_mapping_constraint!
  update_columns: [unpublished_field_pipeline_datasource_mapping_update_column!]! = []
  where: unpublished_field_pipeline_datasource_mapping_bool_exp
}

# Ordering options when selecting data from "unpublished_field_pipeline_datasource_mapping".
input unpublished_field_pipeline_datasource_mapping_order_by {
  id: order_by
  pipeline_datasource: pipeline_datasource_order_by
  pipeline_datasource_id: order_by
  unpublished_field: unpublished_field_order_by
  unpublished_field_id: order_by
}

# primary key columns input for table: unpublished_field_pipeline_datasource_mapping
input unpublished_field_pipeline_datasource_mapping_pk_columns_input {
  id: Int!
}

# select columns of table "unpublished_field_pipeline_datasource_mapping"
enum unpublished_field_pipeline_datasource_mapping_select_column {
  # column name
  id

  # column name
  pipeline_datasource_id

  # column name
  unpublished_field_id
}

# input type for updating data in table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_set_input {
  id: Int
  pipeline_datasource_id: String
  unpublished_field_id: String
}

# aggregate stddev on columns
type unpublished_field_pipeline_datasource_mapping_stddev_fields {
  id: Float
}

# order by stddev() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_stddev_order_by {
  id: order_by
}

# aggregate stddev_pop on columns
type unpublished_field_pipeline_datasource_mapping_stddev_pop_fields {
  id: Float
}

# order by stddev_pop() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_stddev_pop_order_by {
  id: order_by
}

# aggregate stddev_samp on columns
type unpublished_field_pipeline_datasource_mapping_stddev_samp_fields {
  id: Float
}

# order by stddev_samp() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_stddev_samp_order_by {
  id: order_by
}

# aggregate sum on columns
type unpublished_field_pipeline_datasource_mapping_sum_fields {
  id: Int
}

# order by sum() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_sum_order_by {
  id: order_by
}

# update columns of table "unpublished_field_pipeline_datasource_mapping"
enum unpublished_field_pipeline_datasource_mapping_update_column {
  # column name
  id

  # column name
  pipeline_datasource_id

  # column name
  unpublished_field_id
}

input unpublished_field_pipeline_datasource_mapping_updates {
  # increments the numeric columns with given value of the filtered values
  _inc: unpublished_field_pipeline_datasource_mapping_inc_input

  # sets the columns of the filtered rows to the given values
  _set: unpublished_field_pipeline_datasource_mapping_set_input
  where: unpublished_field_pipeline_datasource_mapping_bool_exp!
}

# aggregate var_pop on columns
type unpublished_field_pipeline_datasource_mapping_var_pop_fields {
  id: Float
}

# order by var_pop() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_var_pop_order_by {
  id: order_by
}

# aggregate var_samp on columns
type unpublished_field_pipeline_datasource_mapping_var_samp_fields {
  id: Float
}

# order by var_samp() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_var_samp_order_by {
  id: order_by
}

# aggregate variance on columns
type unpublished_field_pipeline_datasource_mapping_variance_fields {
  id: Float
}

# order by variance() on columns of table "unpublished_field_pipeline_datasource_mapping"
input unpublished_field_pipeline_datasource_mapping_variance_order_by {
  id: order_by
}

# A Relay connection object on "unpublished_field_pipeline_datasource_mapping"
type unpublished_field_pipeline_datasource_mappingConnection {
  edges: [unpublished_field_pipeline_datasource_mappingEdge!]!
  pageInfo: PageInfo!
}

type unpublished_field_pipeline_datasource_mappingEdge {
  cursor: String!
  node: unpublished_field_pipeline_datasource_mapping!
}

# primary key columns input for table: unpublished_field
input unpublished_field_pk_columns_input {
  id: String!
}

# prepend existing jsonb value of filtered columns with new jsonb value
input unpublished_field_prepend_input {
  calculation: jsonb
}

# select columns of table "unpublished_field"
enum unpublished_field_select_column {
  # column name
  calculation

  # column name
  description

  # column name
  id

  # column name
  name

  # column name
  short_name
}

# input type for updating data in table "unpublished_field"
input unpublished_field_set_input {
  calculation: jsonb
  description: String
  id: String
  name: String
  short_name: String
}

# update columns of table "unpublished_field"
enum unpublished_field_update_column {
  # column name
  calculation

  # column name
  description

  # column name
  id

  # column name
  name

  # column name
  short_name
}

input unpublished_field_updates {
  # append existing jsonb value of filtered columns with new jsonb value
  _append: unpublished_field_append_input

  # delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  _delete_at_path: unpublished_field_delete_at_path_input

  # delete the array element with specified index (negative integers count from
  # the end). throws an error if top level container is not an array
  _delete_elem: unpublished_field_delete_elem_input

  # delete key/value pair or string element. key/value pairs are matched based on their key value
  _delete_key: unpublished_field_delete_key_input

  # prepend existing jsonb value of filtered columns with new jsonb value
  _prepend: unpublished_field_prepend_input

  # sets the columns of the filtered rows to the given values
  _set: unpublished_field_set_input
  where: unpublished_field_bool_exp!
}

# A Relay connection object on "unpublished_field"
type unpublished_fieldConnection {
  edges: [unpublished_fieldEdge!]!
  pageInfo: PageInfo!
}

type unpublished_fieldEdge {
  cursor: String!
  node: unpublished_field!
}

scalar visibility_status_enum

# Boolean expression to compare columns of type "visibility_status_enum". All fields are combined with logical 'AND'.
input visibility_status_enum_comparison_exp {
  _eq: visibility_status_enum
  _gt: visibility_status_enum
  _gte: visibility_status_enum
  _in: [visibility_status_enum!]
  _is_null: Boolean
  _lt: visibility_status_enum
  _lte: visibility_status_enum
  _neq: visibility_status_enum
  _nin: [visibility_status_enum!]
}

